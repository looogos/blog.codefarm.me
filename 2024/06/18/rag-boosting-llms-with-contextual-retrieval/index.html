<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>RAG: Boosting LLMs with Contextual Retrieval | CODE FARM</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="RAG: Boosting LLMs with Contextual Retrieval" />
<meta property="og:locale" content="en" />
<meta name="description" content="RAG (Retrieval-Augmented Generation) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) like GPT-4. While LLMs excel at generating text, they often lack context and struggle to understand the deeper meaning behind user queries. RAG bridges this gap by incorporating information retrieval to provide LLMs with relevant context, leading to improved response quality. 1. How does RAG work? 2. Deep Dive into Context Enrichment for RAG Systems 3. Automatic Prompt Construction 4. Build RAG with Milvus 4.1. Prepare the data in Milvus 4.2. Use LLM to get a RAG response References 1. How does RAG work? RAG is a pattern which uses your data with an LLM to generate answers specific to your data. When a user asks a question, the data store is searched based on user input. The user question is then combined with the matching results and sent to the LLM using a prompt (explicit instructions to an AI or machine learning model) to generate the desired answer. This can be illustrated as follows. [1] User Input: The user provides a query or prompt. Vector Search: A vector database (like Milvus) efficiently retrieves documents or passages most relevant to the user&#8217;s query based on semantic similarity. Context Enrichment: Techniques like summarization, keyphrase extraction, or entity recognition are applied to the retrieved information, providing context for the LLM. Prompt Construction: The user&#8217;s original query is combined with the extracted context to form a new, enriched prompt for the LLM. Enhanced Generation: The LLM leverages the enriched prompt to generate a more informative and relevant response that addresses the user&#8217;s specific intent and considers the retrieved context. While Milvus and GPT-like LLMs are key players, consider these additional aspects for a well-rounded RAG system: Machine Learning Fundamentals: Understanding concepts like word embeddings and information retrieval is crucial. Alternative Tools: Explore other vector databases and pre-trained word embedding models. Prompt Construction Techniques: Utilize template-based prompts, conditional logic, or fine-tuning for automatic prompt generation. Evaluation: Continuously monitor performance to identify areas for improvement. In essence, RAG empowers LLMs to become more contextually aware, leading to a more informative and engaging user experience. 2. Deep Dive into Context Enrichment for RAG Systems Context enrichment is a crucial step in RAG (Retrieval-Augmented Generation) that bridges the gap between a user&#8217;s query and the LLM&#8217;s response. It involves processing the information retrieved from the vector database (like Milvus) to provide the LLM with a deeper understanding of the user&#8217;s intent and the relevant context. Here&#8217;s a breakdown of some popular libraries and techniques for context enrichment: Text Summarization: Goal: Condense retrieved documents into concise summaries for the LLM to grasp the key points. Libraries: Gensim (Python): Offers various summarization techniques, including extractive (selecting important sentences) and abstractive (generating a new summary). BART (Transformers library): A powerful pre-trained model specifically designed for text summarization. Keyword Extraction: Goal: Identify the most relevant keywords or keyphrases within retrieved documents to highlight the main themes. Libraries: spaCy (Python): Provides functionalities for part-of-speech tagging, named entity recognition, and keyword extraction. NLTK (Python): A comprehensive toolkit for various NLP tasks, including keyword extraction using techniques like TF-IDF (Term Frequency-Inverse Document Frequency). Named Entity Recognition (NER): Goal: Recognize and classify named entities (people, locations, organizations) within retrieved text, enriching the context for the LLM. Libraries: spaCy: Offers pre-trained NER models for various languages, allowing the LLM to understand the context of specific entities. Stanford NER: A widely used Java-based library for named entity recognition. Choosing the Right Technique: The best approach for context enrichment depends on your specific needs and the type of data you&#8217;re working with. Here&#8217;s a quick guide: For factual or informative responses: Text summarization can be highly effective. For understanding the main topics: Keyword extraction is a good choice. For tasks involving specific entities: Named entity recognition becomes crucial. Advanced Techniques: Combining Techniques: Don&#8217;t be limited to a single approach. Combine summarization with keyword extraction or NER to provide richer context to the LLM. Custom Summarization Models: For specialized domains, consider training custom summarization models using domain-specific data. 3. Automatic Prompt Construction Several approaches can automate prompt construction based on user input and extracted context: Template-Based Prompts: Pre-defined templates can be used to structure the prompt, incorporating user query and extracted elements (e.g., &quot;{user_query}: Based on similar content, here are some key points: {key_phrases}. Can you elaborate?&quot;). Conditional Logic: Conditional statements can be used based on the chosen context enrichment technique. For example, if using summaries, the prompt might say &quot;Here&#8217;s a summary of relevant information&#8230;&#8203;&quot; while using keyphrases, it might mention &quot;Here are some key points&#8230;&#8203;&quot; Fine-tuning Language Models: Techniques like fine-tuning pre-trained LLMs can be explored to allow them to automatically learn how to integrate user queries and retrieved context into a cohesive prompt. This is an advanced approach requiring expertise in machine learning. Choosing the Right Tool: The best tool or approach depends on your specific needs and available resources. Here&#8217;s a basic guideline: Simpler Systems: For less complex RAG systems, template-based prompts with basic summarization or keyword extraction tools might suffice. Advanced Systems: For more sophisticated applications, consider exploring conditional logic, fine-tuning LLMs, or combining different context enrichment techniques to create richer prompts. By combining vector databases with the right context enrichment tools and automatic prompt construction techniques, we can build a robust RAG system that leverages the power of LLMs to generate more informative and relevant responses. 4. Build RAG with Milvus We will use Phi-3, an open small language model, to provide an OpenAI-compatible API. Prepare the Phi3 LLM with Ollama on Linux Install Ollama on Linux: curl -fsSL https://ollama.com/install.sh | sh Pull model phi3:mini, and make sure the model checkpoint is prepared: ollama pull phi3:mini $ ollama list NAME ID SIZE MODIFIED phi3:mini 64c1188f2485 2.4 GB 17 minutes ago Check the Phi3 model with the Chat Completion API: curl http://localhost:11434/v1/chat/completions \ -H &quot;Content-Type: application/json&quot; \ -d &#39;{&quot;model&quot;:&quot;phi3:mini&quot;,&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;Hi, who are you?&quot;}]}&#39; { &quot;id&quot;: &quot;chatcmpl-866&quot;, &quot;object&quot;: &quot;chat.completion&quot;, &quot;created&quot;: 1718872510, &quot;model&quot;: &quot;phi3:mini&quot;, &quot;system_fingerprint&quot;: &quot;fp_ollama&quot;, &quot;choices&quot;: [ { &quot;index&quot;: 0, &quot;message&quot;: { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot; I am Phi, an AI developed to provide information and answer questions to the best of my ability. How can I assist you today?&quot; }, &quot;finish_reason&quot;: &quot;stop&quot; } ], &quot;usage&quot;: { &quot;prompt_tokens&quot;: 0, &quot;completion_tokens&quot;: 30, &quot;total_tokens&quot;: 30 } } 4.1. Prepare the data in Milvus Dependencies and Environment pip install --upgrade &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; openai requests # pipenv install -v &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; openai requests Prepare the embedding model from pymilvus.model.dense import SentenceTransformerEmbeddingFunction # Sentence Transformer pre-trained models # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, # Specify the model name ) Create the collection in Milvus from pymilvus import MilvusClient, DataType COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; # 1. Set up a Milvus client client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # 2. Check if the collection already exists and drop it if it does. if client.has_collection(COLLECTION_NAME): client.drop_collection(COLLECTION_NAME) # 3. Create a new collection with specified parameters. client.create_collection( collection_name=COLLECTION_NAME, dimension=384, # The vector has 384 dimensions, matching the SBERT embedding function with all-MiniLM-L6-v2 auto_id=True, # default is False # primary_field_name=&quot;id&quot;, # id_type=&quot;int&quot;, # vector_field_name=&quot;vector&quot;, # metric_type=&quot;COSINE&quot;, # enable_dynamic_field=True, ) # 4. (optional) To load a collection, use the load_collection() method. # client.load_collection( # collection_name=COLLECTION_NAME # ) # # To release a collection, use the release_collection() method. # client.release_collection( # collection_name=COLLECTION_NAME # ) # 5. (optional) The collection created above is loaded automatically. res = client.get_load_state( collection_name=COLLECTION_NAME ) print(res) # 6. (optional) List detailed information about the collection. import json desc = client.describe_collection( collection_name=COLLECTION_NAME, ) print(json.dumps(desc, indent=2)) {&#39;state&#39;: &lt;LoadState: Loaded&gt;} { &quot;collection_name&quot;: &quot;my_rag_collection&quot;, &quot;auto_id&quot;: true, &quot;num_shards&quot;: 1, &quot;description&quot;: &quot;&quot;, &quot;fields&quot;: [ { &quot;field_id&quot;: 100, &quot;name&quot;: &quot;id&quot;, &quot;description&quot;: &quot;&quot;, &quot;type&quot;: 5, &quot;params&quot;: {}, &quot;auto_id&quot;: true, &quot;is_primary&quot;: true }, { &quot;field_id&quot;: 101, &quot;name&quot;: &quot;vector&quot;, &quot;description&quot;: &quot;&quot;, &quot;type&quot;: 101, &quot;params&quot;: { &quot;dim&quot;: 384 } } ], &quot;aliases&quot;: [], &quot;collection_id&quot;: 450568843972908135, &quot;consistency_level&quot;: 2, &quot;properties&quot;: {}, &quot;num_partitions&quot;: 1, &quot;enable_dynamic_field&quot;: true } Use the Milvus development guide to be as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline. # download and save it as a local text file. import os import urllib.request URL = &quot;https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md&quot; FILE_PATH = &quot;./Milvus_DEVELOPMENT.md1&quot; if not os.path.exists(FILE_PATH): urllib.request.urlretrieve(URL, FILE_PATH) Create embeddings, and then insert the data into Milvus from pymilvus import MilvusClient, model COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, ) FILE_PATH = &quot;./Milvus_DEVELOPMENT.md&quot; with open(FILE_PATH, &quot;r+t&quot;, encoding=&#39;utf-8&#39;) as fi: text = fi.read() # Use &quot;# &quot; to separate the content in the file, which can roughly separate # the content of each main part of the markdown file. docs = text.split(&quot;# &quot;) vectors = ef.encode_documents(docs) data = [{&quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i]} for i in range(len(vectors))] res = client.insert(collection_name=COLLECTION_NAME, data=data) print(res) {&#39;insert_count&#39;: 47, &#39;ids&#39;: [450568843971283844, ... , 450568843971283889, 450568843971283890], &#39;cost&#39;: 0} 4.2. Use LLM to get a RAG response from openai import OpenAI from pymilvus import MilvusClient, model COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, ) # Define a query question about the content of the development guide documentation. question = &quot;what is the hardware requirements specification if I want to build Milvus and run from source code?&quot; # Search for the question in the collection and retrieve the semantic top-3 matches. res = client.search( collection_name=COLLECTION_NAME, data=ef.encode_queries([question]), limit=3, # Return top 3 results output_fields=[&quot;text&quot;], # Return the text field ) retrieved_lines_with_distances = [ (r[&quot;entity&quot;][&quot;text&quot;], r[&quot;distance&quot;]) for r in res[0] ] # [ # [ # &quot;Hardware Requirements\n\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\n\n```\n- 8GB of RAM\n- 50GB of free disk space\n```\n\n##&quot;, # 0.8904632329940796 # ], # [ # &quot;Software Requirements\n\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\n\nHere&#39;s a list of verified OS types where Milvus can successfully build and run:\n\n- Debian/Ubuntu\n- Amazon Linux\n- MacOS (x86_64)\n- MacOS (Apple Silicon)\n\n##&quot;, # 0.7089803814888 # ], # [ # &quot;Building Milvus on a local OS/shell environment\n\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\n\n##&quot;, # 0.7013456225395203 # ] # ] # Convert the retrieved documents into a string format. context = &quot;\n&quot;.join( [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances] ) # Define system and user prompts for the Lanage Model. SYSTEM_PROMPT = &quot;&quot;&quot; Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided. &quot;&quot;&quot; USER_PROMPT = f&quot;&quot;&quot; Use the following pieces of information enclosed in &lt;context&gt; tags to provide an answer to the question enclosed in &lt;question&gt; tags. &lt;context&gt; {context} &lt;/context&gt; &lt;question&gt; {question} &lt;/question&gt; &quot;&quot;&quot; # Use OpenAI Chat Completion API to generate a response based on the prompts. OAI_API_KEY = &quot;EMPTY&quot; OAI_API_BASE = &quot;http://localhost:11434/v1&quot; oai_client = OpenAI( api_key=OAI_API_KEY, base_url=OAI_API_BASE, ) response = oai_client.chat.completions.create( model=&quot;phi3:mini&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: SYSTEM_PROMPT}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: USER_PROMPT}, ], ) print(response.choices[0].message.content) The hardware requirements specification for building Milvus and running it from source code includes having at least 8GB of RAM and 50GB of free disk space. References [1] https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation [2 ] https://milvus.io/docs/build-rag-with-milvus.md" />
<meta property="og:description" content="RAG (Retrieval-Augmented Generation) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) like GPT-4. While LLMs excel at generating text, they often lack context and struggle to understand the deeper meaning behind user queries. RAG bridges this gap by incorporating information retrieval to provide LLMs with relevant context, leading to improved response quality. 1. How does RAG work? 2. Deep Dive into Context Enrichment for RAG Systems 3. Automatic Prompt Construction 4. Build RAG with Milvus 4.1. Prepare the data in Milvus 4.2. Use LLM to get a RAG response References 1. How does RAG work? RAG is a pattern which uses your data with an LLM to generate answers specific to your data. When a user asks a question, the data store is searched based on user input. The user question is then combined with the matching results and sent to the LLM using a prompt (explicit instructions to an AI or machine learning model) to generate the desired answer. This can be illustrated as follows. [1] User Input: The user provides a query or prompt. Vector Search: A vector database (like Milvus) efficiently retrieves documents or passages most relevant to the user&#8217;s query based on semantic similarity. Context Enrichment: Techniques like summarization, keyphrase extraction, or entity recognition are applied to the retrieved information, providing context for the LLM. Prompt Construction: The user&#8217;s original query is combined with the extracted context to form a new, enriched prompt for the LLM. Enhanced Generation: The LLM leverages the enriched prompt to generate a more informative and relevant response that addresses the user&#8217;s specific intent and considers the retrieved context. While Milvus and GPT-like LLMs are key players, consider these additional aspects for a well-rounded RAG system: Machine Learning Fundamentals: Understanding concepts like word embeddings and information retrieval is crucial. Alternative Tools: Explore other vector databases and pre-trained word embedding models. Prompt Construction Techniques: Utilize template-based prompts, conditional logic, or fine-tuning for automatic prompt generation. Evaluation: Continuously monitor performance to identify areas for improvement. In essence, RAG empowers LLMs to become more contextually aware, leading to a more informative and engaging user experience. 2. Deep Dive into Context Enrichment for RAG Systems Context enrichment is a crucial step in RAG (Retrieval-Augmented Generation) that bridges the gap between a user&#8217;s query and the LLM&#8217;s response. It involves processing the information retrieved from the vector database (like Milvus) to provide the LLM with a deeper understanding of the user&#8217;s intent and the relevant context. Here&#8217;s a breakdown of some popular libraries and techniques for context enrichment: Text Summarization: Goal: Condense retrieved documents into concise summaries for the LLM to grasp the key points. Libraries: Gensim (Python): Offers various summarization techniques, including extractive (selecting important sentences) and abstractive (generating a new summary). BART (Transformers library): A powerful pre-trained model specifically designed for text summarization. Keyword Extraction: Goal: Identify the most relevant keywords or keyphrases within retrieved documents to highlight the main themes. Libraries: spaCy (Python): Provides functionalities for part-of-speech tagging, named entity recognition, and keyword extraction. NLTK (Python): A comprehensive toolkit for various NLP tasks, including keyword extraction using techniques like TF-IDF (Term Frequency-Inverse Document Frequency). Named Entity Recognition (NER): Goal: Recognize and classify named entities (people, locations, organizations) within retrieved text, enriching the context for the LLM. Libraries: spaCy: Offers pre-trained NER models for various languages, allowing the LLM to understand the context of specific entities. Stanford NER: A widely used Java-based library for named entity recognition. Choosing the Right Technique: The best approach for context enrichment depends on your specific needs and the type of data you&#8217;re working with. Here&#8217;s a quick guide: For factual or informative responses: Text summarization can be highly effective. For understanding the main topics: Keyword extraction is a good choice. For tasks involving specific entities: Named entity recognition becomes crucial. Advanced Techniques: Combining Techniques: Don&#8217;t be limited to a single approach. Combine summarization with keyword extraction or NER to provide richer context to the LLM. Custom Summarization Models: For specialized domains, consider training custom summarization models using domain-specific data. 3. Automatic Prompt Construction Several approaches can automate prompt construction based on user input and extracted context: Template-Based Prompts: Pre-defined templates can be used to structure the prompt, incorporating user query and extracted elements (e.g., &quot;{user_query}: Based on similar content, here are some key points: {key_phrases}. Can you elaborate?&quot;). Conditional Logic: Conditional statements can be used based on the chosen context enrichment technique. For example, if using summaries, the prompt might say &quot;Here&#8217;s a summary of relevant information&#8230;&#8203;&quot; while using keyphrases, it might mention &quot;Here are some key points&#8230;&#8203;&quot; Fine-tuning Language Models: Techniques like fine-tuning pre-trained LLMs can be explored to allow them to automatically learn how to integrate user queries and retrieved context into a cohesive prompt. This is an advanced approach requiring expertise in machine learning. Choosing the Right Tool: The best tool or approach depends on your specific needs and available resources. Here&#8217;s a basic guideline: Simpler Systems: For less complex RAG systems, template-based prompts with basic summarization or keyword extraction tools might suffice. Advanced Systems: For more sophisticated applications, consider exploring conditional logic, fine-tuning LLMs, or combining different context enrichment techniques to create richer prompts. By combining vector databases with the right context enrichment tools and automatic prompt construction techniques, we can build a robust RAG system that leverages the power of LLMs to generate more informative and relevant responses. 4. Build RAG with Milvus We will use Phi-3, an open small language model, to provide an OpenAI-compatible API. Prepare the Phi3 LLM with Ollama on Linux Install Ollama on Linux: curl -fsSL https://ollama.com/install.sh | sh Pull model phi3:mini, and make sure the model checkpoint is prepared: ollama pull phi3:mini $ ollama list NAME ID SIZE MODIFIED phi3:mini 64c1188f2485 2.4 GB 17 minutes ago Check the Phi3 model with the Chat Completion API: curl http://localhost:11434/v1/chat/completions \ -H &quot;Content-Type: application/json&quot; \ -d &#39;{&quot;model&quot;:&quot;phi3:mini&quot;,&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;Hi, who are you?&quot;}]}&#39; { &quot;id&quot;: &quot;chatcmpl-866&quot;, &quot;object&quot;: &quot;chat.completion&quot;, &quot;created&quot;: 1718872510, &quot;model&quot;: &quot;phi3:mini&quot;, &quot;system_fingerprint&quot;: &quot;fp_ollama&quot;, &quot;choices&quot;: [ { &quot;index&quot;: 0, &quot;message&quot;: { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot; I am Phi, an AI developed to provide information and answer questions to the best of my ability. How can I assist you today?&quot; }, &quot;finish_reason&quot;: &quot;stop&quot; } ], &quot;usage&quot;: { &quot;prompt_tokens&quot;: 0, &quot;completion_tokens&quot;: 30, &quot;total_tokens&quot;: 30 } } 4.1. Prepare the data in Milvus Dependencies and Environment pip install --upgrade &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; openai requests # pipenv install -v &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; openai requests Prepare the embedding model from pymilvus.model.dense import SentenceTransformerEmbeddingFunction # Sentence Transformer pre-trained models # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, # Specify the model name ) Create the collection in Milvus from pymilvus import MilvusClient, DataType COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; # 1. Set up a Milvus client client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # 2. Check if the collection already exists and drop it if it does. if client.has_collection(COLLECTION_NAME): client.drop_collection(COLLECTION_NAME) # 3. Create a new collection with specified parameters. client.create_collection( collection_name=COLLECTION_NAME, dimension=384, # The vector has 384 dimensions, matching the SBERT embedding function with all-MiniLM-L6-v2 auto_id=True, # default is False # primary_field_name=&quot;id&quot;, # id_type=&quot;int&quot;, # vector_field_name=&quot;vector&quot;, # metric_type=&quot;COSINE&quot;, # enable_dynamic_field=True, ) # 4. (optional) To load a collection, use the load_collection() method. # client.load_collection( # collection_name=COLLECTION_NAME # ) # # To release a collection, use the release_collection() method. # client.release_collection( # collection_name=COLLECTION_NAME # ) # 5. (optional) The collection created above is loaded automatically. res = client.get_load_state( collection_name=COLLECTION_NAME ) print(res) # 6. (optional) List detailed information about the collection. import json desc = client.describe_collection( collection_name=COLLECTION_NAME, ) print(json.dumps(desc, indent=2)) {&#39;state&#39;: &lt;LoadState: Loaded&gt;} { &quot;collection_name&quot;: &quot;my_rag_collection&quot;, &quot;auto_id&quot;: true, &quot;num_shards&quot;: 1, &quot;description&quot;: &quot;&quot;, &quot;fields&quot;: [ { &quot;field_id&quot;: 100, &quot;name&quot;: &quot;id&quot;, &quot;description&quot;: &quot;&quot;, &quot;type&quot;: 5, &quot;params&quot;: {}, &quot;auto_id&quot;: true, &quot;is_primary&quot;: true }, { &quot;field_id&quot;: 101, &quot;name&quot;: &quot;vector&quot;, &quot;description&quot;: &quot;&quot;, &quot;type&quot;: 101, &quot;params&quot;: { &quot;dim&quot;: 384 } } ], &quot;aliases&quot;: [], &quot;collection_id&quot;: 450568843972908135, &quot;consistency_level&quot;: 2, &quot;properties&quot;: {}, &quot;num_partitions&quot;: 1, &quot;enable_dynamic_field&quot;: true } Use the Milvus development guide to be as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline. # download and save it as a local text file. import os import urllib.request URL = &quot;https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md&quot; FILE_PATH = &quot;./Milvus_DEVELOPMENT.md1&quot; if not os.path.exists(FILE_PATH): urllib.request.urlretrieve(URL, FILE_PATH) Create embeddings, and then insert the data into Milvus from pymilvus import MilvusClient, model COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, ) FILE_PATH = &quot;./Milvus_DEVELOPMENT.md&quot; with open(FILE_PATH, &quot;r+t&quot;, encoding=&#39;utf-8&#39;) as fi: text = fi.read() # Use &quot;# &quot; to separate the content in the file, which can roughly separate # the content of each main part of the markdown file. docs = text.split(&quot;# &quot;) vectors = ef.encode_documents(docs) data = [{&quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i]} for i in range(len(vectors))] res = client.insert(collection_name=COLLECTION_NAME, data=data) print(res) {&#39;insert_count&#39;: 47, &#39;ids&#39;: [450568843971283844, ... , 450568843971283889, 450568843971283890], &#39;cost&#39;: 0} 4.2. Use LLM to get a RAG response from openai import OpenAI from pymilvus import MilvusClient, model COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, ) # Define a query question about the content of the development guide documentation. question = &quot;what is the hardware requirements specification if I want to build Milvus and run from source code?&quot; # Search for the question in the collection and retrieve the semantic top-3 matches. res = client.search( collection_name=COLLECTION_NAME, data=ef.encode_queries([question]), limit=3, # Return top 3 results output_fields=[&quot;text&quot;], # Return the text field ) retrieved_lines_with_distances = [ (r[&quot;entity&quot;][&quot;text&quot;], r[&quot;distance&quot;]) for r in res[0] ] # [ # [ # &quot;Hardware Requirements\n\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\n\n```\n- 8GB of RAM\n- 50GB of free disk space\n```\n\n##&quot;, # 0.8904632329940796 # ], # [ # &quot;Software Requirements\n\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\n\nHere&#39;s a list of verified OS types where Milvus can successfully build and run:\n\n- Debian/Ubuntu\n- Amazon Linux\n- MacOS (x86_64)\n- MacOS (Apple Silicon)\n\n##&quot;, # 0.7089803814888 # ], # [ # &quot;Building Milvus on a local OS/shell environment\n\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\n\n##&quot;, # 0.7013456225395203 # ] # ] # Convert the retrieved documents into a string format. context = &quot;\n&quot;.join( [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances] ) # Define system and user prompts for the Lanage Model. SYSTEM_PROMPT = &quot;&quot;&quot; Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided. &quot;&quot;&quot; USER_PROMPT = f&quot;&quot;&quot; Use the following pieces of information enclosed in &lt;context&gt; tags to provide an answer to the question enclosed in &lt;question&gt; tags. &lt;context&gt; {context} &lt;/context&gt; &lt;question&gt; {question} &lt;/question&gt; &quot;&quot;&quot; # Use OpenAI Chat Completion API to generate a response based on the prompts. OAI_API_KEY = &quot;EMPTY&quot; OAI_API_BASE = &quot;http://localhost:11434/v1&quot; oai_client = OpenAI( api_key=OAI_API_KEY, base_url=OAI_API_BASE, ) response = oai_client.chat.completions.create( model=&quot;phi3:mini&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: SYSTEM_PROMPT}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: USER_PROMPT}, ], ) print(response.choices[0].message.content) The hardware requirements specification for building Milvus and running it from source code includes having at least 8GB of RAM and 50GB of free disk space. References [1] https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation [2 ] https://milvus.io/docs/build-rag-with-milvus.md" />
<link rel="canonical" href="https://blog.codefarm.me/2024/06/18/rag-boosting-llms-with-contextual-retrieval/" />
<meta property="og:url" content="https://blog.codefarm.me/2024/06/18/rag-boosting-llms-with-contextual-retrieval/" />
<meta property="og:site_name" content="CODE FARM" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-18T13:33:05+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="RAG: Boosting LLMs with Contextual Retrieval" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-18T13:33:05+08:00","datePublished":"2024-06-18T13:33:05+08:00","description":"RAG (Retrieval-Augmented Generation) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) like GPT-4. While LLMs excel at generating text, they often lack context and struggle to understand the deeper meaning behind user queries. RAG bridges this gap by incorporating information retrieval to provide LLMs with relevant context, leading to improved response quality. 1. How does RAG work? 2. Deep Dive into Context Enrichment for RAG Systems 3. Automatic Prompt Construction 4. Build RAG with Milvus 4.1. Prepare the data in Milvus 4.2. Use LLM to get a RAG response References 1. How does RAG work? RAG is a pattern which uses your data with an LLM to generate answers specific to your data. When a user asks a question, the data store is searched based on user input. The user question is then combined with the matching results and sent to the LLM using a prompt (explicit instructions to an AI or machine learning model) to generate the desired answer. This can be illustrated as follows. [1] User Input: The user provides a query or prompt. Vector Search: A vector database (like Milvus) efficiently retrieves documents or passages most relevant to the user&#8217;s query based on semantic similarity. Context Enrichment: Techniques like summarization, keyphrase extraction, or entity recognition are applied to the retrieved information, providing context for the LLM. Prompt Construction: The user&#8217;s original query is combined with the extracted context to form a new, enriched prompt for the LLM. Enhanced Generation: The LLM leverages the enriched prompt to generate a more informative and relevant response that addresses the user&#8217;s specific intent and considers the retrieved context. While Milvus and GPT-like LLMs are key players, consider these additional aspects for a well-rounded RAG system: Machine Learning Fundamentals: Understanding concepts like word embeddings and information retrieval is crucial. Alternative Tools: Explore other vector databases and pre-trained word embedding models. Prompt Construction Techniques: Utilize template-based prompts, conditional logic, or fine-tuning for automatic prompt generation. Evaluation: Continuously monitor performance to identify areas for improvement. In essence, RAG empowers LLMs to become more contextually aware, leading to a more informative and engaging user experience. 2. Deep Dive into Context Enrichment for RAG Systems Context enrichment is a crucial step in RAG (Retrieval-Augmented Generation) that bridges the gap between a user&#8217;s query and the LLM&#8217;s response. It involves processing the information retrieved from the vector database (like Milvus) to provide the LLM with a deeper understanding of the user&#8217;s intent and the relevant context. Here&#8217;s a breakdown of some popular libraries and techniques for context enrichment: Text Summarization: Goal: Condense retrieved documents into concise summaries for the LLM to grasp the key points. Libraries: Gensim (Python): Offers various summarization techniques, including extractive (selecting important sentences) and abstractive (generating a new summary). BART (Transformers library): A powerful pre-trained model specifically designed for text summarization. Keyword Extraction: Goal: Identify the most relevant keywords or keyphrases within retrieved documents to highlight the main themes. Libraries: spaCy (Python): Provides functionalities for part-of-speech tagging, named entity recognition, and keyword extraction. NLTK (Python): A comprehensive toolkit for various NLP tasks, including keyword extraction using techniques like TF-IDF (Term Frequency-Inverse Document Frequency). Named Entity Recognition (NER): Goal: Recognize and classify named entities (people, locations, organizations) within retrieved text, enriching the context for the LLM. Libraries: spaCy: Offers pre-trained NER models for various languages, allowing the LLM to understand the context of specific entities. Stanford NER: A widely used Java-based library for named entity recognition. Choosing the Right Technique: The best approach for context enrichment depends on your specific needs and the type of data you&#8217;re working with. Here&#8217;s a quick guide: For factual or informative responses: Text summarization can be highly effective. For understanding the main topics: Keyword extraction is a good choice. For tasks involving specific entities: Named entity recognition becomes crucial. Advanced Techniques: Combining Techniques: Don&#8217;t be limited to a single approach. Combine summarization with keyword extraction or NER to provide richer context to the LLM. Custom Summarization Models: For specialized domains, consider training custom summarization models using domain-specific data. 3. Automatic Prompt Construction Several approaches can automate prompt construction based on user input and extracted context: Template-Based Prompts: Pre-defined templates can be used to structure the prompt, incorporating user query and extracted elements (e.g., &quot;{user_query}: Based on similar content, here are some key points: {key_phrases}. Can you elaborate?&quot;). Conditional Logic: Conditional statements can be used based on the chosen context enrichment technique. For example, if using summaries, the prompt might say &quot;Here&#8217;s a summary of relevant information&#8230;&#8203;&quot; while using keyphrases, it might mention &quot;Here are some key points&#8230;&#8203;&quot; Fine-tuning Language Models: Techniques like fine-tuning pre-trained LLMs can be explored to allow them to automatically learn how to integrate user queries and retrieved context into a cohesive prompt. This is an advanced approach requiring expertise in machine learning. Choosing the Right Tool: The best tool or approach depends on your specific needs and available resources. Here&#8217;s a basic guideline: Simpler Systems: For less complex RAG systems, template-based prompts with basic summarization or keyword extraction tools might suffice. Advanced Systems: For more sophisticated applications, consider exploring conditional logic, fine-tuning LLMs, or combining different context enrichment techniques to create richer prompts. By combining vector databases with the right context enrichment tools and automatic prompt construction techniques, we can build a robust RAG system that leverages the power of LLMs to generate more informative and relevant responses. 4. Build RAG with Milvus We will use Phi-3, an open small language model, to provide an OpenAI-compatible API. Prepare the Phi3 LLM with Ollama on Linux Install Ollama on Linux: curl -fsSL https://ollama.com/install.sh | sh Pull model phi3:mini, and make sure the model checkpoint is prepared: ollama pull phi3:mini $ ollama list NAME ID SIZE MODIFIED phi3:mini 64c1188f2485 2.4 GB 17 minutes ago Check the Phi3 model with the Chat Completion API: curl http://localhost:11434/v1/chat/completions \\ -H &quot;Content-Type: application/json&quot; \\ -d &#39;{&quot;model&quot;:&quot;phi3:mini&quot;,&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;Hi, who are you?&quot;}]}&#39; { &quot;id&quot;: &quot;chatcmpl-866&quot;, &quot;object&quot;: &quot;chat.completion&quot;, &quot;created&quot;: 1718872510, &quot;model&quot;: &quot;phi3:mini&quot;, &quot;system_fingerprint&quot;: &quot;fp_ollama&quot;, &quot;choices&quot;: [ { &quot;index&quot;: 0, &quot;message&quot;: { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot; I am Phi, an AI developed to provide information and answer questions to the best of my ability. How can I assist you today?&quot; }, &quot;finish_reason&quot;: &quot;stop&quot; } ], &quot;usage&quot;: { &quot;prompt_tokens&quot;: 0, &quot;completion_tokens&quot;: 30, &quot;total_tokens&quot;: 30 } } 4.1. Prepare the data in Milvus Dependencies and Environment pip install --upgrade &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; openai requests # pipenv install -v &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; openai requests Prepare the embedding model from pymilvus.model.dense import SentenceTransformerEmbeddingFunction # Sentence Transformer pre-trained models # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, # Specify the model name ) Create the collection in Milvus from pymilvus import MilvusClient, DataType COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; # 1. Set up a Milvus client client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # 2. Check if the collection already exists and drop it if it does. if client.has_collection(COLLECTION_NAME): client.drop_collection(COLLECTION_NAME) # 3. Create a new collection with specified parameters. client.create_collection( collection_name=COLLECTION_NAME, dimension=384, # The vector has 384 dimensions, matching the SBERT embedding function with all-MiniLM-L6-v2 auto_id=True, # default is False # primary_field_name=&quot;id&quot;, # id_type=&quot;int&quot;, # vector_field_name=&quot;vector&quot;, # metric_type=&quot;COSINE&quot;, # enable_dynamic_field=True, ) # 4. (optional) To load a collection, use the load_collection() method. # client.load_collection( # collection_name=COLLECTION_NAME # ) # # To release a collection, use the release_collection() method. # client.release_collection( # collection_name=COLLECTION_NAME # ) # 5. (optional) The collection created above is loaded automatically. res = client.get_load_state( collection_name=COLLECTION_NAME ) print(res) # 6. (optional) List detailed information about the collection. import json desc = client.describe_collection( collection_name=COLLECTION_NAME, ) print(json.dumps(desc, indent=2)) {&#39;state&#39;: &lt;LoadState: Loaded&gt;} { &quot;collection_name&quot;: &quot;my_rag_collection&quot;, &quot;auto_id&quot;: true, &quot;num_shards&quot;: 1, &quot;description&quot;: &quot;&quot;, &quot;fields&quot;: [ { &quot;field_id&quot;: 100, &quot;name&quot;: &quot;id&quot;, &quot;description&quot;: &quot;&quot;, &quot;type&quot;: 5, &quot;params&quot;: {}, &quot;auto_id&quot;: true, &quot;is_primary&quot;: true }, { &quot;field_id&quot;: 101, &quot;name&quot;: &quot;vector&quot;, &quot;description&quot;: &quot;&quot;, &quot;type&quot;: 101, &quot;params&quot;: { &quot;dim&quot;: 384 } } ], &quot;aliases&quot;: [], &quot;collection_id&quot;: 450568843972908135, &quot;consistency_level&quot;: 2, &quot;properties&quot;: {}, &quot;num_partitions&quot;: 1, &quot;enable_dynamic_field&quot;: true } Use the Milvus development guide to be as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline. # download and save it as a local text file. import os import urllib.request URL = &quot;https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md&quot; FILE_PATH = &quot;./Milvus_DEVELOPMENT.md1&quot; if not os.path.exists(FILE_PATH): urllib.request.urlretrieve(URL, FILE_PATH) Create embeddings, and then insert the data into Milvus from pymilvus import MilvusClient, model COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, ) FILE_PATH = &quot;./Milvus_DEVELOPMENT.md&quot; with open(FILE_PATH, &quot;r+t&quot;, encoding=&#39;utf-8&#39;) as fi: text = fi.read() # Use &quot;# &quot; to separate the content in the file, which can roughly separate # the content of each main part of the markdown file. docs = text.split(&quot;# &quot;) vectors = ef.encode_documents(docs) data = [{&quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i]} for i in range(len(vectors))] res = client.insert(collection_name=COLLECTION_NAME, data=data) print(res) {&#39;insert_count&#39;: 47, &#39;ids&#39;: [450568843971283844, ... , 450568843971283889, 450568843971283890], &#39;cost&#39;: 0} 4.2. Use LLM to get a RAG response from openai import OpenAI from pymilvus import MilvusClient, model COLLECTION_NAME = &quot;my_rag_collection&quot; SERVER_ADDR = &quot;http://localhost:19530&quot; ACCESS_TOKEN = &quot;root:Milvus&quot; DB_NAME = &quot;default&quot; client = MilvusClient( uri=SERVER_ADDR, token=ACCESS_TOKEN, db_name=DB_NAME, ) # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, ) # Define a query question about the content of the development guide documentation. question = &quot;what is the hardware requirements specification if I want to build Milvus and run from source code?&quot; # Search for the question in the collection and retrieve the semantic top-3 matches. res = client.search( collection_name=COLLECTION_NAME, data=ef.encode_queries([question]), limit=3, # Return top 3 results output_fields=[&quot;text&quot;], # Return the text field ) retrieved_lines_with_distances = [ (r[&quot;entity&quot;][&quot;text&quot;], r[&quot;distance&quot;]) for r in res[0] ] # [ # [ # &quot;Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##&quot;, # 0.8904632329940796 # ], # [ # &quot;Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere&#39;s a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##&quot;, # 0.7089803814888 # ], # [ # &quot;Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##&quot;, # 0.7013456225395203 # ] # ] # Convert the retrieved documents into a string format. context = &quot;\\n&quot;.join( [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances] ) # Define system and user prompts for the Lanage Model. SYSTEM_PROMPT = &quot;&quot;&quot; Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided. &quot;&quot;&quot; USER_PROMPT = f&quot;&quot;&quot; Use the following pieces of information enclosed in &lt;context&gt; tags to provide an answer to the question enclosed in &lt;question&gt; tags. &lt;context&gt; {context} &lt;/context&gt; &lt;question&gt; {question} &lt;/question&gt; &quot;&quot;&quot; # Use OpenAI Chat Completion API to generate a response based on the prompts. OAI_API_KEY = &quot;EMPTY&quot; OAI_API_BASE = &quot;http://localhost:11434/v1&quot; oai_client = OpenAI( api_key=OAI_API_KEY, base_url=OAI_API_BASE, ) response = oai_client.chat.completions.create( model=&quot;phi3:mini&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: SYSTEM_PROMPT}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: USER_PROMPT}, ], ) print(response.choices[0].message.content) The hardware requirements specification for building Milvus and running it from source code includes having at least 8GB of RAM and 50GB of free disk space. References [1] https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation [2 ] https://milvus.io/docs/build-rag-with-milvus.md","headline":"RAG: Boosting LLMs with Contextual Retrieval","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.codefarm.me/2024/06/18/rag-boosting-llms-with-contextual-retrieval/"},"url":"https://blog.codefarm.me/2024/06/18/rag-boosting-llms-with-contextual-retrieval/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="/assets/css/style.css"><!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SN88FJ18E5"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-SN88FJ18E5');
    </script></head>
  <body>
    <header class="c-header">
  <div class="o-container">
    <a class="c-header-title" href="/">CODE FARM</a>
    <button class="c-header-nav-toggle" id="nav-toggle" aria-label="Toggle navigation">
      <span class="c-header-nav-toggle-icon"></span>
    </button>
    <div class="c-header-nav-wrapper" id="nav-wrapper">
      <nav class="c-header-nav">
        <a href="/">Home</a>
        <a href="/categories/">Category</a>
        <a href="/tags/">Tag</a>
        <a href="/archives/">Archive</a>
        <a href="/about/">About</a>
        <a href="https://resume.github.io/?looogos" target="_blank">R&eacute;sum&eacute;</a>
      </nav>
    </div>
  </div>
  



<div class="o-container">
  <div class="c-banner">
    <img src="/assets/images/galaxy.svg" alt="Galaxy background" class="c-banner-bg">
    <div class="c-banner-quote">
      <p>"The Renaissance was a time when art, science, and philosophy flourished."</p>
      <cite>- Michelangelo</cite>
    </div>
  </div>
</div>
</header>

    <main class="o-container">
      <article class="c-post">
  <header class="c-post-header">
    <h1 class="c-post-title">RAG: Boosting LLMs with Contextual Retrieval</h1><p class="c-post-meta">18 Jun 2024</p>
  </header>

  <div class="c-post-content">
    <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>RAG (Retrieval-Augmented Generation) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) like GPT-4. While LLMs excel at generating text, they often lack context and struggle to understand the deeper meaning behind user queries. RAG bridges this gap by incorporating information retrieval to provide LLMs with relevant context, leading to improved response quality.</p>
</div>
</div>
<div id="toc" class="toc">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#how-does-rag-work">1. How does RAG work?</a></li>
<li><a href="#deep-dive-into-context-enrichment-for-rag-systems">2. Deep Dive into Context Enrichment for RAG Systems</a></li>
<li><a href="#automatic-prompt-construction">3. Automatic Prompt Construction</a></li>
<li><a href="#build-rag-with-milvus">4. Build RAG with Milvus</a>
<ul class="sectlevel2">
<li><a href="#prepare-the-data-in-milvus">4.1. Prepare the data in Milvus</a></li>
<li><a href="#use-llm-to-get-a-rag-response">4.2. Use LLM to get a RAG response</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>
</div>
<div class="sect1">
<h2 id="how-does-rag-work">1. How does RAG work?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>RAG is a pattern which uses your data with an LLM to generate answers specific to your data. When a user asks a question, the data store is searched based on user input. The user question is then combined with the matching results and sent to the LLM using a prompt (explicit instructions to an AI or machine learning model) to generate the desired answer. This can be illustrated as follows. <a href="#ms-az-ai-rag">[1]</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://learn.microsoft.com/en-us/azure/ai-studio/media/index-retrieve/rag-pattern.png#lightbox" alt="Screenshot of the RAG pattern." width="55%" height="55%">
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>User Input: The user provides a query or prompt.</p>
</li>
<li>
<p>Vector Search: A vector database (like Milvus) efficiently retrieves documents or passages most relevant to the user&#8217;s query based on semantic similarity.</p>
</li>
<li>
<p>Context Enrichment: Techniques like summarization, keyphrase extraction, or entity recognition are applied to the retrieved information, providing context for the LLM.</p>
</li>
<li>
<p>Prompt Construction: The user&#8217;s original query is combined with the extracted context to form a new, enriched prompt for the LLM.</p>
</li>
<li>
<p>Enhanced Generation: The LLM leverages the enriched prompt to generate a more informative and relevant response that addresses the user&#8217;s specific intent and considers the retrieved context.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>While Milvus and GPT-like LLMs are key players, consider these additional aspects for a well-rounded RAG system:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Machine Learning Fundamentals: Understanding concepts like word embeddings and information retrieval is crucial.</p>
</li>
<li>
<p>Alternative Tools: Explore other vector databases and pre-trained word embedding models.</p>
</li>
<li>
<p>Prompt Construction Techniques: Utilize template-based prompts, conditional logic, or fine-tuning for automatic prompt generation.</p>
</li>
<li>
<p>Evaluation: Continuously monitor performance to identify areas for improvement.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, RAG empowers LLMs to become more contextually aware, leading to a more informative and engaging user experience.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="deep-dive-into-context-enrichment-for-rag-systems">2. Deep Dive into Context Enrichment for RAG Systems</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Context enrichment is a crucial step in RAG (Retrieval-Augmented Generation) that bridges the gap between a user&#8217;s query and the LLM&#8217;s response. It involves processing the information retrieved from the vector database (like Milvus) to provide the LLM with a deeper understanding of the user&#8217;s intent and the relevant context.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown of some popular libraries and techniques for context enrichment:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Text Summarization:</p>
<div class="ulist">
<ul>
<li>
<p>Goal: Condense retrieved documents into concise summaries for the LLM to grasp the key points.</p>
</li>
<li>
<p>Libraries:</p>
<div class="ulist">
<ul>
<li>
<p>Gensim (Python): Offers various summarization techniques, including extractive (selecting important sentences) and abstractive (generating a new summary).</p>
</li>
<li>
<p>BART (Transformers library): A powerful pre-trained model specifically designed for text summarization.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Keyword Extraction:</p>
<div class="ulist">
<ul>
<li>
<p>Goal: Identify the most relevant keywords or keyphrases within retrieved documents to highlight the main themes.</p>
</li>
<li>
<p>Libraries:</p>
<div class="ulist">
<ul>
<li>
<p>spaCy (Python): Provides functionalities for part-of-speech tagging, named entity recognition, and keyword extraction.</p>
</li>
<li>
<p>NLTK (Python): A comprehensive toolkit for various NLP tasks, including keyword extraction using techniques like TF-IDF (Term Frequency-Inverse Document Frequency).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Named Entity Recognition (NER):</p>
<div class="ulist">
<ul>
<li>
<p>Goal: Recognize and classify named entities (people, locations, organizations) within retrieved text, enriching the context for the LLM.</p>
</li>
<li>
<p>Libraries:</p>
<div class="ulist">
<ul>
<li>
<p>spaCy: Offers pre-trained NER models for various languages, allowing the LLM to understand the context of specific entities.</p>
</li>
<li>
<p>Stanford NER: A widely used Java-based library for named entity recognition.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Choosing the Right Technique:</strong></p>
</div>
<div class="paragraph">
<p>The best approach for context enrichment depends on your specific needs and the type of data you&#8217;re working with. Here&#8217;s a quick guide:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For factual or informative responses: Text summarization can be highly effective.</p>
</li>
<li>
<p>For understanding the main topics: Keyword extraction is a good choice.</p>
</li>
<li>
<p>For tasks involving specific entities: Named entity recognition becomes crucial.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Advanced Techniques:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Combining Techniques: Don&#8217;t be limited to a single approach. Combine summarization with keyword extraction or NER to provide richer context to the LLM.</p>
</li>
<li>
<p>Custom Summarization Models: For specialized domains, consider training custom summarization models using domain-specific data.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="automatic-prompt-construction">3. Automatic Prompt Construction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Several approaches can automate prompt construction based on user input and extracted context:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Template-Based Prompts: Pre-defined templates can be used to structure the prompt, incorporating user query and extracted elements (e.g., "{user_query}: Based on similar content, here are some key points: {key_phrases}. Can you elaborate?").</p>
</li>
<li>
<p>Conditional Logic: Conditional statements can be used based on the chosen context enrichment technique. For example, if using summaries, the prompt might say "Here&#8217;s a summary of relevant information&#8230;&#8203;" while using keyphrases, it might mention "Here are some key points&#8230;&#8203;"</p>
</li>
<li>
<p>Fine-tuning Language Models: Techniques like fine-tuning pre-trained LLMs can be explored to allow them to automatically learn how to integrate user queries and retrieved context into a cohesive prompt. This is an advanced approach requiring expertise in machine learning.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Choosing the Right Tool:</strong></p>
</div>
<div class="paragraph">
<p>The best tool or approach depends on your specific needs and available resources. Here&#8217;s a basic guideline:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Simpler Systems: For less complex RAG systems, template-based prompts with basic summarization or keyword extraction tools might suffice.</p>
</li>
<li>
<p>Advanced Systems: For more sophisticated applications, consider exploring conditional logic, fine-tuning LLMs, or combining different context enrichment techniques to create richer prompts.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>By combining vector databases with the right context enrichment tools and automatic prompt construction techniques, we can build a robust RAG system that leverages the power of LLMs to generate more informative and relevant responses.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="build-rag-with-milvus">4. Build RAG with Milvus</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will use <a href="https://azure.microsoft.com/en-us/products/phi-3">Phi-3</a>, an open small language model, to provide an OpenAI-compatible API.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Prepare the Phi3 LLM with Ollama on Linux</div>
<div class="ulist">
<ul>
<li>
<p>Install Ollama on Linux:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">curl <span class="nt">-fsSL</span> https://ollama.com/install.sh | sh</code></pre>
</div>
</div>
</li>
<li>
<p>Pull model <code>phi3:mini</code>, and make sure the model checkpoint is prepared:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">ollama pull phi3:mini</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>ollama list
<span class="go">NAME                    ID              SIZE    MODIFIED
phi3:mini               64c1188f2485    2.4 GB  17 minutes ago</span></code></pre>
</div>
</div>
</li>
<li>
<p>Check the Phi3 model with the Chat Completion API:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">curl http://localhost:11434/v1/chat/completions <span class="se">\</span>
    <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
    <span class="nt">-d</span> <span class="s1">'{"model":"phi3:mini","messages":[{"role":"user","content":"Hi, who are you?"}]}'</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{
  "id": "chatcmpl-866",
  "object": "chat.completion",
  "created": 1718872510,
  "model": "phi3:mini",
  "system_fingerprint": "fp_ollama",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": " I am Phi, an AI developed to provide information and answer questions to the best of my ability. How can I assist you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 30,
    "total_tokens": 30
  }
}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="prepare-the-data-in-milvus">4.1. Prepare the data in Milvus</h3>
<div class="ulist">
<ul>
<li>
<p>Dependencies and Environment</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">pip <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="s1">'pymilvus[model]==2.4.4'</span> <span class="s1">'numpy&lt;2'</span> openai requests
<span class="c"># pipenv install -v 'pymilvus[model]==2.4.4' 'numpy&lt;2'  openai requests</span></code></pre>
</div>
</div>
</li>
<li>
<p>Prepare the embedding model</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus.model.dense</span> <span class="kn">import</span> <span class="n">SentenceTransformerEmbeddingFunction</span>  <span class="c1"># Sentence Transformer pre-trained models
</span>
<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="n">ef</span> <span class="o">=</span> <span class="nc">SentenceTransformerEmbeddingFunction</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># Specify the model name
</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create the collection in Milvus</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span><span class="p">,</span> <span class="n">DataType</span>

<span class="n">COLLECTION_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">my_rag_collection</span><span class="sh">"</span>
<span class="n">SERVER_ADDR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span>
<span class="n">ACCESS_TOKEN</span> <span class="o">=</span> <span class="sh">"</span><span class="s">root:Milvus</span><span class="sh">"</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">default</span><span class="sh">"</span>

<span class="c1"># 1. Set up a Milvus client
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="n">SERVER_ADDR</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">ACCESS_TOKEN</span><span class="p">,</span>
    <span class="n">db_name</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 2. Check if the collection already exists and drop it if it does.
</span><span class="k">if</span> <span class="n">client</span><span class="p">.</span><span class="nf">has_collection</span><span class="p">(</span><span class="n">COLLECTION_NAME</span><span class="p">):</span>
    <span class="n">client</span><span class="p">.</span><span class="nf">drop_collection</span><span class="p">(</span><span class="n">COLLECTION_NAME</span><span class="p">)</span>

<span class="c1"># 3. Create a new collection with specified parameters.
</span><span class="n">client</span><span class="p">.</span><span class="nf">create_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
    <span class="n">dimension</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span>  <span class="c1"># The vector has 384 dimensions, matching the SBERT embedding function with all-MiniLM-L6-v2
</span>    <span class="n">auto_id</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># default is False
</span>    <span class="c1"># primary_field_name="id",
</span>    <span class="c1"># id_type="int",
</span>    <span class="c1"># vector_field_name="vector",
</span>    <span class="c1"># metric_type="COSINE",
</span>    <span class="c1"># enable_dynamic_field=True,
</span><span class="p">)</span>

<span class="c1"># 4. (optional) To load a collection, use the load_collection() method.
# client.load_collection(
#     collection_name=COLLECTION_NAME
# )
#
# To release a collection, use the release_collection() method.
# client.release_collection(
#     collection_name=COLLECTION_NAME
# )
</span>
<span class="c1"># 5. (optional) The collection created above is loaded automatically.
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_load_state</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># 6. (optional) List detailed information about the collection.
</span><span class="kn">import</span> <span class="n">json</span>
<span class="n">desc</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">describe_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">desc</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">{'state': &lt;LoadState: Loaded&gt;</span><span class="o">}</span>
<span class="go">{
  "collection_name": "my_rag_collection",
  "auto_id": true,
  "num_shards": 1,
  "description": "",
  "fields": [
    {
      "field_id": 100,
      "name": "id",
      "description": "",
      "type": 5,
      "params": {},
      "auto_id": true,
      "is_primary": true
    },
    {
      "field_id": 101,
      "name": "vector",
      "description": "",
      "type": 101,
      "params": {
        "dim": 384
      }
    }
  ],
  "aliases": [],
  "collection_id": 450568843972908135,
  "consistency_level": 2,
  "properties": {},
  "num_partitions": 1,
  "enable_dynamic_field": true
}</span></code></pre>
</div>
</div>
</li>
<li>
<p>Use the <a href="https://github.com/milvus-io/milvus/blob/master/DEVELOPMENT.md">Milvus development guide</a> to be as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># download and save it as a local text file.
</span><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">urllib.request</span>

<span class="n">URL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md</span><span class="sh">"</span>
<span class="n">FILE_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./Milvus_DEVELOPMENT.md1</span><span class="sh">"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">FILE_PATH</span><span class="p">):</span>
    <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="nf">urlretrieve</span><span class="p">(</span><span class="n">URL</span><span class="p">,</span> <span class="n">FILE_PATH</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create embeddings, and then insert the data into Milvus</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span><span class="p">,</span> <span class="n">model</span>

<span class="n">COLLECTION_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">my_rag_collection</span><span class="sh">"</span>
<span class="n">SERVER_ADDR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span>
<span class="n">ACCESS_TOKEN</span> <span class="o">=</span> <span class="sh">"</span><span class="s">root:Milvus</span><span class="sh">"</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">default</span><span class="sh">"</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="n">SERVER_ADDR</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">ACCESS_TOKEN</span><span class="p">,</span>
    <span class="n">db_name</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="n">ef</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">dense</span><span class="p">.</span><span class="nc">SentenceTransformerEmbeddingFunction</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">FILE_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./Milvus_DEVELOPMENT.md</span><span class="sh">"</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">FILE_PATH</span><span class="p">,</span> <span class="sh">"</span><span class="s">r+t</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fi</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">fi</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="c1"># Use "# " to separate the content in the file, which can roughly separate
# the content of each main part of the markdown file.
</span><span class="n">docs</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"># </span><span class="sh">"</span><span class="p">)</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">ef</span><span class="p">.</span><span class="nf">encode_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">:</span> <span class="n">vectors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">docs</span><span class="p">[</span><span class="n">i</span><span class="p">]}</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">vectors</span><span class="p">))]</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'insert_count': 47, 'ids': [450568843971283844, ... , 450568843971283889, 450568843971283890], 'cost': 0}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="use-llm-to-get-a-rag-response">4.2. Use LLM to get a RAG response</h3>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span><span class="p">,</span> <span class="n">model</span>

<span class="n">COLLECTION_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">my_rag_collection</span><span class="sh">"</span>
<span class="n">SERVER_ADDR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span>
<span class="n">ACCESS_TOKEN</span> <span class="o">=</span> <span class="sh">"</span><span class="s">root:Milvus</span><span class="sh">"</span>
<span class="n">DB_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">default</span><span class="sh">"</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="n">SERVER_ADDR</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">ACCESS_TOKEN</span><span class="p">,</span>
    <span class="n">db_name</span><span class="o">=</span><span class="n">DB_NAME</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="n">ef</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">dense</span><span class="p">.</span><span class="nc">SentenceTransformerEmbeddingFunction</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define a query question about the content of the development guide documentation.
</span><span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">what is the hardware requirements specification if I want to build Milvus and run from source code?</span><span class="sh">"</span>

<span class="c1"># Search for the question in the collection and retrieve the semantic top-3 matches.
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">ef</span><span class="p">.</span><span class="nf">encode_queries</span><span class="p">([</span><span class="n">question</span><span class="p">]),</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># Return top 3 results
</span>    <span class="n">output_fields</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span>  <span class="c1"># Return the text field
</span><span class="p">)</span>

<span class="n">retrieved_lines_with_distances</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="sh">"</span><span class="s">entity</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="sh">"</span><span class="s">distance</span><span class="sh">"</span><span class="p">])</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">]</span>
<span class="c1"># [
#   [
#     "Hardware Requirements\n\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\n\n```\n- 8GB of RAM\n- 50GB of free disk space\n```\n\n##",
#     0.8904632329940796
#   ],
#   [
#     "Software Requirements\n\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\n\nHere's a list of verified OS types where Milvus can successfully build and run:\n\n- Debian/Ubuntu\n- Amazon Linux\n- MacOS (x86_64)\n- MacOS (Apple Silicon)\n\n##",
#     0.7089803814888
#   ],
#   [
#     "Building Milvus on a local OS/shell environment\n\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\n\n##",
#     0.7013456225395203
#   ]
# ]
</span>
<span class="c1"># Convert the retrieved documents into a string format.
</span><span class="n">context</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span>
    <span class="p">[</span><span class="n">line_with_distance</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">line_with_distance</span> <span class="ow">in</span> <span class="n">retrieved_lines_with_distances</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Define system and user prompts for the Lanage Model.
</span><span class="n">SYSTEM_PROMPT</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.
</span><span class="sh">"""</span>
<span class="n">USER_PROMPT</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
Use the following pieces of information enclosed in &lt;context&gt; tags to provide an answer to the question enclosed in &lt;question&gt; tags.
&lt;context&gt;
</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">
&lt;/context&gt;
&lt;question&gt;
</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s">
&lt;/question&gt;
</span><span class="sh">"""</span>

<span class="c1"># Use OpenAI Chat Completion API to generate a response based on the prompts.
</span><span class="n">OAI_API_KEY</span> <span class="o">=</span> <span class="sh">"</span><span class="s">EMPTY</span><span class="sh">"</span>
<span class="n">OAI_API_BASE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:11434/v1</span><span class="sh">"</span>

<span class="n">oai_client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">OAI_API_KEY</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="n">OAI_API_BASE</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">oai_client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">phi3:mini</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">SYSTEM_PROMPT</span><span class="p">},</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">USER_PROMPT</span><span class="p">},</span>
    <span class="p">],</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">The hardware requirements specification for building Milvus and running it from source code includes having at least 8GB of RAM and 50GB of free disk space.</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="references">References</h2>
<div class="sectionbody">
<div class="ulist bibliography">
<ul class="bibliography">
<li>
<p><a id="ms-az-ai-rag"></a>[1] <a href="https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation" class="bare">https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation</a></p>
</li>
<li>
<p><a id="milvus-arg"></a>[2 ] <a href="https://milvus.io/docs/build-rag-with-milvus.md" class="bare">https://milvus.io/docs/build-rag-with-milvus.md</a></p>
</li>
</ul>
</div>
</div>
</div>
<style>
  .utterances {
      max-width: 100%;
  }
</style>
<script src="https://utteranc.es/client.js"
        repo="looogos/utterances"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

</div>
</article>
    </main>
    <footer class="c-footer">
  <div class="c-footer-license">
    <span>Article licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span>
  </div>
  
  <details class="c-footer-extralinks" open>
    <summary class="c-footer-extralinks-summary">Extral Links</summary>
    <div class="c-footer-extralinks-content">
      
      <a href="https://jekyllrb.com/">Jekyll</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://shopify.github.io/liquid/">Liquid</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://docs.asciidoctor.org/">Asciidoctor</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://github.com/qqbuby/">GitHub</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="/feed.xml">RSS</a>
      
      
    </div>
  </details>
  
</footer>

    <script src="/assets/js/nav.js" defer></script>
    <script src="/assets/js/heading-anchors.js" defer></script>
    <!-- https://cdn.jsdelivr.net/gh/lurongkai/anti-baidu/js/anti-baidu-latest.min.js -->    
    <script type="text/javascript" src="/js/anti-baidu.min.js" charset="UTF-8"></script>
  </body>
</html>
