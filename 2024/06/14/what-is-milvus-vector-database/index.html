<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>What is Milvus? | CODE FARM</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="What is Milvus?" />
<meta property="og:locale" content="en" />
<meta name="description" content="Milvus (/ˈmɪlvəs/) is an open-source vector database to store, index, and manage massive embedding vectors generated by deep neural networks and machine learning (ML) models. [1] Unlike existing relational databases which mainly deal with structured data following a pre-defined pattern, Milvus is designed from the bottom-up to handle embedding vectors converted from unstructured data, including images, video, audio, and natural language. Embedding vectors or vectors, the output data format of Neural Network models, can effectively encode information and serve a pivotal role in AI applications such as knowledge base, semantic search, Retrieval Augmented Generation (RAG) and more. Mathematically speaking, an embedding vector is an array of floating-point numbers or binaries. Modern embedding techniques are used to convert unstructured data to embedding vectors. Milvus is able to analyze the correlation between two vectors by calculating their similarity distance. If the two embedding vectors are very similar, it means that the original data sources are similar as well. Vector similarity search is the process of comparing a vector to a database to find vectors that are most similar to the query vector. Approximate nearest neighbor (ANN) search algorithms are used to accelerate the searching process. If the two embedding vectors are very similar, it means that the original data sources are similar as well. Milvus adopts a shared-storage architecture featuring storage and computing disaggregation and horizontal scalability for its computing nodes. Following the principle of data plane and control plane disaggregation, Milvus comprises four layers: access layer, coordinator service, worker node, and storage. [2] 1. Install Milvus 1.1. Run Milvus with Docker Compose 1.2. Run Milvus Lite locally 1.3. Milvus Command-Line Interface (CLI) 2. Schema and collections 2.1. Load &amp; release collection 2.2. Dynamic field 3. Embeddings References 1. Install Milvus Milvus Lite is good for getting started with vector search or building demos and prototypes, and supports the following OS distributions and sillicon types: Ubuntu &gt;= 20.04 (x86_64), and macOS &gt;= 11.0 (Apple Silicon and x86_64), and Debian 12 (x86_64) on Windows with WSL 2 enabled. [4] [5] For a production use case, It&#8217;s recommended using Milvus on Docker and Kubenetes, or considering the fully-managed Milvus on Zilliz Cloud. All deployment modes of Milvus share the same API, so your client side code doesn&#8217;t need to change much if moving to another deployment mode. Simply specify the URI and Token of a Milvus server deployed anywhere: [5] from pymilvus import MilvusClient # Authentication not enabled client = MilvusClient(&quot;http://localhost:19530&quot;) # Authentication enabled with the root user client = MilvusClient( uri=&quot;http://localhost:19530&quot;, token=&quot;root:Milvus&quot;, db_name=&quot;default&quot; ) # Authentication enabled with a non-root user client = MilvusClient( uri=&quot;http://localhost:19530&quot;, token=&quot;user:password&quot;, # replace this with your token db_name=&quot;default&quot; ) Milvus provides REST and gRPC API, with client libraries in languages such as Python, Java, Go, C# and Node.js. 1.1. Run Milvus with Docker Compose Milvus provides a Docker Compose configuration file in the Milvus repository. To install Milvus using Docker Compose, just run [install_standalone-docker-compose] # Download the configuration file $ wget https://github.com/milvus-io/milvus/releases/download/v2.4.4/milvus-standalone-docker-compose.yml -O docker-compose.yml # Start Milvus $ sudo docker compose up -d Creating milvus-etcd ... done Creating milvus-minio ... done Creating milvus-standalone ... done After starting up Milvus, containers named milvus-standalone, milvus-minio, and milvus-etcd are up. The milvus-etcd container does not expose any ports to the host and maps its data to volumes/etcd in the current folder. The milvus-minio container serves ports 9090 and 9091 locally with the default authentication credentials and maps its data to volumes/minio in the current folder. The milvus-standalone container serves ports 19530 locally with the default settings and maps its data to volumes/milvus in the current folder. You can check if the containers are up and running using the following command: $ sudo docker compose ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------- milvus-etcd etcd -advertise-client-url ... Up 2379/tcp, 2380/tcp milvus-minio /usr/bin/docker-entrypoint ... Up (healthy) 9000/tcp milvus-standalone /tini -- milvus run standalone Up 0.0.0.0:19530-&gt;19530/tcp, 0.0.0.0:9091-&gt;9091/tcp You can stop and delete this container as follows # Stop Milvus $ sudo docker compose down # Delete service data $ sudo rm -rf volumes 1.2. Run Milvus Lite locally Milvus Lite is the lightweight version of Milvus included in the Python SDK of Milvus, which can be imported into a Python application, providing the core vector search functionality of Milvus. Install Milvus # set up Milvus Lite with pymilvus, the Python SDK library of Milvus pip install &quot;pymilvus&gt;=2.4.2&quot; Set up vector database # connect to Milvus Lite from pymilvus import MilvusClient # generate or load an existing vector database file named milvus_demo.db in the current folder client = MilvusClient(&quot;milvus_demo.db&quot;) Create a collection # create a collection to store vectors and their associated metadata client.create_collection( collection_name=&quot;demo_collection&quot;, dimension=768, # The vectors we will use in this demo has 768 dimensions ) The primary key and vector fields use their default names (&quot;id&quot; and &quot;vector&quot;). The metric type (vector distance definition) is set to its default value (COSINE). The primary key field accepts integers and does not automatically increments (namely not using auto-id feature) Represent text with vectors To perform semantic search on text, it&#8217;s needed to generate vectors for text by downloading embedding models, which can be easily done by using the utility functions from pymilvus[model] library including essential ML tools such as PyTorch. pip install &quot;pymilvus[model]&gt;=2.4.2&quot; Milvus expects data to be inserted organized as a list of dictionaries, where each dictionary represents a data record, termed as an entity. # generate vector embeddings with default model from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). embedding_fn = model.DefaultEmbeddingFunction() # Text strings to search from. docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] vectors = embedding_fn.encode_documents(docs) # The output vector has 768 dimensions, matching the collection that we just created. print(&quot;Dim:&quot;, embedding_fn.dim, vectors[0].shape) # Dim: 768 (768,) # Each entity has id, vector representation, raw text, and a subject label that we use # to demo metadata filtering later. data = [ {&quot;id&quot;: i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;history&quot;} for i in range(len(vectors)) ] print(&quot;Data has&quot;, len(data), &quot;entities, each with fields: &quot;, data[0].keys()) print(&quot;Vector dim:&quot;, len(data[0][&quot;vector&quot;])) Dim: 768 (768,) Data has 3 entities, each with fields: dict_keys([&#39;id&#39;, &#39;vector&#39;, &#39;text&#39;, &#39;subject&#39;]) Vector dim: 768 Insert data into the collection. res = client.insert(collection_name=&quot;demo_collection&quot;, data=data) print(res) {&#39;insert_count&#39;: 3, &#39;ids&#39;: [0, 1, 2], &#39;cost&#39;: 0} Semantic search Milvus accepts one or multiple vector search requests as a list of vectors, where each vector is an array of float numbers, at the same time. # from pymilvus import MilvusClient, model # # client = MilvusClient(&quot;milvus_demo.db&quot;) # # # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # # # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). # embedding_fn = model.DefaultEmbeddingFunction() query_vectors = embedding_fn.encode_queries([&quot;Who is Alan Turing?&quot;]) res = client.search( collection_name=&quot;demo_collection&quot;, # target collection data=query_vectors, # query vectors limit=2, # number of returned entities output_fields=[&quot;text&quot;, &quot;subject&quot;], # specifies fields to be returned ) print(res) data: [&quot;[{&#39;id&#39;: 2, &#39;distance&#39;: 0.5859944820404053, &#39;entity&#39;: {&#39;text&#39;: &#39;Born in Maida Vale, London, Turing was raised in southern England.&#39;, &#39;subject&#39;: &#39;history&#39;}}, {&#39;id&#39;: 1, &#39;distance&#39;: 0.5118255019187927, &#39;entity&#39;: {&#39;text&#39;: &#39;Alan Turing was the first person to conduct substantial research in AI.&#39;, &#39;subject&#39;: &#39;history&#39;}}]&quot;] , extra_info: {&#39;cost&#39;: 0} # Vector search with metadata filtering # Insert more docs in another subject. docs = [ &quot;Machine learning has been used for drug design.&quot;, &quot;Computational synthesis with AI algorithms predicts molecular properties.&quot;, &quot;DDR1 is involved in cancers and fibrosis.&quot;, ] vectors = embedding_fn.encode_documents(docs) data = [ {&quot;id&quot;: 3 + i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;biology&quot;} for i in range(len(vectors)) ] client.insert(collection_name=&quot;demo_collection&quot;, data=data) # This will exclude any text in &quot;history&quot; subject despite close to the query vector. res = client.search( collection_name=&quot;demo_collection&quot;, data=embedding_fn.encode_queries([&quot;tell me AI related information&quot;]), filter=&quot;subject == &#39;biology&#39;&quot;, limit=2, output_fields=[&quot;text&quot;, &quot;subject&quot;], ) print(res) data: [&quot;[{&#39;id&#39;: 4, &#39;distance&#39;: 0.27030572295188904, &#39;entity&#39;: {&#39;text&#39;: &#39;Computational synthesis with AI algorithms predicts molecular properties.&#39;, &#39;subject&#39;: &#39;biology&#39;}}, {&#39;id&#39;: 3, &#39;distance&#39;: 0.1642588973045349, &#39;entity&#39;: {&#39;text&#39;: &#39;Machine learning has been used for drug design.&#39;, &#39;subject&#39;: &#39;biology&#39;}}]&quot;] , extra_info: {&#39;cost&#39;: 0} A query() is an operation that retrieves all entities matching a cretria, such as a filter expression or matching some ids. # retrieving all entities whose scalar field has a particular value res = client.query( collection_name=&quot;demo_collection&quot;, filter=&quot;subject == &#39;history&#39;&quot;, output_fields=[&quot;text&quot;, &quot;subject&quot;], ) # retrieving entities by primary key directly res = client.query( collection_name=&quot;demo_collection&quot;, ids=[0, 2], output_fields=[&quot;vector&quot;, &quot;text&quot;, &quot;subject&quot;], ) Delete entities specifying the primary key or delete all entities matching a particular filter expression. # Delete entities by primary key res = client.delete(collection_name=&quot;demo_collection&quot;, ids=[0, 2]) print(res) # Delete entities by a filter expression res = client.delete( collection_name=&quot;demo_collection&quot;, filter=&quot;subject == &#39;biology&#39;&quot;, ) print(res) # Drop collection client.drop_collection(collection_name=&quot;demo_collection&quot;) [0, 2] [3, 4, 5] 1.3. Milvus Command-Line Interface (CLI) Milvus Command-Line Interface (CLI), based on Milvus Python SDK, is a command-line tool that supports database connection, data operations, and import and export of data. [6] Install via pip pip install milvus-cli Install with Docker docker run -it zilliz/milvus_cli:latest Commands milvus_cli &gt; connect -uri http://127.0.0.1:19530 milvus_cli &gt; create database -db testdb milvus_cli &gt; list databases milvus_cli &gt; use database -db testdb milvus_cli &gt; list collections milvus_cli &gt; show collection -c test_collection_insert milvus_cli &gt; list connections milvus_cli &gt; search Collection name (car, test_collection): car The vectors of search data(the length of data is number of query (nq), the dim of every vector in data must be equal to vector field’s of collection. You can also import a csv file out headers): examples/import_csv/search_vectors.csv The vector field used to search of collection (vector): vector Metric type: L2 Search parameter nprobe&#39;s value: 10 The max number of returned record, also known as topk: 2 The boolean expression used to filter attribute []: id &gt; 0 The names of partitions to search (split by &quot;,&quot; if multiple) [&#39;_default&#39;] []: _default timeout []: Guarantee Timestamp(It instructs Milvus to see all operations performed before a provided timestamp. If no such timestamp is provided, then Milvus will search all operations performed to date) [0]: 2. Schema and collections In Milvus, schema is used to define the properties of a collection and the fields within. [7] A field schema is the logical definition of a field, and Milvus supports only one primary key field in a collection. To reduce the complexity in data inserts, Milvus allows to specify a default value for each scalar field during field schema creation, excluding the primary key field. Create a regular field schema: from pymilvus import FieldSchema id_field = FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True, description=&quot;primary id&quot;) age_field = FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, description=&quot;age&quot;) embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) # The following creates a field and use it as the partition key position_field = FieldSchema(name=&quot;position&quot;, dtype=DataType.VARCHAR, max_length=256, is_partition_key=True) Create a field schema with default field values: from pymilvus import FieldSchema fields = [ FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True), # configure default value `25` for field `age` FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, default_value=25, description=&quot;age&quot;), embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) ] A collection schema is the logical definition of a collection. Define the field schemas before defining a collection schema. Create a collection schema from pymilvus import FieldSchema, CollectionSchema id_field = FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True, description=&quot;primary id&quot;) age_field = FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, description=&quot;age&quot;) embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) # Enable partition key on a field if you need to implement multi-tenancy based on the partition-key field position_field = FieldSchema(name=&quot;position&quot;, dtype=DataType.VARCHAR, max_length=256, is_partition_key=True) # Set enable_dynamic_field to True if you need to use dynamic fields. schema = CollectionSchema(fields=[id_field, age_field, embedding_field], auto_id=False, enable_dynamic_field=True, description=&quot;desc of a collection&quot;) Enable dynamic schema by setting enable_dynamic_field to True in the collection schema. Create a collection with the schema specified: from pymilvus import Collection collection_name1 = &quot;tutorial_1&quot; collection1 = Collection(name=collection_name1, schema=schema, using=&#39;default&#39;, shards_num=2) 2.1. Load &amp; release collection Before conducting searches in a collection, ensure that the collection is loaded. During the loading process of a collection, Milvus loads the collection&#8217;s index file into memory. Conversely, when releasing a collection, Milvus unloads the index file from memory. [8] To load a collection, use the load_collection() method, specifying the collection name. # Load the collection client.load_collection( collection_name=&quot;customized_setup_2&quot;, replica_number=1 # Number of replicas to create on query nodes. Max value is 1 for Milvus Standalone, and no greater than `queryNode.replicas` for Milvus Cluster. ) res = client.get_load_state( collection_name=&quot;customized_setup_2&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: Loaded&gt;&quot; # } To release a collection, use the release_collection() method, specifying the collection name. # Release the collection client.release_collection( collection_name=&quot;customized_setup_2&quot; ) res = client.get_load_state( collection_name=&quot;customized_setup_2&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: NotLoad&gt;&quot; # } 2.2. Dynamic field The dynamic field in a collection is a reserved JSON field named $meta. It can hold non-schema-defined fields and their values as key-value pairs. Using the dynamic field, search and query both schema-defined fields and any non-schema-defined fields they may have. Enable dynamic field When defining a schema for a collection, set enable_dynamic_field to True to enable the reserved dynamic field, indicating that any non-schema-defined fields and their values inserted later on will be saved as key-value pairs in the reserved dynamic field. import random, time from pymilvus import connections, MilvusClient, DataType SERVER_ADDR = &quot;http://localhost:19530&quot; # 1. Set up a Milvus client client = MilvusClient( uri=SERVER_ADDR ) # 2. Create a collection schema = MilvusClient.create_schema( auto_id=False, # highlight-next-line enable_dynamic_field=True, ) schema.add_field(field_name=&quot;id&quot;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=&quot;vector&quot;, datatype=DataType.FLOAT_VECTOR, dim=5) index_params = MilvusClient.prepare_index_params() index_params.add_index( field_name=&quot;id&quot;, index_type=&quot;STL_SORT&quot; ) index_params.add_index( field_name=&quot;vector&quot;, index_type=&quot;IVF_FLAT&quot;, metric_type=&quot;L2&quot;, params={&quot;nlist&quot;: 1024} ) client.create_collection( collection_name=&quot;test_collection&quot;, schema=schema, index_params=index_params ) res = client.get_load_state( collection_name=&quot;test_collection&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: Loaded&gt;&quot; # } # check the details of the collection. res = client.describe_collection( collection_name=&quot;test_collection&quot; ) print(res) # Output # # { # &quot;collection_name&quot;: &quot;test_collection&quot;, # &quot;auto_id&quot;: false, # &quot;num_shards&quot;: 1, # &quot;description&quot;: &quot;&quot;, # &quot;fields&quot;: [ # { # &quot;field_id&quot;: 100, # &quot;name&quot;: &quot;id&quot;, # &quot;description&quot;: &quot;&quot;, # &quot;type&quot;: 5, # &quot;params&quot;: {}, # &quot;is_primary&quot;: true # }, # { # &quot;field_id&quot;: 101, # &quot;name&quot;: &quot;vector&quot;, # &quot;description&quot;: &quot;&quot;, # &quot;type&quot;: 101, # &quot;params&quot;: { # &quot;dim&quot;: 5 # } # } # ], # &quot;aliases&quot;: [], # &quot;collection_id&quot;: 450568843971279780, # &quot;consistency_level&quot;: 2, # &quot;properties&quot;: {}, # &quot;num_partitions&quot;: 1, # &quot;enable_dynamic_field&quot;: true # } Insert dynamic data Prepare some randomly generated data for the insertion later on. colors = [&quot;green&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;black&quot;, &quot;white&quot;, &quot;purple&quot;, &quot;pink&quot;, &quot;orange&quot;, &quot;brown&quot;, &quot;grey&quot;] data = [] for i in range(1000): current_color = random.choice(colors) current_tag = random.randint(1000, 9999) data.append({ &quot;id&quot;: i, &quot;vector&quot;: [ random.uniform(-1, 1) for _ in range(5) ], &quot;color&quot;: current_color, &quot;tag&quot;: current_tag, &quot;color_tag&quot;: f&quot;{current_color}_{str(current_tag)}&quot; }) print(data[0]) Insert the data into the collection. res = client.insert( collection_name=&quot;test_collection&quot;, data=data, ) print(res) # Output # # { # &quot;insert_count&quot;: 1000, # &quot;ids&quot;: [ # 0, # 1, # 2, # 3, # 4, # 5, # 6, # 7, # 8, # 9, # &quot;(990 more items hidden)&quot; # ] # } time.sleep(5) Search with dynamic fields # 4. Search with dynamic fields query_vectors = [[0.3580376395471989, -0.6023495712049978, 0.18414012509913835, -0.26286205330961354, 0.9029438446296592]] res = client.search( collection_name=&quot;test_collection&quot;, data=query_vectors, filter=&quot;color in [\&quot;red\&quot;, \&quot;green\&quot;]&quot;, search_params={&quot;metric_type&quot;: &quot;L2&quot;, &quot;params&quot;: {&quot;nprobe&quot;: 10}}, limit=3 ) print(res) # Output # # [ # [ # { # &quot;id&quot;: 863, # &quot;distance&quot;: 0.188413605093956, # &quot;entity&quot;: { # &quot;id&quot;: 863, # &quot;color_tag&quot;: &quot;red_2371&quot; # } # }, # { # &quot;id&quot;: 799, # &quot;distance&quot;: 0.29188022017478943, # &quot;entity&quot;: { # &quot;id&quot;: 799, # &quot;color_tag&quot;: &quot;red_2235&quot; # } # }, # { # &quot;id&quot;: 564, # &quot;distance&quot;: 0.3492690920829773, # &quot;entity&quot;: { # &quot;id&quot;: 564, # &quot;color_tag&quot;: &quot;red_9186&quot; # } # } # ] # ] 3. Embeddings Embedding is a machine learning concept for mapping data into a high-dimensional space, where data of similar semantic are placed close together. [9] Typically being a Deep Neural Network from BERT or other Transformer families, the embedding model can effectively represent the semantics of text, images, and other data types with a series of numbers known as vectors. A key feature of these models is that the mathematical distance between vectors in the high-dimensional space can indicate the similarity of the semantics of original text or images, that unlocks many information retrieval applications, such as web search engines like Google and Bing, product search and recommendations on e-commerce sites, and the recently popular Retrieval Augmented Generation (RAG) paradigm in generative AI. There are two main categories of embeddings, each producing a different type of vector: Dense embedding: Most embedding models represent information as a floating point vector of hundreds to thousands of dimensions. The output is called &quot;dense&quot; vectors as most dimensions have non-zero values. Dense embedding is a technique used in natural language processing to represent words or phrases as continuous, dense vectors in a high-dimensional space, capturing semantic relationships. For instance, the popular open-source embedding model BAAI/bge-base-en-v1.5 outputs vectors of 768 floating point numbers (768-dimension float vector). Sparse embedding: In contrast, the output vectors of sparse embeddings has most dimensions being zero, namely &quot;sparse&quot; vectors. These vectors often have much higher dimensions (tens of thousands or more) which is determined by the size of the token vocabulary. Sparse vectors can be generated by Deep Neural Networks or statistical analysis of text corpora. Due to their interpretability and observed better out-of-domain generalization capabilities, sparse embeddings are increasingly adopted by developers as a complement to dense embeddings. Milvus is a vector database designed for vector data management, storage, and retrieval. By integrating mainstream embedding and reranking models, it can easily transform original text into searchable vectors or rerank the results using powerful models to achieve more accurate results for RAG, and simplifies text transformation and eliminates the need for additional embedding or reranking components, thereby streamlining RAG development and validation. To use embedding functions with Milvus, first install the PyMilvus client library with the model subpackage that wraps all the utilities for embedding generation. pip install pymilvus[model] # or pip install &quot;pymilvus[model]&quot; for zsh. # or pipenv install &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; The model subpackage supports various embedding models, from OpenAI, Sentence Transformers, BGE M3, BM25, to SPLADE pretrained models. Use default embedding function to generate dense vectors from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). embedding_fn = model.DefaultEmbeddingFunction() # Text strings to search from. docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] vectors = embedding_fn.encode_documents(docs) # The output vector has 768 dimensions, matching the collection that we just created. print(&quot;Dim:&quot;, embedding_fn.dim, vectors[0].shape) # Dim: 768 (768,) # Each entity has id, vector representation, raw text, and a subject label that we use # to demo metadata filtering later. data = [ {&quot;id&quot;: i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;history&quot;} for i in range(len(vectors)) ] print(&quot;Data has&quot;, len(data), &quot;entities, each with fields: &quot;, data[0].keys()) print(&quot;Vector dim:&quot;, len(data[0][&quot;vector&quot;])) Dim: 768 (768,) Data has 3 entities, each with fields: dict_keys([&#39;id&#39;, &#39;vector&#39;, &#39;text&#39;, &#39;subject&#39;]) Vector dim: 768 # To create embeddings for queries, use the encode_queries() method: query_vectors = embedding_fn.encode_queries([&quot;Who is Alan Turing?&quot;]) Use sentence transformer embedding function to generate dense vectors with Sentence Transformer pre-trained models pip install sentence_transformers # (optional) install sentence_transformers manually from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; sentence_transformer_ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, # Specify the model name device=&#39;cpu&#39; # Specify the device to use, e.g., &#39;cpu&#39;, &#39;cuda:0&#39;. If None, checks if a GPU can be used. ) docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] docs_embeddings = sentence_transformer_ef.encode_documents(docs) # Print embeddings print(&quot;Embeddings:&quot;, docs_embeddings) # Print dimension and shape of embeddings print(&quot;Dim:&quot;, sentence_transformer_ef.dim, docs_embeddings[0].shape) Embeddings: [array([-3.09392996e-02, -1.80662833e-02, 1.34775648e-02, 2.77156215e-02, -4.86349640e-03, -3.12581174e-02, -3.55921760e-02, 5.76934684e-03, 2.80773244e-03, 1.35783911e-01, 3.59678417e-02, 6.17732145e-02, ... -4.61330153e-02, -4.85207550e-02, 3.13997865e-02, 7.82178566e-02, -4.75336798e-02, 5.21207601e-02, 9.04406682e-02, -5.36676683e-02], dtype=float32)] Dim: 384 (384,) # To create embeddings for queries, use the encode_queries() method: queries = [&quot;When was artificial intelligence founded&quot;, &quot;Where was Alan Turing born?&quot;] query_embeddings = sentence_transformer_ef.encode_queries(queries) # Print embeddings print(&quot;Embeddings:&quot;, query_embeddings) # Print dimension and shape of embeddings print(&quot;Dim:&quot;, sentence_transformer_ef.dim, query_embeddings[0].shape) Embeddings: [array([-2.52114702e-02, -5.29330298e-02, 1.14570223e-02, 1.95571519e-02, -2.46500354e-02, -2.66519729e-02, -8.48201662e-03, 2.82961670e-02, -3.65092754e-02, 7.50745758e-02, 4.28900979e-02, 7.18822703e-02, ... -6.76431581e-02, -6.45996556e-02, -4.67132553e-02, 4.78532910e-02, -2.31596199e-03, 4.13446948e-02, 1.06935494e-01, -1.08258888e-01], dtype=float32)] Dim: 384 (384,) References [1] https://milvus.io/docs/overview.md [2] https://milvus.io/docs/architecture_overview.md [3] https://milvus.io/docs/install_standalone-docker-compose.md [4] https://milvus.io/docs/milvus_lite.md [5] https://milvus.io/docs/quickstart.md [6] https://milvus.io/docs/cli_overview.md [7] https://milvus.io/docs/schema.md [8] https://milvus.io/docs/manage-collections.md [9] https://milvus.io/docs/embeddings.md" />
<meta property="og:description" content="Milvus (/ˈmɪlvəs/) is an open-source vector database to store, index, and manage massive embedding vectors generated by deep neural networks and machine learning (ML) models. [1] Unlike existing relational databases which mainly deal with structured data following a pre-defined pattern, Milvus is designed from the bottom-up to handle embedding vectors converted from unstructured data, including images, video, audio, and natural language. Embedding vectors or vectors, the output data format of Neural Network models, can effectively encode information and serve a pivotal role in AI applications such as knowledge base, semantic search, Retrieval Augmented Generation (RAG) and more. Mathematically speaking, an embedding vector is an array of floating-point numbers or binaries. Modern embedding techniques are used to convert unstructured data to embedding vectors. Milvus is able to analyze the correlation between two vectors by calculating their similarity distance. If the two embedding vectors are very similar, it means that the original data sources are similar as well. Vector similarity search is the process of comparing a vector to a database to find vectors that are most similar to the query vector. Approximate nearest neighbor (ANN) search algorithms are used to accelerate the searching process. If the two embedding vectors are very similar, it means that the original data sources are similar as well. Milvus adopts a shared-storage architecture featuring storage and computing disaggregation and horizontal scalability for its computing nodes. Following the principle of data plane and control plane disaggregation, Milvus comprises four layers: access layer, coordinator service, worker node, and storage. [2] 1. Install Milvus 1.1. Run Milvus with Docker Compose 1.2. Run Milvus Lite locally 1.3. Milvus Command-Line Interface (CLI) 2. Schema and collections 2.1. Load &amp; release collection 2.2. Dynamic field 3. Embeddings References 1. Install Milvus Milvus Lite is good for getting started with vector search or building demos and prototypes, and supports the following OS distributions and sillicon types: Ubuntu &gt;= 20.04 (x86_64), and macOS &gt;= 11.0 (Apple Silicon and x86_64), and Debian 12 (x86_64) on Windows with WSL 2 enabled. [4] [5] For a production use case, It&#8217;s recommended using Milvus on Docker and Kubenetes, or considering the fully-managed Milvus on Zilliz Cloud. All deployment modes of Milvus share the same API, so your client side code doesn&#8217;t need to change much if moving to another deployment mode. Simply specify the URI and Token of a Milvus server deployed anywhere: [5] from pymilvus import MilvusClient # Authentication not enabled client = MilvusClient(&quot;http://localhost:19530&quot;) # Authentication enabled with the root user client = MilvusClient( uri=&quot;http://localhost:19530&quot;, token=&quot;root:Milvus&quot;, db_name=&quot;default&quot; ) # Authentication enabled with a non-root user client = MilvusClient( uri=&quot;http://localhost:19530&quot;, token=&quot;user:password&quot;, # replace this with your token db_name=&quot;default&quot; ) Milvus provides REST and gRPC API, with client libraries in languages such as Python, Java, Go, C# and Node.js. 1.1. Run Milvus with Docker Compose Milvus provides a Docker Compose configuration file in the Milvus repository. To install Milvus using Docker Compose, just run [install_standalone-docker-compose] # Download the configuration file $ wget https://github.com/milvus-io/milvus/releases/download/v2.4.4/milvus-standalone-docker-compose.yml -O docker-compose.yml # Start Milvus $ sudo docker compose up -d Creating milvus-etcd ... done Creating milvus-minio ... done Creating milvus-standalone ... done After starting up Milvus, containers named milvus-standalone, milvus-minio, and milvus-etcd are up. The milvus-etcd container does not expose any ports to the host and maps its data to volumes/etcd in the current folder. The milvus-minio container serves ports 9090 and 9091 locally with the default authentication credentials and maps its data to volumes/minio in the current folder. The milvus-standalone container serves ports 19530 locally with the default settings and maps its data to volumes/milvus in the current folder. You can check if the containers are up and running using the following command: $ sudo docker compose ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------- milvus-etcd etcd -advertise-client-url ... Up 2379/tcp, 2380/tcp milvus-minio /usr/bin/docker-entrypoint ... Up (healthy) 9000/tcp milvus-standalone /tini -- milvus run standalone Up 0.0.0.0:19530-&gt;19530/tcp, 0.0.0.0:9091-&gt;9091/tcp You can stop and delete this container as follows # Stop Milvus $ sudo docker compose down # Delete service data $ sudo rm -rf volumes 1.2. Run Milvus Lite locally Milvus Lite is the lightweight version of Milvus included in the Python SDK of Milvus, which can be imported into a Python application, providing the core vector search functionality of Milvus. Install Milvus # set up Milvus Lite with pymilvus, the Python SDK library of Milvus pip install &quot;pymilvus&gt;=2.4.2&quot; Set up vector database # connect to Milvus Lite from pymilvus import MilvusClient # generate or load an existing vector database file named milvus_demo.db in the current folder client = MilvusClient(&quot;milvus_demo.db&quot;) Create a collection # create a collection to store vectors and their associated metadata client.create_collection( collection_name=&quot;demo_collection&quot;, dimension=768, # The vectors we will use in this demo has 768 dimensions ) The primary key and vector fields use their default names (&quot;id&quot; and &quot;vector&quot;). The metric type (vector distance definition) is set to its default value (COSINE). The primary key field accepts integers and does not automatically increments (namely not using auto-id feature) Represent text with vectors To perform semantic search on text, it&#8217;s needed to generate vectors for text by downloading embedding models, which can be easily done by using the utility functions from pymilvus[model] library including essential ML tools such as PyTorch. pip install &quot;pymilvus[model]&gt;=2.4.2&quot; Milvus expects data to be inserted organized as a list of dictionaries, where each dictionary represents a data record, termed as an entity. # generate vector embeddings with default model from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). embedding_fn = model.DefaultEmbeddingFunction() # Text strings to search from. docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] vectors = embedding_fn.encode_documents(docs) # The output vector has 768 dimensions, matching the collection that we just created. print(&quot;Dim:&quot;, embedding_fn.dim, vectors[0].shape) # Dim: 768 (768,) # Each entity has id, vector representation, raw text, and a subject label that we use # to demo metadata filtering later. data = [ {&quot;id&quot;: i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;history&quot;} for i in range(len(vectors)) ] print(&quot;Data has&quot;, len(data), &quot;entities, each with fields: &quot;, data[0].keys()) print(&quot;Vector dim:&quot;, len(data[0][&quot;vector&quot;])) Dim: 768 (768,) Data has 3 entities, each with fields: dict_keys([&#39;id&#39;, &#39;vector&#39;, &#39;text&#39;, &#39;subject&#39;]) Vector dim: 768 Insert data into the collection. res = client.insert(collection_name=&quot;demo_collection&quot;, data=data) print(res) {&#39;insert_count&#39;: 3, &#39;ids&#39;: [0, 1, 2], &#39;cost&#39;: 0} Semantic search Milvus accepts one or multiple vector search requests as a list of vectors, where each vector is an array of float numbers, at the same time. # from pymilvus import MilvusClient, model # # client = MilvusClient(&quot;milvus_demo.db&quot;) # # # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # # # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). # embedding_fn = model.DefaultEmbeddingFunction() query_vectors = embedding_fn.encode_queries([&quot;Who is Alan Turing?&quot;]) res = client.search( collection_name=&quot;demo_collection&quot;, # target collection data=query_vectors, # query vectors limit=2, # number of returned entities output_fields=[&quot;text&quot;, &quot;subject&quot;], # specifies fields to be returned ) print(res) data: [&quot;[{&#39;id&#39;: 2, &#39;distance&#39;: 0.5859944820404053, &#39;entity&#39;: {&#39;text&#39;: &#39;Born in Maida Vale, London, Turing was raised in southern England.&#39;, &#39;subject&#39;: &#39;history&#39;}}, {&#39;id&#39;: 1, &#39;distance&#39;: 0.5118255019187927, &#39;entity&#39;: {&#39;text&#39;: &#39;Alan Turing was the first person to conduct substantial research in AI.&#39;, &#39;subject&#39;: &#39;history&#39;}}]&quot;] , extra_info: {&#39;cost&#39;: 0} # Vector search with metadata filtering # Insert more docs in another subject. docs = [ &quot;Machine learning has been used for drug design.&quot;, &quot;Computational synthesis with AI algorithms predicts molecular properties.&quot;, &quot;DDR1 is involved in cancers and fibrosis.&quot;, ] vectors = embedding_fn.encode_documents(docs) data = [ {&quot;id&quot;: 3 + i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;biology&quot;} for i in range(len(vectors)) ] client.insert(collection_name=&quot;demo_collection&quot;, data=data) # This will exclude any text in &quot;history&quot; subject despite close to the query vector. res = client.search( collection_name=&quot;demo_collection&quot;, data=embedding_fn.encode_queries([&quot;tell me AI related information&quot;]), filter=&quot;subject == &#39;biology&#39;&quot;, limit=2, output_fields=[&quot;text&quot;, &quot;subject&quot;], ) print(res) data: [&quot;[{&#39;id&#39;: 4, &#39;distance&#39;: 0.27030572295188904, &#39;entity&#39;: {&#39;text&#39;: &#39;Computational synthesis with AI algorithms predicts molecular properties.&#39;, &#39;subject&#39;: &#39;biology&#39;}}, {&#39;id&#39;: 3, &#39;distance&#39;: 0.1642588973045349, &#39;entity&#39;: {&#39;text&#39;: &#39;Machine learning has been used for drug design.&#39;, &#39;subject&#39;: &#39;biology&#39;}}]&quot;] , extra_info: {&#39;cost&#39;: 0} A query() is an operation that retrieves all entities matching a cretria, such as a filter expression or matching some ids. # retrieving all entities whose scalar field has a particular value res = client.query( collection_name=&quot;demo_collection&quot;, filter=&quot;subject == &#39;history&#39;&quot;, output_fields=[&quot;text&quot;, &quot;subject&quot;], ) # retrieving entities by primary key directly res = client.query( collection_name=&quot;demo_collection&quot;, ids=[0, 2], output_fields=[&quot;vector&quot;, &quot;text&quot;, &quot;subject&quot;], ) Delete entities specifying the primary key or delete all entities matching a particular filter expression. # Delete entities by primary key res = client.delete(collection_name=&quot;demo_collection&quot;, ids=[0, 2]) print(res) # Delete entities by a filter expression res = client.delete( collection_name=&quot;demo_collection&quot;, filter=&quot;subject == &#39;biology&#39;&quot;, ) print(res) # Drop collection client.drop_collection(collection_name=&quot;demo_collection&quot;) [0, 2] [3, 4, 5] 1.3. Milvus Command-Line Interface (CLI) Milvus Command-Line Interface (CLI), based on Milvus Python SDK, is a command-line tool that supports database connection, data operations, and import and export of data. [6] Install via pip pip install milvus-cli Install with Docker docker run -it zilliz/milvus_cli:latest Commands milvus_cli &gt; connect -uri http://127.0.0.1:19530 milvus_cli &gt; create database -db testdb milvus_cli &gt; list databases milvus_cli &gt; use database -db testdb milvus_cli &gt; list collections milvus_cli &gt; show collection -c test_collection_insert milvus_cli &gt; list connections milvus_cli &gt; search Collection name (car, test_collection): car The vectors of search data(the length of data is number of query (nq), the dim of every vector in data must be equal to vector field’s of collection. You can also import a csv file out headers): examples/import_csv/search_vectors.csv The vector field used to search of collection (vector): vector Metric type: L2 Search parameter nprobe&#39;s value: 10 The max number of returned record, also known as topk: 2 The boolean expression used to filter attribute []: id &gt; 0 The names of partitions to search (split by &quot;,&quot; if multiple) [&#39;_default&#39;] []: _default timeout []: Guarantee Timestamp(It instructs Milvus to see all operations performed before a provided timestamp. If no such timestamp is provided, then Milvus will search all operations performed to date) [0]: 2. Schema and collections In Milvus, schema is used to define the properties of a collection and the fields within. [7] A field schema is the logical definition of a field, and Milvus supports only one primary key field in a collection. To reduce the complexity in data inserts, Milvus allows to specify a default value for each scalar field during field schema creation, excluding the primary key field. Create a regular field schema: from pymilvus import FieldSchema id_field = FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True, description=&quot;primary id&quot;) age_field = FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, description=&quot;age&quot;) embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) # The following creates a field and use it as the partition key position_field = FieldSchema(name=&quot;position&quot;, dtype=DataType.VARCHAR, max_length=256, is_partition_key=True) Create a field schema with default field values: from pymilvus import FieldSchema fields = [ FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True), # configure default value `25` for field `age` FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, default_value=25, description=&quot;age&quot;), embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) ] A collection schema is the logical definition of a collection. Define the field schemas before defining a collection schema. Create a collection schema from pymilvus import FieldSchema, CollectionSchema id_field = FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True, description=&quot;primary id&quot;) age_field = FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, description=&quot;age&quot;) embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) # Enable partition key on a field if you need to implement multi-tenancy based on the partition-key field position_field = FieldSchema(name=&quot;position&quot;, dtype=DataType.VARCHAR, max_length=256, is_partition_key=True) # Set enable_dynamic_field to True if you need to use dynamic fields. schema = CollectionSchema(fields=[id_field, age_field, embedding_field], auto_id=False, enable_dynamic_field=True, description=&quot;desc of a collection&quot;) Enable dynamic schema by setting enable_dynamic_field to True in the collection schema. Create a collection with the schema specified: from pymilvus import Collection collection_name1 = &quot;tutorial_1&quot; collection1 = Collection(name=collection_name1, schema=schema, using=&#39;default&#39;, shards_num=2) 2.1. Load &amp; release collection Before conducting searches in a collection, ensure that the collection is loaded. During the loading process of a collection, Milvus loads the collection&#8217;s index file into memory. Conversely, when releasing a collection, Milvus unloads the index file from memory. [8] To load a collection, use the load_collection() method, specifying the collection name. # Load the collection client.load_collection( collection_name=&quot;customized_setup_2&quot;, replica_number=1 # Number of replicas to create on query nodes. Max value is 1 for Milvus Standalone, and no greater than `queryNode.replicas` for Milvus Cluster. ) res = client.get_load_state( collection_name=&quot;customized_setup_2&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: Loaded&gt;&quot; # } To release a collection, use the release_collection() method, specifying the collection name. # Release the collection client.release_collection( collection_name=&quot;customized_setup_2&quot; ) res = client.get_load_state( collection_name=&quot;customized_setup_2&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: NotLoad&gt;&quot; # } 2.2. Dynamic field The dynamic field in a collection is a reserved JSON field named $meta. It can hold non-schema-defined fields and their values as key-value pairs. Using the dynamic field, search and query both schema-defined fields and any non-schema-defined fields they may have. Enable dynamic field When defining a schema for a collection, set enable_dynamic_field to True to enable the reserved dynamic field, indicating that any non-schema-defined fields and their values inserted later on will be saved as key-value pairs in the reserved dynamic field. import random, time from pymilvus import connections, MilvusClient, DataType SERVER_ADDR = &quot;http://localhost:19530&quot; # 1. Set up a Milvus client client = MilvusClient( uri=SERVER_ADDR ) # 2. Create a collection schema = MilvusClient.create_schema( auto_id=False, # highlight-next-line enable_dynamic_field=True, ) schema.add_field(field_name=&quot;id&quot;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=&quot;vector&quot;, datatype=DataType.FLOAT_VECTOR, dim=5) index_params = MilvusClient.prepare_index_params() index_params.add_index( field_name=&quot;id&quot;, index_type=&quot;STL_SORT&quot; ) index_params.add_index( field_name=&quot;vector&quot;, index_type=&quot;IVF_FLAT&quot;, metric_type=&quot;L2&quot;, params={&quot;nlist&quot;: 1024} ) client.create_collection( collection_name=&quot;test_collection&quot;, schema=schema, index_params=index_params ) res = client.get_load_state( collection_name=&quot;test_collection&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: Loaded&gt;&quot; # } # check the details of the collection. res = client.describe_collection( collection_name=&quot;test_collection&quot; ) print(res) # Output # # { # &quot;collection_name&quot;: &quot;test_collection&quot;, # &quot;auto_id&quot;: false, # &quot;num_shards&quot;: 1, # &quot;description&quot;: &quot;&quot;, # &quot;fields&quot;: [ # { # &quot;field_id&quot;: 100, # &quot;name&quot;: &quot;id&quot;, # &quot;description&quot;: &quot;&quot;, # &quot;type&quot;: 5, # &quot;params&quot;: {}, # &quot;is_primary&quot;: true # }, # { # &quot;field_id&quot;: 101, # &quot;name&quot;: &quot;vector&quot;, # &quot;description&quot;: &quot;&quot;, # &quot;type&quot;: 101, # &quot;params&quot;: { # &quot;dim&quot;: 5 # } # } # ], # &quot;aliases&quot;: [], # &quot;collection_id&quot;: 450568843971279780, # &quot;consistency_level&quot;: 2, # &quot;properties&quot;: {}, # &quot;num_partitions&quot;: 1, # &quot;enable_dynamic_field&quot;: true # } Insert dynamic data Prepare some randomly generated data for the insertion later on. colors = [&quot;green&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;black&quot;, &quot;white&quot;, &quot;purple&quot;, &quot;pink&quot;, &quot;orange&quot;, &quot;brown&quot;, &quot;grey&quot;] data = [] for i in range(1000): current_color = random.choice(colors) current_tag = random.randint(1000, 9999) data.append({ &quot;id&quot;: i, &quot;vector&quot;: [ random.uniform(-1, 1) for _ in range(5) ], &quot;color&quot;: current_color, &quot;tag&quot;: current_tag, &quot;color_tag&quot;: f&quot;{current_color}_{str(current_tag)}&quot; }) print(data[0]) Insert the data into the collection. res = client.insert( collection_name=&quot;test_collection&quot;, data=data, ) print(res) # Output # # { # &quot;insert_count&quot;: 1000, # &quot;ids&quot;: [ # 0, # 1, # 2, # 3, # 4, # 5, # 6, # 7, # 8, # 9, # &quot;(990 more items hidden)&quot; # ] # } time.sleep(5) Search with dynamic fields # 4. Search with dynamic fields query_vectors = [[0.3580376395471989, -0.6023495712049978, 0.18414012509913835, -0.26286205330961354, 0.9029438446296592]] res = client.search( collection_name=&quot;test_collection&quot;, data=query_vectors, filter=&quot;color in [\&quot;red\&quot;, \&quot;green\&quot;]&quot;, search_params={&quot;metric_type&quot;: &quot;L2&quot;, &quot;params&quot;: {&quot;nprobe&quot;: 10}}, limit=3 ) print(res) # Output # # [ # [ # { # &quot;id&quot;: 863, # &quot;distance&quot;: 0.188413605093956, # &quot;entity&quot;: { # &quot;id&quot;: 863, # &quot;color_tag&quot;: &quot;red_2371&quot; # } # }, # { # &quot;id&quot;: 799, # &quot;distance&quot;: 0.29188022017478943, # &quot;entity&quot;: { # &quot;id&quot;: 799, # &quot;color_tag&quot;: &quot;red_2235&quot; # } # }, # { # &quot;id&quot;: 564, # &quot;distance&quot;: 0.3492690920829773, # &quot;entity&quot;: { # &quot;id&quot;: 564, # &quot;color_tag&quot;: &quot;red_9186&quot; # } # } # ] # ] 3. Embeddings Embedding is a machine learning concept for mapping data into a high-dimensional space, where data of similar semantic are placed close together. [9] Typically being a Deep Neural Network from BERT or other Transformer families, the embedding model can effectively represent the semantics of text, images, and other data types with a series of numbers known as vectors. A key feature of these models is that the mathematical distance between vectors in the high-dimensional space can indicate the similarity of the semantics of original text or images, that unlocks many information retrieval applications, such as web search engines like Google and Bing, product search and recommendations on e-commerce sites, and the recently popular Retrieval Augmented Generation (RAG) paradigm in generative AI. There are two main categories of embeddings, each producing a different type of vector: Dense embedding: Most embedding models represent information as a floating point vector of hundreds to thousands of dimensions. The output is called &quot;dense&quot; vectors as most dimensions have non-zero values. Dense embedding is a technique used in natural language processing to represent words or phrases as continuous, dense vectors in a high-dimensional space, capturing semantic relationships. For instance, the popular open-source embedding model BAAI/bge-base-en-v1.5 outputs vectors of 768 floating point numbers (768-dimension float vector). Sparse embedding: In contrast, the output vectors of sparse embeddings has most dimensions being zero, namely &quot;sparse&quot; vectors. These vectors often have much higher dimensions (tens of thousands or more) which is determined by the size of the token vocabulary. Sparse vectors can be generated by Deep Neural Networks or statistical analysis of text corpora. Due to their interpretability and observed better out-of-domain generalization capabilities, sparse embeddings are increasingly adopted by developers as a complement to dense embeddings. Milvus is a vector database designed for vector data management, storage, and retrieval. By integrating mainstream embedding and reranking models, it can easily transform original text into searchable vectors or rerank the results using powerful models to achieve more accurate results for RAG, and simplifies text transformation and eliminates the need for additional embedding or reranking components, thereby streamlining RAG development and validation. To use embedding functions with Milvus, first install the PyMilvus client library with the model subpackage that wraps all the utilities for embedding generation. pip install pymilvus[model] # or pip install &quot;pymilvus[model]&quot; for zsh. # or pipenv install &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; The model subpackage supports various embedding models, from OpenAI, Sentence Transformers, BGE M3, BM25, to SPLADE pretrained models. Use default embedding function to generate dense vectors from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). embedding_fn = model.DefaultEmbeddingFunction() # Text strings to search from. docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] vectors = embedding_fn.encode_documents(docs) # The output vector has 768 dimensions, matching the collection that we just created. print(&quot;Dim:&quot;, embedding_fn.dim, vectors[0].shape) # Dim: 768 (768,) # Each entity has id, vector representation, raw text, and a subject label that we use # to demo metadata filtering later. data = [ {&quot;id&quot;: i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;history&quot;} for i in range(len(vectors)) ] print(&quot;Data has&quot;, len(data), &quot;entities, each with fields: &quot;, data[0].keys()) print(&quot;Vector dim:&quot;, len(data[0][&quot;vector&quot;])) Dim: 768 (768,) Data has 3 entities, each with fields: dict_keys([&#39;id&#39;, &#39;vector&#39;, &#39;text&#39;, &#39;subject&#39;]) Vector dim: 768 # To create embeddings for queries, use the encode_queries() method: query_vectors = embedding_fn.encode_queries([&quot;Who is Alan Turing?&quot;]) Use sentence transformer embedding function to generate dense vectors with Sentence Transformer pre-trained models pip install sentence_transformers # (optional) install sentence_transformers manually from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; sentence_transformer_ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, # Specify the model name device=&#39;cpu&#39; # Specify the device to use, e.g., &#39;cpu&#39;, &#39;cuda:0&#39;. If None, checks if a GPU can be used. ) docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] docs_embeddings = sentence_transformer_ef.encode_documents(docs) # Print embeddings print(&quot;Embeddings:&quot;, docs_embeddings) # Print dimension and shape of embeddings print(&quot;Dim:&quot;, sentence_transformer_ef.dim, docs_embeddings[0].shape) Embeddings: [array([-3.09392996e-02, -1.80662833e-02, 1.34775648e-02, 2.77156215e-02, -4.86349640e-03, -3.12581174e-02, -3.55921760e-02, 5.76934684e-03, 2.80773244e-03, 1.35783911e-01, 3.59678417e-02, 6.17732145e-02, ... -4.61330153e-02, -4.85207550e-02, 3.13997865e-02, 7.82178566e-02, -4.75336798e-02, 5.21207601e-02, 9.04406682e-02, -5.36676683e-02], dtype=float32)] Dim: 384 (384,) # To create embeddings for queries, use the encode_queries() method: queries = [&quot;When was artificial intelligence founded&quot;, &quot;Where was Alan Turing born?&quot;] query_embeddings = sentence_transformer_ef.encode_queries(queries) # Print embeddings print(&quot;Embeddings:&quot;, query_embeddings) # Print dimension and shape of embeddings print(&quot;Dim:&quot;, sentence_transformer_ef.dim, query_embeddings[0].shape) Embeddings: [array([-2.52114702e-02, -5.29330298e-02, 1.14570223e-02, 1.95571519e-02, -2.46500354e-02, -2.66519729e-02, -8.48201662e-03, 2.82961670e-02, -3.65092754e-02, 7.50745758e-02, 4.28900979e-02, 7.18822703e-02, ... -6.76431581e-02, -6.45996556e-02, -4.67132553e-02, 4.78532910e-02, -2.31596199e-03, 4.13446948e-02, 1.06935494e-01, -1.08258888e-01], dtype=float32)] Dim: 384 (384,) References [1] https://milvus.io/docs/overview.md [2] https://milvus.io/docs/architecture_overview.md [3] https://milvus.io/docs/install_standalone-docker-compose.md [4] https://milvus.io/docs/milvus_lite.md [5] https://milvus.io/docs/quickstart.md [6] https://milvus.io/docs/cli_overview.md [7] https://milvus.io/docs/schema.md [8] https://milvus.io/docs/manage-collections.md [9] https://milvus.io/docs/embeddings.md" />
<link rel="canonical" href="https://blog.codefarm.me/2024/06/14/what-is-milvus-vector-database/" />
<meta property="og:url" content="https://blog.codefarm.me/2024/06/14/what-is-milvus-vector-database/" />
<meta property="og:site_name" content="CODE FARM" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-14T13:53:48+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="What is Milvus?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-14T13:53:48+08:00","datePublished":"2024-06-14T13:53:48+08:00","description":"Milvus (/ˈmɪlvəs/) is an open-source vector database to store, index, and manage massive embedding vectors generated by deep neural networks and machine learning (ML) models. [1] Unlike existing relational databases which mainly deal with structured data following a pre-defined pattern, Milvus is designed from the bottom-up to handle embedding vectors converted from unstructured data, including images, video, audio, and natural language. Embedding vectors or vectors, the output data format of Neural Network models, can effectively encode information and serve a pivotal role in AI applications such as knowledge base, semantic search, Retrieval Augmented Generation (RAG) and more. Mathematically speaking, an embedding vector is an array of floating-point numbers or binaries. Modern embedding techniques are used to convert unstructured data to embedding vectors. Milvus is able to analyze the correlation between two vectors by calculating their similarity distance. If the two embedding vectors are very similar, it means that the original data sources are similar as well. Vector similarity search is the process of comparing a vector to a database to find vectors that are most similar to the query vector. Approximate nearest neighbor (ANN) search algorithms are used to accelerate the searching process. If the two embedding vectors are very similar, it means that the original data sources are similar as well. Milvus adopts a shared-storage architecture featuring storage and computing disaggregation and horizontal scalability for its computing nodes. Following the principle of data plane and control plane disaggregation, Milvus comprises four layers: access layer, coordinator service, worker node, and storage. [2] 1. Install Milvus 1.1. Run Milvus with Docker Compose 1.2. Run Milvus Lite locally 1.3. Milvus Command-Line Interface (CLI) 2. Schema and collections 2.1. Load &amp; release collection 2.2. Dynamic field 3. Embeddings References 1. Install Milvus Milvus Lite is good for getting started with vector search or building demos and prototypes, and supports the following OS distributions and sillicon types: Ubuntu &gt;= 20.04 (x86_64), and macOS &gt;= 11.0 (Apple Silicon and x86_64), and Debian 12 (x86_64) on Windows with WSL 2 enabled. [4] [5] For a production use case, It&#8217;s recommended using Milvus on Docker and Kubenetes, or considering the fully-managed Milvus on Zilliz Cloud. All deployment modes of Milvus share the same API, so your client side code doesn&#8217;t need to change much if moving to another deployment mode. Simply specify the URI and Token of a Milvus server deployed anywhere: [5] from pymilvus import MilvusClient # Authentication not enabled client = MilvusClient(&quot;http://localhost:19530&quot;) # Authentication enabled with the root user client = MilvusClient( uri=&quot;http://localhost:19530&quot;, token=&quot;root:Milvus&quot;, db_name=&quot;default&quot; ) # Authentication enabled with a non-root user client = MilvusClient( uri=&quot;http://localhost:19530&quot;, token=&quot;user:password&quot;, # replace this with your token db_name=&quot;default&quot; ) Milvus provides REST and gRPC API, with client libraries in languages such as Python, Java, Go, C# and Node.js. 1.1. Run Milvus with Docker Compose Milvus provides a Docker Compose configuration file in the Milvus repository. To install Milvus using Docker Compose, just run [install_standalone-docker-compose] # Download the configuration file $ wget https://github.com/milvus-io/milvus/releases/download/v2.4.4/milvus-standalone-docker-compose.yml -O docker-compose.yml # Start Milvus $ sudo docker compose up -d Creating milvus-etcd ... done Creating milvus-minio ... done Creating milvus-standalone ... done After starting up Milvus, containers named milvus-standalone, milvus-minio, and milvus-etcd are up. The milvus-etcd container does not expose any ports to the host and maps its data to volumes/etcd in the current folder. The milvus-minio container serves ports 9090 and 9091 locally with the default authentication credentials and maps its data to volumes/minio in the current folder. The milvus-standalone container serves ports 19530 locally with the default settings and maps its data to volumes/milvus in the current folder. You can check if the containers are up and running using the following command: $ sudo docker compose ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------- milvus-etcd etcd -advertise-client-url ... Up 2379/tcp, 2380/tcp milvus-minio /usr/bin/docker-entrypoint ... Up (healthy) 9000/tcp milvus-standalone /tini -- milvus run standalone Up 0.0.0.0:19530-&gt;19530/tcp, 0.0.0.0:9091-&gt;9091/tcp You can stop and delete this container as follows # Stop Milvus $ sudo docker compose down # Delete service data $ sudo rm -rf volumes 1.2. Run Milvus Lite locally Milvus Lite is the lightweight version of Milvus included in the Python SDK of Milvus, which can be imported into a Python application, providing the core vector search functionality of Milvus. Install Milvus # set up Milvus Lite with pymilvus, the Python SDK library of Milvus pip install &quot;pymilvus&gt;=2.4.2&quot; Set up vector database # connect to Milvus Lite from pymilvus import MilvusClient # generate or load an existing vector database file named milvus_demo.db in the current folder client = MilvusClient(&quot;milvus_demo.db&quot;) Create a collection # create a collection to store vectors and their associated metadata client.create_collection( collection_name=&quot;demo_collection&quot;, dimension=768, # The vectors we will use in this demo has 768 dimensions ) The primary key and vector fields use their default names (&quot;id&quot; and &quot;vector&quot;). The metric type (vector distance definition) is set to its default value (COSINE). The primary key field accepts integers and does not automatically increments (namely not using auto-id feature) Represent text with vectors To perform semantic search on text, it&#8217;s needed to generate vectors for text by downloading embedding models, which can be easily done by using the utility functions from pymilvus[model] library including essential ML tools such as PyTorch. pip install &quot;pymilvus[model]&gt;=2.4.2&quot; Milvus expects data to be inserted organized as a list of dictionaries, where each dictionary represents a data record, termed as an entity. # generate vector embeddings with default model from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). embedding_fn = model.DefaultEmbeddingFunction() # Text strings to search from. docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] vectors = embedding_fn.encode_documents(docs) # The output vector has 768 dimensions, matching the collection that we just created. print(&quot;Dim:&quot;, embedding_fn.dim, vectors[0].shape) # Dim: 768 (768,) # Each entity has id, vector representation, raw text, and a subject label that we use # to demo metadata filtering later. data = [ {&quot;id&quot;: i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;history&quot;} for i in range(len(vectors)) ] print(&quot;Data has&quot;, len(data), &quot;entities, each with fields: &quot;, data[0].keys()) print(&quot;Vector dim:&quot;, len(data[0][&quot;vector&quot;])) Dim: 768 (768,) Data has 3 entities, each with fields: dict_keys([&#39;id&#39;, &#39;vector&#39;, &#39;text&#39;, &#39;subject&#39;]) Vector dim: 768 Insert data into the collection. res = client.insert(collection_name=&quot;demo_collection&quot;, data=data) print(res) {&#39;insert_count&#39;: 3, &#39;ids&#39;: [0, 1, 2], &#39;cost&#39;: 0} Semantic search Milvus accepts one or multiple vector search requests as a list of vectors, where each vector is an array of float numbers, at the same time. # from pymilvus import MilvusClient, model # # client = MilvusClient(&quot;milvus_demo.db&quot;) # # # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # # # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). # embedding_fn = model.DefaultEmbeddingFunction() query_vectors = embedding_fn.encode_queries([&quot;Who is Alan Turing?&quot;]) res = client.search( collection_name=&quot;demo_collection&quot;, # target collection data=query_vectors, # query vectors limit=2, # number of returned entities output_fields=[&quot;text&quot;, &quot;subject&quot;], # specifies fields to be returned ) print(res) data: [&quot;[{&#39;id&#39;: 2, &#39;distance&#39;: 0.5859944820404053, &#39;entity&#39;: {&#39;text&#39;: &#39;Born in Maida Vale, London, Turing was raised in southern England.&#39;, &#39;subject&#39;: &#39;history&#39;}}, {&#39;id&#39;: 1, &#39;distance&#39;: 0.5118255019187927, &#39;entity&#39;: {&#39;text&#39;: &#39;Alan Turing was the first person to conduct substantial research in AI.&#39;, &#39;subject&#39;: &#39;history&#39;}}]&quot;] , extra_info: {&#39;cost&#39;: 0} # Vector search with metadata filtering # Insert more docs in another subject. docs = [ &quot;Machine learning has been used for drug design.&quot;, &quot;Computational synthesis with AI algorithms predicts molecular properties.&quot;, &quot;DDR1 is involved in cancers and fibrosis.&quot;, ] vectors = embedding_fn.encode_documents(docs) data = [ {&quot;id&quot;: 3 + i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;biology&quot;} for i in range(len(vectors)) ] client.insert(collection_name=&quot;demo_collection&quot;, data=data) # This will exclude any text in &quot;history&quot; subject despite close to the query vector. res = client.search( collection_name=&quot;demo_collection&quot;, data=embedding_fn.encode_queries([&quot;tell me AI related information&quot;]), filter=&quot;subject == &#39;biology&#39;&quot;, limit=2, output_fields=[&quot;text&quot;, &quot;subject&quot;], ) print(res) data: [&quot;[{&#39;id&#39;: 4, &#39;distance&#39;: 0.27030572295188904, &#39;entity&#39;: {&#39;text&#39;: &#39;Computational synthesis with AI algorithms predicts molecular properties.&#39;, &#39;subject&#39;: &#39;biology&#39;}}, {&#39;id&#39;: 3, &#39;distance&#39;: 0.1642588973045349, &#39;entity&#39;: {&#39;text&#39;: &#39;Machine learning has been used for drug design.&#39;, &#39;subject&#39;: &#39;biology&#39;}}]&quot;] , extra_info: {&#39;cost&#39;: 0} A query() is an operation that retrieves all entities matching a cretria, such as a filter expression or matching some ids. # retrieving all entities whose scalar field has a particular value res = client.query( collection_name=&quot;demo_collection&quot;, filter=&quot;subject == &#39;history&#39;&quot;, output_fields=[&quot;text&quot;, &quot;subject&quot;], ) # retrieving entities by primary key directly res = client.query( collection_name=&quot;demo_collection&quot;, ids=[0, 2], output_fields=[&quot;vector&quot;, &quot;text&quot;, &quot;subject&quot;], ) Delete entities specifying the primary key or delete all entities matching a particular filter expression. # Delete entities by primary key res = client.delete(collection_name=&quot;demo_collection&quot;, ids=[0, 2]) print(res) # Delete entities by a filter expression res = client.delete( collection_name=&quot;demo_collection&quot;, filter=&quot;subject == &#39;biology&#39;&quot;, ) print(res) # Drop collection client.drop_collection(collection_name=&quot;demo_collection&quot;) [0, 2] [3, 4, 5] 1.3. Milvus Command-Line Interface (CLI) Milvus Command-Line Interface (CLI), based on Milvus Python SDK, is a command-line tool that supports database connection, data operations, and import and export of data. [6] Install via pip pip install milvus-cli Install with Docker docker run -it zilliz/milvus_cli:latest Commands milvus_cli &gt; connect -uri http://127.0.0.1:19530 milvus_cli &gt; create database -db testdb milvus_cli &gt; list databases milvus_cli &gt; use database -db testdb milvus_cli &gt; list collections milvus_cli &gt; show collection -c test_collection_insert milvus_cli &gt; list connections milvus_cli &gt; search Collection name (car, test_collection): car The vectors of search data(the length of data is number of query (nq), the dim of every vector in data must be equal to vector field’s of collection. You can also import a csv file out headers): examples/import_csv/search_vectors.csv The vector field used to search of collection (vector): vector Metric type: L2 Search parameter nprobe&#39;s value: 10 The max number of returned record, also known as topk: 2 The boolean expression used to filter attribute []: id &gt; 0 The names of partitions to search (split by &quot;,&quot; if multiple) [&#39;_default&#39;] []: _default timeout []: Guarantee Timestamp(It instructs Milvus to see all operations performed before a provided timestamp. If no such timestamp is provided, then Milvus will search all operations performed to date) [0]: 2. Schema and collections In Milvus, schema is used to define the properties of a collection and the fields within. [7] A field schema is the logical definition of a field, and Milvus supports only one primary key field in a collection. To reduce the complexity in data inserts, Milvus allows to specify a default value for each scalar field during field schema creation, excluding the primary key field. Create a regular field schema: from pymilvus import FieldSchema id_field = FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True, description=&quot;primary id&quot;) age_field = FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, description=&quot;age&quot;) embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) # The following creates a field and use it as the partition key position_field = FieldSchema(name=&quot;position&quot;, dtype=DataType.VARCHAR, max_length=256, is_partition_key=True) Create a field schema with default field values: from pymilvus import FieldSchema fields = [ FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True), # configure default value `25` for field `age` FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, default_value=25, description=&quot;age&quot;), embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) ] A collection schema is the logical definition of a collection. Define the field schemas before defining a collection schema. Create a collection schema from pymilvus import FieldSchema, CollectionSchema id_field = FieldSchema(name=&quot;id&quot;, dtype=DataType.INT64, is_primary=True, description=&quot;primary id&quot;) age_field = FieldSchema(name=&quot;age&quot;, dtype=DataType.INT64, description=&quot;age&quot;) embedding_field = FieldSchema(name=&quot;embedding&quot;, dtype=DataType.FLOAT_VECTOR, dim=128, description=&quot;vector&quot;) # Enable partition key on a field if you need to implement multi-tenancy based on the partition-key field position_field = FieldSchema(name=&quot;position&quot;, dtype=DataType.VARCHAR, max_length=256, is_partition_key=True) # Set enable_dynamic_field to True if you need to use dynamic fields. schema = CollectionSchema(fields=[id_field, age_field, embedding_field], auto_id=False, enable_dynamic_field=True, description=&quot;desc of a collection&quot;) Enable dynamic schema by setting enable_dynamic_field to True in the collection schema. Create a collection with the schema specified: from pymilvus import Collection collection_name1 = &quot;tutorial_1&quot; collection1 = Collection(name=collection_name1, schema=schema, using=&#39;default&#39;, shards_num=2) 2.1. Load &amp; release collection Before conducting searches in a collection, ensure that the collection is loaded. During the loading process of a collection, Milvus loads the collection&#8217;s index file into memory. Conversely, when releasing a collection, Milvus unloads the index file from memory. [8] To load a collection, use the load_collection() method, specifying the collection name. # Load the collection client.load_collection( collection_name=&quot;customized_setup_2&quot;, replica_number=1 # Number of replicas to create on query nodes. Max value is 1 for Milvus Standalone, and no greater than `queryNode.replicas` for Milvus Cluster. ) res = client.get_load_state( collection_name=&quot;customized_setup_2&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: Loaded&gt;&quot; # } To release a collection, use the release_collection() method, specifying the collection name. # Release the collection client.release_collection( collection_name=&quot;customized_setup_2&quot; ) res = client.get_load_state( collection_name=&quot;customized_setup_2&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: NotLoad&gt;&quot; # } 2.2. Dynamic field The dynamic field in a collection is a reserved JSON field named $meta. It can hold non-schema-defined fields and their values as key-value pairs. Using the dynamic field, search and query both schema-defined fields and any non-schema-defined fields they may have. Enable dynamic field When defining a schema for a collection, set enable_dynamic_field to True to enable the reserved dynamic field, indicating that any non-schema-defined fields and their values inserted later on will be saved as key-value pairs in the reserved dynamic field. import random, time from pymilvus import connections, MilvusClient, DataType SERVER_ADDR = &quot;http://localhost:19530&quot; # 1. Set up a Milvus client client = MilvusClient( uri=SERVER_ADDR ) # 2. Create a collection schema = MilvusClient.create_schema( auto_id=False, # highlight-next-line enable_dynamic_field=True, ) schema.add_field(field_name=&quot;id&quot;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=&quot;vector&quot;, datatype=DataType.FLOAT_VECTOR, dim=5) index_params = MilvusClient.prepare_index_params() index_params.add_index( field_name=&quot;id&quot;, index_type=&quot;STL_SORT&quot; ) index_params.add_index( field_name=&quot;vector&quot;, index_type=&quot;IVF_FLAT&quot;, metric_type=&quot;L2&quot;, params={&quot;nlist&quot;: 1024} ) client.create_collection( collection_name=&quot;test_collection&quot;, schema=schema, index_params=index_params ) res = client.get_load_state( collection_name=&quot;test_collection&quot; ) print(res) # Output # # { # &quot;state&quot;: &quot;&lt;LoadState: Loaded&gt;&quot; # } # check the details of the collection. res = client.describe_collection( collection_name=&quot;test_collection&quot; ) print(res) # Output # # { # &quot;collection_name&quot;: &quot;test_collection&quot;, # &quot;auto_id&quot;: false, # &quot;num_shards&quot;: 1, # &quot;description&quot;: &quot;&quot;, # &quot;fields&quot;: [ # { # &quot;field_id&quot;: 100, # &quot;name&quot;: &quot;id&quot;, # &quot;description&quot;: &quot;&quot;, # &quot;type&quot;: 5, # &quot;params&quot;: {}, # &quot;is_primary&quot;: true # }, # { # &quot;field_id&quot;: 101, # &quot;name&quot;: &quot;vector&quot;, # &quot;description&quot;: &quot;&quot;, # &quot;type&quot;: 101, # &quot;params&quot;: { # &quot;dim&quot;: 5 # } # } # ], # &quot;aliases&quot;: [], # &quot;collection_id&quot;: 450568843971279780, # &quot;consistency_level&quot;: 2, # &quot;properties&quot;: {}, # &quot;num_partitions&quot;: 1, # &quot;enable_dynamic_field&quot;: true # } Insert dynamic data Prepare some randomly generated data for the insertion later on. colors = [&quot;green&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;black&quot;, &quot;white&quot;, &quot;purple&quot;, &quot;pink&quot;, &quot;orange&quot;, &quot;brown&quot;, &quot;grey&quot;] data = [] for i in range(1000): current_color = random.choice(colors) current_tag = random.randint(1000, 9999) data.append({ &quot;id&quot;: i, &quot;vector&quot;: [ random.uniform(-1, 1) for _ in range(5) ], &quot;color&quot;: current_color, &quot;tag&quot;: current_tag, &quot;color_tag&quot;: f&quot;{current_color}_{str(current_tag)}&quot; }) print(data[0]) Insert the data into the collection. res = client.insert( collection_name=&quot;test_collection&quot;, data=data, ) print(res) # Output # # { # &quot;insert_count&quot;: 1000, # &quot;ids&quot;: [ # 0, # 1, # 2, # 3, # 4, # 5, # 6, # 7, # 8, # 9, # &quot;(990 more items hidden)&quot; # ] # } time.sleep(5) Search with dynamic fields # 4. Search with dynamic fields query_vectors = [[0.3580376395471989, -0.6023495712049978, 0.18414012509913835, -0.26286205330961354, 0.9029438446296592]] res = client.search( collection_name=&quot;test_collection&quot;, data=query_vectors, filter=&quot;color in [\\&quot;red\\&quot;, \\&quot;green\\&quot;]&quot;, search_params={&quot;metric_type&quot;: &quot;L2&quot;, &quot;params&quot;: {&quot;nprobe&quot;: 10}}, limit=3 ) print(res) # Output # # [ # [ # { # &quot;id&quot;: 863, # &quot;distance&quot;: 0.188413605093956, # &quot;entity&quot;: { # &quot;id&quot;: 863, # &quot;color_tag&quot;: &quot;red_2371&quot; # } # }, # { # &quot;id&quot;: 799, # &quot;distance&quot;: 0.29188022017478943, # &quot;entity&quot;: { # &quot;id&quot;: 799, # &quot;color_tag&quot;: &quot;red_2235&quot; # } # }, # { # &quot;id&quot;: 564, # &quot;distance&quot;: 0.3492690920829773, # &quot;entity&quot;: { # &quot;id&quot;: 564, # &quot;color_tag&quot;: &quot;red_9186&quot; # } # } # ] # ] 3. Embeddings Embedding is a machine learning concept for mapping data into a high-dimensional space, where data of similar semantic are placed close together. [9] Typically being a Deep Neural Network from BERT or other Transformer families, the embedding model can effectively represent the semantics of text, images, and other data types with a series of numbers known as vectors. A key feature of these models is that the mathematical distance between vectors in the high-dimensional space can indicate the similarity of the semantics of original text or images, that unlocks many information retrieval applications, such as web search engines like Google and Bing, product search and recommendations on e-commerce sites, and the recently popular Retrieval Augmented Generation (RAG) paradigm in generative AI. There are two main categories of embeddings, each producing a different type of vector: Dense embedding: Most embedding models represent information as a floating point vector of hundreds to thousands of dimensions. The output is called &quot;dense&quot; vectors as most dimensions have non-zero values. Dense embedding is a technique used in natural language processing to represent words or phrases as continuous, dense vectors in a high-dimensional space, capturing semantic relationships. For instance, the popular open-source embedding model BAAI/bge-base-en-v1.5 outputs vectors of 768 floating point numbers (768-dimension float vector). Sparse embedding: In contrast, the output vectors of sparse embeddings has most dimensions being zero, namely &quot;sparse&quot; vectors. These vectors often have much higher dimensions (tens of thousands or more) which is determined by the size of the token vocabulary. Sparse vectors can be generated by Deep Neural Networks or statistical analysis of text corpora. Due to their interpretability and observed better out-of-domain generalization capabilities, sparse embeddings are increasingly adopted by developers as a complement to dense embeddings. Milvus is a vector database designed for vector data management, storage, and retrieval. By integrating mainstream embedding and reranking models, it can easily transform original text into searchable vectors or rerank the results using powerful models to achieve more accurate results for RAG, and simplifies text transformation and eliminates the need for additional embedding or reranking components, thereby streamlining RAG development and validation. To use embedding functions with Milvus, first install the PyMilvus client library with the model subpackage that wraps all the utilities for embedding generation. pip install pymilvus[model] # or pip install &quot;pymilvus[model]&quot; for zsh. # or pipenv install &#39;pymilvus[model]==2.4.4&#39; &#39;numpy&lt;2&#39; The model subpackage supports various embedding models, from OpenAI, Sentence Transformers, BGE M3, BM25, to SPLADE pretrained models. Use default embedding function to generate dense vectors from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # This will download a small embedding model &quot;paraphrase-albert-small-v2&quot; (~50MB). embedding_fn = model.DefaultEmbeddingFunction() # Text strings to search from. docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] vectors = embedding_fn.encode_documents(docs) # The output vector has 768 dimensions, matching the collection that we just created. print(&quot;Dim:&quot;, embedding_fn.dim, vectors[0].shape) # Dim: 768 (768,) # Each entity has id, vector representation, raw text, and a subject label that we use # to demo metadata filtering later. data = [ {&quot;id&quot;: i, &quot;vector&quot;: vectors[i], &quot;text&quot;: docs[i], &quot;subject&quot;: &quot;history&quot;} for i in range(len(vectors)) ] print(&quot;Data has&quot;, len(data), &quot;entities, each with fields: &quot;, data[0].keys()) print(&quot;Vector dim:&quot;, len(data[0][&quot;vector&quot;])) Dim: 768 (768,) Data has 3 entities, each with fields: dict_keys([&#39;id&#39;, &#39;vector&#39;, &#39;text&#39;, &#39;subject&#39;]) Vector dim: 768 # To create embeddings for queries, use the encode_queries() method: query_vectors = embedding_fn.encode_queries([&quot;Who is Alan Turing?&quot;]) Use sentence transformer embedding function to generate dense vectors with Sentence Transformer pre-trained models pip install sentence_transformers # (optional) install sentence_transformers manually from pymilvus import model # If connection to https://huggingface.co/ failed, uncomment the following path # import os # os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; sentence_transformer_ef = model.dense.SentenceTransformerEmbeddingFunction( model_name=&#39;all-MiniLM-L6-v2&#39;, # Specify the model name device=&#39;cpu&#39; # Specify the device to use, e.g., &#39;cpu&#39;, &#39;cuda:0&#39;. If None, checks if a GPU can be used. ) docs = [ &quot;Artificial intelligence was founded as an academic discipline in 1956.&quot;, &quot;Alan Turing was the first person to conduct substantial research in AI.&quot;, &quot;Born in Maida Vale, London, Turing was raised in southern England.&quot;, ] docs_embeddings = sentence_transformer_ef.encode_documents(docs) # Print embeddings print(&quot;Embeddings:&quot;, docs_embeddings) # Print dimension and shape of embeddings print(&quot;Dim:&quot;, sentence_transformer_ef.dim, docs_embeddings[0].shape) Embeddings: [array([-3.09392996e-02, -1.80662833e-02, 1.34775648e-02, 2.77156215e-02, -4.86349640e-03, -3.12581174e-02, -3.55921760e-02, 5.76934684e-03, 2.80773244e-03, 1.35783911e-01, 3.59678417e-02, 6.17732145e-02, ... -4.61330153e-02, -4.85207550e-02, 3.13997865e-02, 7.82178566e-02, -4.75336798e-02, 5.21207601e-02, 9.04406682e-02, -5.36676683e-02], dtype=float32)] Dim: 384 (384,) # To create embeddings for queries, use the encode_queries() method: queries = [&quot;When was artificial intelligence founded&quot;, &quot;Where was Alan Turing born?&quot;] query_embeddings = sentence_transformer_ef.encode_queries(queries) # Print embeddings print(&quot;Embeddings:&quot;, query_embeddings) # Print dimension and shape of embeddings print(&quot;Dim:&quot;, sentence_transformer_ef.dim, query_embeddings[0].shape) Embeddings: [array([-2.52114702e-02, -5.29330298e-02, 1.14570223e-02, 1.95571519e-02, -2.46500354e-02, -2.66519729e-02, -8.48201662e-03, 2.82961670e-02, -3.65092754e-02, 7.50745758e-02, 4.28900979e-02, 7.18822703e-02, ... -6.76431581e-02, -6.45996556e-02, -4.67132553e-02, 4.78532910e-02, -2.31596199e-03, 4.13446948e-02, 1.06935494e-01, -1.08258888e-01], dtype=float32)] Dim: 384 (384,) References [1] https://milvus.io/docs/overview.md [2] https://milvus.io/docs/architecture_overview.md [3] https://milvus.io/docs/install_standalone-docker-compose.md [4] https://milvus.io/docs/milvus_lite.md [5] https://milvus.io/docs/quickstart.md [6] https://milvus.io/docs/cli_overview.md [7] https://milvus.io/docs/schema.md [8] https://milvus.io/docs/manage-collections.md [9] https://milvus.io/docs/embeddings.md","headline":"What is Milvus?","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.codefarm.me/2024/06/14/what-is-milvus-vector-database/"},"url":"https://blog.codefarm.me/2024/06/14/what-is-milvus-vector-database/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="/assets/css/style.css"><!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SN88FJ18E5"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-SN88FJ18E5');
    </script></head>
  <body>
    <header class="c-header">
  <div class="o-container">
    <a class="c-header-title" href="/">CODE FARM</a>
    <button class="c-header-nav-toggle" id="nav-toggle" aria-label="Toggle navigation">
      <span class="c-header-nav-toggle-icon"></span>
    </button>
    <div class="c-header-nav-wrapper" id="nav-wrapper">
      <nav class="c-header-nav">
        <a href="/">Home</a>
        <a href="/categories/">Category</a>
        <a href="/tags/">Tag</a>
        <a href="/archives/">Archive</a>
        <a href="/about/">About</a>
        <a href="https://resume.github.io/?looogos" target="_blank">R&eacute;sum&eacute;</a>
      </nav>
    </div>
  </div>
  



<div class="o-container">
  <div class="c-banner">
    <img src="/assets/images/galaxy.svg" alt="Galaxy background" class="c-banner-bg">
    <div class="c-banner-quote">
      <p>"The Renaissance was a time when art, science, and philosophy flourished."</p>
      <cite>- Michelangelo</cite>
    </div>
  </div>
</div>
</header>

    <main class="o-container">
      <article class="c-post">
  <header class="c-post-header">
    <h1 class="c-post-title">What is Milvus?</h1><p class="c-post-meta">14 Jun 2024</p>
  </header>

  <div class="c-post-content">
    <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Milvus (/ˈmɪlvəs/) is an open-source vector database to store, index, and manage massive <a href="https://milvus.io/docs/overview.md#Embedding-vectors">embedding vectors</a> generated by deep neural networks and machine learning (ML) models. <a href="#milvus-overview">[1]</a></p>
</div>
<div class="paragraph">
<p>Unlike existing relational databases which mainly deal with structured data following a pre-defined pattern, Milvus is designed from the bottom-up to handle embedding vectors converted from <a href="https://milvus.io/docs/overview.md#Unstructured-data">unstructured data</a>, including images, video, audio, and natural language.</p>
</div>
<div class="paragraph">
<p>Embedding vectors or vectors, the output data format of Neural Network models, can effectively encode information and serve a pivotal role in AI applications such as knowledge base, semantic search, Retrieval Augmented Generation (RAG) and more. Mathematically speaking, an embedding vector is an array of floating-point numbers or binaries. Modern embedding techniques are used to convert unstructured data to embedding vectors.</p>
</div>
<div class="paragraph">
<p>Milvus is able to analyze the correlation between two vectors by calculating their similarity distance. If the two embedding vectors are very similar, it means that the original data sources are similar as well. <a href="https://milvus.io/docs/overview.md#Vector-similarity-search">Vector similarity search</a> is the process of comparing a vector to a database to find vectors that are most similar to the query vector. Approximate nearest neighbor (ANN) search algorithms are used to accelerate the searching process. If the two embedding vectors are very similar, it means that the original data sources are similar as well.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://milvus.io/docs/v2.4.x/assets/milvus_workflow.jpeg" alt="Milvus Workflow" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>Milvus adopts a shared-storage architecture featuring storage and computing disaggregation and horizontal scalability for its computing nodes. Following the principle of data plane and control plane disaggregation, Milvus comprises four layers: access layer, coordinator service, worker node, and storage. <a href="#milvus-architecture_overview">[2]</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://milvus.io/docs/v2.4.x/assets/milvus_architecture.png" alt="Milvus Architecture" width="75%" height="75%">
</div>
</div>
</div>
<div id="toc" class="toc">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#install-milvus">1. Install Milvus</a>
<ul class="sectlevel2">
<li><a href="#run-milvus-with-docker-compose">1.1. Run Milvus with Docker Compose</a></li>
<li><a href="#run-milvus-lite-locally">1.2. Run Milvus Lite locally</a></li>
<li><a href="#milvus-command-line-interface-cli">1.3. Milvus Command-Line Interface (CLI)</a></li>
</ul>
</li>
<li><a href="#schema-and-collections">2. Schema and collections</a>
<ul class="sectlevel2">
<li><a href="#load-release-collection">2.1. Load &amp; release collection</a></li>
<li><a href="#dynamic-field">2.2. Dynamic field</a></li>
</ul>
</li>
<li><a href="#embeddings">3. Embeddings</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
</div>
<div class="sect1">
<h2 id="install-milvus">1. Install Milvus</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Milvus Lite is good for getting started with vector search or building demos and prototypes, and supports the following OS distributions and sillicon types: Ubuntu &gt;= 20.04 (x86_64), and macOS &gt;= 11.0 (Apple Silicon and x86_64), and Debian 12 (x86_64) on Windows with WSL 2 enabled. <a href="#milvus_lite">[4]</a> <a href="#milvus-quickstart">[5]</a></p>
</div>
<div class="paragraph">
<p>For a production use case, It&#8217;s recommended using Milvus on <a href="https://milvus.io/docs/install_standalone-docker.md">Docker</a> and <a href="https://milvus.io/docs/install_cluster-milvusoperator.md">Kubenetes</a>, or considering the fully-managed Milvus on <a href="https://zilliz.com/cloud">Zilliz Cloud</a>.</p>
</div>
<div class="paragraph">
<p>All deployment modes of Milvus share the same API, so your client side code doesn&#8217;t need to change much if moving to another deployment mode. Simply specify the <a href="https://milvus.io/api-reference/pymilvus/v2.4.x/MilvusClient/Client/MilvusClient.md">URI and Token</a> of a Milvus server deployed anywhere: <a href="#milvus-quickstart">[5]</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span>

<span class="c1"># Authentication not enabled
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span><span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Authentication enabled with the root user
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="sh">"</span><span class="s">root:Milvus</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">db_name</span><span class="o">=</span><span class="sh">"</span><span class="s">default</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># Authentication enabled with a non-root user
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="sh">"</span><span class="s">user:password</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># replace this with your token
</span>    <span class="n">db_name</span><span class="o">=</span><span class="sh">"</span><span class="s">default</span><span class="sh">"</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Milvus provides REST and gRPC API, with client libraries in languages such as Python, Java, Go, C# and Node.js.</p>
</div>
<div class="sect2">
<h3 id="run-milvus-with-docker-compose">1.1. Run Milvus with Docker Compose</h3>
<div class="paragraph">
<p>Milvus provides a Docker Compose configuration file in the Milvus repository. To install Milvus using Docker Compose, just run <a href="#install_standalone-docker-compose">[install_standalone-docker-compose]</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="c"># Download the configuration file</span>
<span class="nv">$ </span>wget https://github.com/milvus-io/milvus/releases/download/v2.4.4/milvus-standalone-docker-compose.yml <span class="nt">-O</span> docker-compose.yml

<span class="c"># Start Milvus</span>
<span class="nv">$ </span><span class="nb">sudo </span>docker compose up <span class="nt">-d</span>

Creating milvus-etcd  ... <span class="k">done
</span>Creating milvus-minio ... <span class="k">done
</span>Creating milvus-standalone ... <span class="k">done</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>After starting up Milvus, containers named <code>milvus-standalone</code>, <code>milvus-minio</code>, and <code>milvus-etcd</code> are up.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>milvus-etcd</code> container does not expose any ports to the host and maps its data to <code>volumes/etcd</code> in the current folder.</p>
</li>
<li>
<p>The <code>milvus-minio</code> container serves ports <code>9090</code> and <code>9091</code> locally with the default authentication credentials and maps its data to <code>volumes/minio</code> in the current folder.</p>
</li>
<li>
<p>The <code>milvus-standalone</code> container serves ports <code>19530</code> locally with the default settings and maps its data to <code>volumes/milvus</code> in the current folder.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can check if the containers are up and running using the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>docker compose ps
<span class="go">
      Name                     Command                  State                            Ports
--------------------------------------------------------------------------------------------------------------------
milvus-etcd         etcd -advertise-client-url ...   Up             2379/tcp, 2380/tcp
milvus-minio        /usr/bin/docker-entrypoint ...   Up (healthy)   9000/tcp
</span><span class="gp">milvus-standalone   /tini -- milvus run standalone   Up             0.0.0.0:19530-&gt;</span>19530/tcp, 0.0.0.0:9091-&gt;9091/tcp</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can stop and delete this container as follows</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">#</span><span class="w"> </span>Stop Milvus
<span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>docker compose down
<span class="go">
</span><span class="gp">#</span><span class="w"> </span>Delete service data
<span class="gp">$</span><span class="w"> </span><span class="nb">sudo rm</span> <span class="nt">-rf</span> volumes</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="run-milvus-lite-locally">1.2. Run Milvus Lite locally</h3>
<div class="paragraph">
<p>Milvus Lite is the lightweight version of Milvus included in the <a href="https://github.com/milvus-io/pymilvus">Python SDK of Milvus</a>, which can be imported into a Python application, providing the core vector search functionality of Milvus.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Install Milvus</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="c"># set up Milvus Lite with pymilvus, the Python SDK library of Milvus</span>
pip <span class="nb">install</span> <span class="s2">"pymilvus&gt;=2.4.2"</span></code></pre>
</div>
</div>
</li>
<li>
<p>Set up vector database</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># connect to Milvus Lite
</span><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">MilvusClient</span>

<span class="c1"># generate  or load an existing vector database file named milvus_demo.db in the current folder
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span><span class="sh">"</span><span class="s">milvus_demo.db</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create a collection</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># create a collection to store vectors and their associated metadata
</span><span class="n">client</span><span class="p">.</span><span class="nf">create_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>  <span class="c1"># The vectors we will use in this demo has 768 dimensions
</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>The primary key and vector fields use their default names ("id" and "vector").</p>
</li>
<li>
<p>The metric type (vector distance definition) is set to its default value (<a href="https://milvus.io/docs/metric.md#Cosine-Similarity">COSINE</a>).</p>
</li>
<li>
<p>The primary key field accepts integers and does not automatically increments (namely not using <a href="https://milvus.io/docs/schema.md">auto-id feature</a>)</p>
</li>
</ul>
</div>
</div>
</div>
</li>
<li>
<p>Represent text with vectors</p>
<div class="ulist">
<ul>
<li>
<p>To perform semantic search on text, it&#8217;s needed to generate vectors for text by downloading embedding models, which can be easily done by using the utility functions from <code>pymilvus[model]</code> library including essential ML tools such as PyTorch.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">pip <span class="nb">install</span> <span class="s2">"pymilvus[model]&gt;=2.4.2"</span></code></pre>
</div>
</div>
</li>
<li>
<p>Milvus expects data to be inserted organized as a list of dictionaries, where each dictionary represents a data record, termed as an entity.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># generate vector embeddings with default model
</span><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">model</span>

<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="c1"># This will download a small embedding model "paraphrase-albert-small-v2" (~50MB).
</span><span class="n">embedding_fn</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nc">DefaultEmbeddingFunction</span><span class="p">()</span>

<span class="c1"># Text strings to search from.
</span><span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Artificial intelligence was founded as an academic discipline in 1956.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Alan Turing was the first person to conduct substantial research in AI.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Born in Maida Vale, London, Turing was raised in southern England.</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">embedding_fn</span><span class="p">.</span><span class="nf">encode_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="c1"># The output vector has 768 dimensions, matching the collection that we just created.
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Dim:</span><span class="sh">"</span><span class="p">,</span> <span class="n">embedding_fn</span><span class="p">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">vectors</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Dim: 768 (768,)
</span>
<span class="c1"># Each entity has id, vector representation, raw text, and a subject label that we use
# to demo metadata filtering later.
</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">:</span> <span class="n">vectors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">docs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">history</span><span class="sh">"</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">vectors</span><span class="p">))</span>
<span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Data has</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="sh">"</span><span class="s">entities, each with fields: </span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">keys</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Vector dim:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">]))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Dim: 768 (768,)
Data has 3 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'subject'])
Vector dim: 768</span></code></pre>
</div>
</div>
</li>
<li>
<p>Insert data into the collection.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'insert_count': 3, 'ids': [0, 1, 2], 'cost': 0}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Semantic search</p>
<div class="ulist">
<ul>
<li>
<p>Milvus accepts one or multiple <strong>vector search</strong> requests as a list of vectors, where each vector is an array of float numbers, at the same time.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># from pymilvus import MilvusClient, model
#
# client = MilvusClient("milvus_demo.db")
#
# # If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
#
# # This will download a small embedding model "paraphrase-albert-small-v2" (~50MB).
# embedding_fn = model.DefaultEmbeddingFunction()
</span>
<span class="n">query_vectors</span> <span class="o">=</span> <span class="n">embedding_fn</span><span class="p">.</span><span class="nf">encode_queries</span><span class="p">([</span><span class="sh">"</span><span class="s">Who is Alan Turing?</span><span class="sh">"</span><span class="p">])</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># target collection
</span>    <span class="n">data</span><span class="o">=</span><span class="n">query_vectors</span><span class="p">,</span>  <span class="c1"># query vectors
</span>    <span class="n">limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># number of returned entities
</span>    <span class="n">output_fields</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">],</span>  <span class="c1"># specifies fields to be returned
</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">data: ["[{'id': 2, 'distance': 0.5859944820404053, 'entity': {'text': 'Born in Maida Vale, London, Turing was raised in southern England.', 'subject': 'history'}}, {'id': 1, 'distance': 0.5118255019187927, 'entity': {'text': 'Alan Turing was the first person to conduct substantial research in AI.', 'subject': 'history'}}]"] , extra_info: {'cost': 0}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># Vector search with metadata filtering
</span>
<span class="c1"># Insert more docs in another subject.
</span><span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Machine learning has been used for drug design.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Computational synthesis with AI algorithms predicts molecular properties.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">DDR1 is involved in cancers and fibrosis.</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">embedding_fn</span><span class="p">.</span><span class="nf">encode_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">:</span> <span class="n">vectors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">docs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">biology</span><span class="sh">"</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">vectors</span><span class="p">))</span>
<span class="p">]</span>

<span class="n">client</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># This will exclude any text in "history" subject despite close to the query vector.
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">embedding_fn</span><span class="p">.</span><span class="nf">encode_queries</span><span class="p">([</span><span class="sh">"</span><span class="s">tell me AI related information</span><span class="sh">"</span><span class="p">]),</span>
    <span class="nb">filter</span><span class="o">=</span><span class="sh">"</span><span class="s">subject == </span><span class="sh">'</span><span class="s">biology</span><span class="sh">'"</span><span class="p">,</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">output_fields</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">data: ["[{'id': 4, 'distance': 0.27030572295188904, 'entity': {'text': 'Computational synthesis with AI algorithms predicts molecular properties.', 'subject': 'biology'}}, {'id': 3, 'distance': 0.1642588973045349, 'entity': {'text': 'Machine learning has been used for drug design.', 'subject': 'biology'}}]"] , extra_info: {'cost': 0}</span></code></pre>
</div>
</div>
</li>
<li>
<p>A query() is an operation that retrieves all entities matching a cretria, such as a filter expression or matching some ids.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># retrieving all entities whose scalar field has a particular value
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="nb">filter</span><span class="o">=</span><span class="sh">"</span><span class="s">subject == </span><span class="sh">'</span><span class="s">history</span><span class="sh">'"</span><span class="p">,</span>
    <span class="n">output_fields</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># retrieving entities by primary key directly
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">output_fields</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Delete entities specifying the primary key or delete all entities matching a particular filter expression.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># Delete entities by primary key
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Delete entities by a filter expression
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="nb">filter</span><span class="o">=</span><span class="sh">"</span><span class="s">subject == </span><span class="sh">'</span><span class="s">biology</span><span class="sh">'"</span><span class="p">,</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Drop collection
</span><span class="n">client</span><span class="p">.</span><span class="nf">drop_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">demo_collection</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">[0, 2]
[3, 4, 5]</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="milvus-command-line-interface-cli">1.3. Milvus Command-Line Interface (CLI)</h3>
<div class="paragraph">
<p>Milvus Command-Line Interface (CLI), based on <a href="https://github.com/milvus-io/pymilvus">Milvus Python SDK</a>, is a command-line tool that supports database connection, data operations, and import and export of data. <a href="#milvus-cli_overview">[6]</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Install via pip</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">pip <span class="nb">install </span>milvus-cli</code></pre>
</div>
</div>
</li>
<li>
<p>Install with Docker</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">docker run <span class="nt">-it</span> zilliz/milvus_cli:latest</code></pre>
</div>
</div>
</li>
<li>
<p>Commands</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">milvus_cli &gt;</span><span class="w"> </span>connect <span class="nt">-uri</span> http://127.0.0.1:19530
<span class="gp">milvus_cli &gt;</span><span class="w"> </span>create database <span class="nt">-db</span> testdb
<span class="gp">milvus_cli &gt;</span><span class="w"> </span>list databases
<span class="gp">milvus_cli &gt;</span><span class="w"> </span>use database <span class="nt">-db</span> testdb
<span class="gp">milvus_cli &gt;</span><span class="w"> </span>list collections
<span class="gp">milvus_cli &gt;</span><span class="w"> </span>show collection <span class="nt">-c</span> test_collection_insert
<span class="gp">milvus_cli &gt;</span><span class="w"> </span>list connections
<span class="gp">milvus_cli &gt;</span><span class="w"> </span>search
<span class="go">
Collection name (car, test_collection): car

The vectors of search data(the length of data is number of query (nq), the dim of every vector in data must be equal to vector field’s of collection. You can also import a csv file
out headers): examples/import_csv/search_vectors.csv

The vector field used to search of collection (vector): vector

Metric type: L2

Search parameter nprobe's value: 10

The max number of returned record, also known as topk: 2

</span><span class="gp">The boolean expression used to filter attribute []: id &gt;</span><span class="w"> </span>0
<span class="go">
The names of partitions to search (split by "," if multiple) ['_default'] []: _default

timeout []:

Guarantee Timestamp(It instructs Milvus to see all operations performed before a provided timestamp. If no such timestamp is provided, then Milvus will search all operations performed to date) [0]:</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="schema-and-collections">2. Schema and collections</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In Milvus, schema is used to define the properties of a collection and the fields within. <a href="#milvus-schema">[7]</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p>A <em>field schema</em> is the logical definition of a field, and Milvus supports only one primary key field in a collection.</p>
<div class="paragraph">
<p>To reduce the complexity in data inserts, Milvus allows to specify a default value for each scalar field during field schema creation, excluding the primary key field.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a regular field schema:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">FieldSchema</span>
<span class="n">id_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">is_primary</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">primary id</span><span class="sh">"</span><span class="p">)</span>
<span class="n">age_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">)</span>
<span class="n">embedding_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">FLOAT_VECTOR</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># The following creates a field and use it as the partition key
</span><span class="n">position_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">position</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">VARCHAR</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">is_partition_key</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create a field schema with default field values:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">FieldSchema</span>

<span class="n">fields</span> <span class="o">=</span> <span class="p">[</span>
  <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">is_primary</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
  <span class="c1"># configure default value `25` for field `age`
</span>  <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">),</span>
  <span class="n">embedding_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">FLOAT_VECTOR</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">)</span>
<span class="p">]</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>A <em>collection schema</em> is the logical definition of a collection.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Define the field schemas before defining a collection schema.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a collection schema</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">FieldSchema</span><span class="p">,</span> <span class="n">CollectionSchema</span>
<span class="n">id_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">is_primary</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">primary id</span><span class="sh">"</span><span class="p">)</span>
<span class="n">age_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">)</span>
<span class="n">embedding_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">FLOAT_VECTOR</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Enable partition key on a field if you need to implement multi-tenancy based on the partition-key field
</span><span class="n">position_field</span> <span class="o">=</span> <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">position</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">VARCHAR</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">is_partition_key</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Set enable_dynamic_field to True if you need to use dynamic fields.
</span><span class="n">schema</span> <span class="o">=</span> <span class="nc">CollectionSchema</span><span class="p">(</span><span class="n">fields</span><span class="o">=</span><span class="p">[</span><span class="n">id_field</span><span class="p">,</span> <span class="n">age_field</span><span class="p">,</span> <span class="n">embedding_field</span><span class="p">],</span> <span class="n">auto_id</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">enable_dynamic_field</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">desc of a collection</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Enable dynamic schema by setting <code>enable_dynamic_field</code> to <code>True</code> in the collection schema.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a collection with the schema specified:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">Collection</span>
<span class="n">collection_name1</span> <span class="o">=</span> <span class="sh">"</span><span class="s">tutorial_1</span><span class="sh">"</span>
<span class="n">collection1</span> <span class="o">=</span> <span class="nc">Collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name1</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">using</span><span class="o">=</span><span class="sh">'</span><span class="s">default</span><span class="sh">'</span><span class="p">,</span> <span class="n">shards_num</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="load-release-collection">2.1. Load &amp; release collection</h3>
<div class="paragraph">
<p>Before conducting searches in a collection, ensure that the collection is loaded. During the loading process of a collection, Milvus loads the collection&#8217;s index file into memory. Conversely, when releasing a collection, Milvus unloads the index file from memory. <a href="#milvus-manage-collections">[8]</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p>To load a collection, use the <a href="https://milvus.io/api-reference/pymilvus/v2.4.x/MilvusClient/Management/load_collection.md">load_collection()</a> method, specifying the collection name.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># Load the collection
</span><span class="n">client</span><span class="p">.</span><span class="nf">load_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">customized_setup_2</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">replica_number</span><span class="o">=</span><span class="mi">1</span> <span class="c1"># Number of replicas to create on query nodes. Max value is 1 for Milvus Standalone, and no greater than `queryNode.replicas` for Milvus Cluster.
</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_load_state</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">customized_setup_2</span><span class="sh">"</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Output
#
# {
#     "state": "&lt;LoadState: Loaded&gt;"
# }</span></code></pre>
</div>
</div>
</li>
<li>
<p>To release a collection, use the <a href="https://milvus.io/api-reference/pymilvus/v2.4.x/MilvusClient/Management/release_collection.md">release_collection()</a> method, specifying the collection name.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># Release the collection
</span><span class="n">client</span><span class="p">.</span><span class="nf">release_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">customized_setup_2</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_load_state</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">customized_setup_2</span><span class="sh">"</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Output
#
# {
#     "state": "&lt;LoadState: NotLoad&gt;"
# }</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="dynamic-field">2.2. Dynamic field</h3>
<div class="paragraph">
<p>The <a href="https://milvus.io/docs/enable-dynamic-field.md">dynamic field</a> in a collection is a reserved JSON field named <code>$meta</code>. It can hold non-schema-defined fields and their values as key-value pairs. Using the dynamic field, search and query both schema-defined fields and any non-schema-defined fields they may have.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Enable dynamic field</p>
<div class="paragraph">
<p>When defining a schema for a collection, set <code>enable_dynamic_field</code> to <code>True</code> to enable the reserved dynamic field, indicating that any non-schema-defined fields and their values inserted later on will be saved as key-value pairs in the reserved dynamic field.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">time</span>
<span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">connections</span><span class="p">,</span> <span class="n">MilvusClient</span><span class="p">,</span> <span class="n">DataType</span>

<span class="n">SERVER_ADDR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:19530</span><span class="sh">"</span>

<span class="c1"># 1. Set up a Milvus client
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">MilvusClient</span><span class="p">(</span>
    <span class="n">uri</span><span class="o">=</span><span class="n">SERVER_ADDR</span>
<span class="p">)</span>

<span class="c1"># 2. Create a collection
</span><span class="n">schema</span> <span class="o">=</span> <span class="n">MilvusClient</span><span class="p">.</span><span class="nf">create_schema</span><span class="p">(</span>
    <span class="n">auto_id</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="c1"># highlight-next-line
</span>    <span class="n">enable_dynamic_field</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">schema</span><span class="p">.</span><span class="nf">add_field</span><span class="p">(</span><span class="n">field_name</span><span class="o">=</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="n">datatype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">is_primary</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">schema</span><span class="p">.</span><span class="nf">add_field</span><span class="p">(</span><span class="n">field_name</span><span class="o">=</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">,</span> <span class="n">datatype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">FLOAT_VECTOR</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">index_params</span> <span class="o">=</span> <span class="n">MilvusClient</span><span class="p">.</span><span class="nf">prepare_index_params</span><span class="p">()</span>

<span class="n">index_params</span><span class="p">.</span><span class="nf">add_index</span><span class="p">(</span>
    <span class="n">field_name</span><span class="o">=</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">index_type</span><span class="o">=</span><span class="sh">"</span><span class="s">STL_SORT</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">index_params</span><span class="p">.</span><span class="nf">add_index</span><span class="p">(</span>
    <span class="n">field_name</span><span class="o">=</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">index_type</span><span class="o">=</span><span class="sh">"</span><span class="s">IVF_FLAT</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">metric_type</span><span class="o">=</span><span class="sh">"</span><span class="s">L2</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">nlist</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1024</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">client</span><span class="p">.</span><span class="nf">create_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">test_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span>
    <span class="n">index_params</span><span class="o">=</span><span class="n">index_params</span>
<span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_load_state</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">test_collection</span><span class="sh">"</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Output
#
# {
#     "state": "&lt;LoadState: Loaded&gt;"
# }</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># check the details of the collection.
</span><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">describe_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">test_collection</span><span class="sh">"</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Output
#
# {
#   "collection_name": "test_collection",
#   "auto_id": false,
#   "num_shards": 1,
#   "description": "",
#   "fields": [
#     {
#       "field_id": 100,
#       "name": "id",
#       "description": "",
#       "type": 5,
#       "params": {},
#       "is_primary": true
#     },
#     {
#       "field_id": 101,
#       "name": "vector",
#       "description": "",
#       "type": 101,
#       "params": {
#         "dim": 5
#       }
#     }
#   ],
#   "aliases": [],
#   "collection_id": 450568843971279780,
#   "consistency_level": 2,
#   "properties": {},
#   "num_partitions": 1,
#   "enable_dynamic_field": true
# }</span></code></pre>
</div>
</div>
</li>
<li>
<p>Insert dynamic data</p>
<div class="ulist">
<ul>
<li>
<p>Prepare some randomly generated data for the insertion later on.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yellow</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">white</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">purple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">pink</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">brown</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">grey</span><span class="sh">"</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">current_color</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">current_tag</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">9999</span><span class="p">)</span>
    <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span> <span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="p">],</span>
        <span class="sh">"</span><span class="s">color</span><span class="sh">"</span><span class="p">:</span> <span class="n">current_color</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">tag</span><span class="sh">"</span><span class="p">:</span> <span class="n">current_tag</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">color_tag</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_color</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="nf">str</span><span class="p">(</span><span class="n">current_tag</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span>
    <span class="p">})</span>

<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></code></pre>
</div>
</div>
</li>
<li>
<p>Insert the data into the collection.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">test_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Output
#
# {
#     "insert_count": 1000,
#     "ids": [
#         0,
#         1,
#         2,
#         3,
#         4,
#         5,
#         6,
#         7,
#         8,
#         9,
#         "(990 more items hidden)"
#     ]
# }
</span>
<span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Search with dynamic fields</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># 4. Search with dynamic fields
</span><span class="n">query_vectors</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.3580376395471989</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6023495712049978</span><span class="p">,</span> <span class="mf">0.18414012509913835</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.26286205330961354</span><span class="p">,</span> <span class="mf">0.9029438446296592</span><span class="p">]]</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">test_collection</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">query_vectors</span><span class="p">,</span>
    <span class="nb">filter</span><span class="o">=</span><span class="sh">"</span><span class="s">color in [</span><span class="se">\"</span><span class="s">red</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s">green</span><span class="se">\"</span><span class="s">]</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">search_params</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">metric_type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">L2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">nprobe</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">}},</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Output
#
# [
#     [
#         {
#             "id": 863,
#             "distance": 0.188413605093956,
#             "entity": {
#                 "id": 863,
#                 "color_tag": "red_2371"
#             }
#         },
#         {
#             "id": 799,
#             "distance": 0.29188022017478943,
#             "entity": {
#                 "id": 799,
#                 "color_tag": "red_2235"
#             }
#         },
#         {
#             "id": 564,
#             "distance": 0.3492690920829773,
#             "entity": {
#                 "id": 564,
#                 "color_tag": "red_9186"
#             }
#         }
#     ]
# ]</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="embeddings">3. Embeddings</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Embedding is a machine learning concept for mapping data into a high-dimensional space, where data of similar semantic are placed close together. <a href="#milvus-embeddings">[9]</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Typically being a Deep Neural Network from BERT or other Transformer families, the <strong>embedding model</strong> can effectively represent the semantics of text, images, and other data types with a series of numbers known as <strong>vectors</strong>.</p>
</li>
<li>
<p>A key feature of these models is that the <strong>mathematical distance</strong> between vectors in the high-dimensional space can indicate the similarity of the semantics of original text or images, that unlocks many information retrieval applications, such as web search engines like Google and Bing, product search and recommendations on e-commerce sites, and the recently popular Retrieval Augmented Generation (RAG) paradigm in generative AI.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There are two main categories of embeddings, each producing a different type of vector:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Dense</strong> embedding: Most embedding models represent information as a floating point vector of hundreds to thousands of dimensions. The output is called "dense" vectors as most dimensions have non-zero values.</p>
<div class="paragraph">
<p><a href="https://github.com/milvus-io/bootcamp/blob/master/bootcamp/model/embedding_functions.ipynb">Dense embedding</a> is a technique used in natural language processing to represent words or phrases as continuous, dense vectors in a high-dimensional space, capturing semantic relationships.</p>
</div>
<div class="paragraph">
<p>For instance, the popular open-source embedding model BAAI/bge-base-en-v1.5 outputs vectors of 768 floating point numbers (768-dimension float vector).</p>
</div>
</li>
<li>
<p><strong>Sparse</strong> embedding: In contrast, the output vectors of sparse embeddings has most dimensions being zero, namely "sparse" vectors. These vectors often have much higher dimensions (tens of thousands or more) which is determined by the size of the token vocabulary.</p>
<div class="paragraph">
<p>Sparse vectors can be generated by Deep Neural Networks or statistical analysis of text corpora. Due to their interpretability and observed better out-of-domain generalization capabilities, sparse embeddings are increasingly adopted by developers as a complement to dense embeddings.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Milvus is a vector database designed for vector data management, storage, and retrieval. By integrating mainstream embedding and <a href="https://milvus.io/docs/rerankers-overview.md">reranking</a> models, it can easily transform original text into searchable vectors or rerank the results using powerful models to achieve more accurate results for RAG, and simplifies text transformation and eliminates the need for additional embedding or reranking components, thereby streamlining RAG development and validation.</p>
</div>
<div class="paragraph">
<p>To use embedding functions with Milvus, first install the PyMilvus client library with the <code>model</code> subpackage that wraps all the utilities for embedding generation.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">pip <span class="nb">install </span>pymilvus[model]
<span class="c"># or pip install "pymilvus[model]" for zsh.</span>
<span class="c"># or pipenv install 'pymilvus[model]==2.4.4' 'numpy&lt;2'</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The model subpackage supports various embedding models, from OpenAI, Sentence Transformers, BGE M3, BM25, to SPLADE pretrained models.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use <a href="https://github.com/milvus-io/milvus-model/blob/main/milvus_model/<em>init</em>.py">default embedding function</a> to generate dense vectors</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">model</span>

<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="c1"># This will download a small embedding model "paraphrase-albert-small-v2" (~50MB).
</span><span class="n">embedding_fn</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nc">DefaultEmbeddingFunction</span><span class="p">()</span>

<span class="c1"># Text strings to search from.
</span><span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Artificial intelligence was founded as an academic discipline in 1956.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Alan Turing was the first person to conduct substantial research in AI.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Born in Maida Vale, London, Turing was raised in southern England.</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">embedding_fn</span><span class="p">.</span><span class="nf">encode_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="c1"># The output vector has 768 dimensions, matching the collection that we just created.
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Dim:</span><span class="sh">"</span><span class="p">,</span> <span class="n">embedding_fn</span><span class="p">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">vectors</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Dim: 768 (768,)
</span>
<span class="c1"># Each entity has id, vector representation, raw text, and a subject label that we use
# to demo metadata filtering later.
</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">:</span> <span class="n">vectors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">docs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">history</span><span class="sh">"</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">vectors</span><span class="p">))</span>
<span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Data has</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="sh">"</span><span class="s">entities, each with fields: </span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">keys</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Vector dim:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">]))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Dim: 768 (768,)
Data has 3 entities, each with fields:  dict_keys(['id', 'vector', 'text', 'subject'])
Vector dim: 768</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># To create embeddings for queries, use the encode_queries() method:
</span><span class="n">query_vectors</span> <span class="o">=</span> <span class="n">embedding_fn</span><span class="p">.</span><span class="nf">encode_queries</span><span class="p">([</span><span class="sh">"</span><span class="s">Who is Alan Turing?</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
</li>
<li>
<p>Use <a href="https://github.com/milvus-io/milvus-model/blob/main/milvus_model/dense/sentence_transformer.py">sentence transformer embedding function</a> to generate dense vectors with <a href="https://huggingface.co/sentence-transformers">Sentence Transformer</a> pre-trained models</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">pip <span class="nb">install </span>sentence_transformers  <span class="c"># (optional) install sentence_transformers manually</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pymilvus</span> <span class="kn">import</span> <span class="n">model</span>

<span class="c1"># If connection to https://huggingface.co/ failed, uncomment the following path
# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
</span>
<span class="n">sentence_transformer_ef</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">dense</span><span class="p">.</span><span class="nc">SentenceTransformerEmbeddingFunction</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># Specify the model name
</span>    <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span> <span class="c1"># Specify the device to use, e.g., 'cpu', 'cuda:0'. If None, checks if a GPU can be used.
</span><span class="p">)</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Artificial intelligence was founded as an academic discipline in 1956.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Alan Turing was the first person to conduct substantial research in AI.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Born in Maida Vale, London, Turing was raised in southern England.</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">docs_embeddings</span> <span class="o">=</span> <span class="n">sentence_transformer_ef</span><span class="p">.</span><span class="nf">encode_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="c1"># Print embeddings
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Embeddings:</span><span class="sh">"</span><span class="p">,</span> <span class="n">docs_embeddings</span><span class="p">)</span>
<span class="c1"># Print dimension and shape of embeddings
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Dim:</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence_transformer_ef</span><span class="p">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">docs_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Embeddings: [array([-3.09392996e-02, -1.80662833e-02,  1.34775648e-02,  2.77156215e-02,
       -4.86349640e-03, -3.12581174e-02, -3.55921760e-02,  5.76934684e-03,
        2.80773244e-03,  1.35783911e-01,  3.59678417e-02,  6.17732145e-02,
</span><span class="c">...
</span><span class="go">       -4.61330153e-02, -4.85207550e-02,  3.13997865e-02,  7.82178566e-02,
       -4.75336798e-02,  5.21207601e-02,  9.04406682e-02, -5.36676683e-02],
      dtype=float32)]
Dim: 384 (384,)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># To create embeddings for queries, use the encode_queries() method:
</span>
<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">When was artificial intelligence founded</span><span class="sh">"</span><span class="p">,</span>
           <span class="sh">"</span><span class="s">Where was Alan Turing born?</span><span class="sh">"</span><span class="p">]</span>

<span class="n">query_embeddings</span> <span class="o">=</span> <span class="n">sentence_transformer_ef</span><span class="p">.</span><span class="nf">encode_queries</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>

<span class="c1"># Print embeddings
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Embeddings:</span><span class="sh">"</span><span class="p">,</span> <span class="n">query_embeddings</span><span class="p">)</span>
<span class="c1"># Print dimension and shape of embeddings
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Dim:</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence_transformer_ef</span><span class="p">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">query_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Embeddings: [array([-2.52114702e-02, -5.29330298e-02,  1.14570223e-02,  1.95571519e-02,
       -2.46500354e-02, -2.66519729e-02, -8.48201662e-03,  2.82961670e-02,
       -3.65092754e-02,  7.50745758e-02,  4.28900979e-02,  7.18822703e-02,
</span><span class="c">...
</span><span class="go">       -6.76431581e-02, -6.45996556e-02, -4.67132553e-02,  4.78532910e-02,
       -2.31596199e-03,  4.13446948e-02,  1.06935494e-01, -1.08258888e-01],
      dtype=float32)]
Dim: 384 (384,)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="references">References</h2>
<div class="sectionbody">
<div class="ulist bibliography">
<ul class="bibliography">
<li>
<p><a id="milvus-overview"></a>[1] <a href="https://milvus.io/docs/overview.md" class="bare">https://milvus.io/docs/overview.md</a></p>
</li>
<li>
<p><a id="milvus-architecture_overview"></a>[2] <a href="https://milvus.io/docs/architecture_overview.md" class="bare">https://milvus.io/docs/architecture_overview.md</a></p>
</li>
<li>
<p><a id="milvus-install_standalone-docker-compose"></a>[3] <a href="https://milvus.io/docs/install_standalone-docker-compose.md" class="bare">https://milvus.io/docs/install_standalone-docker-compose.md</a></p>
</li>
<li>
<p><a id="milvus_lite"></a>[4] <a href="https://milvus.io/docs/milvus_lite.md" class="bare">https://milvus.io/docs/milvus_lite.md</a></p>
</li>
<li>
<p><a id="milvus-quickstart"></a>[5] <a href="https://milvus.io/docs/quickstart.md" class="bare">https://milvus.io/docs/quickstart.md</a></p>
</li>
<li>
<p><a id="milvus-cli_overview"></a>[6] <a href="https://milvus.io/docs/cli_overview.md" class="bare">https://milvus.io/docs/cli_overview.md</a></p>
</li>
<li>
<p><a id="milvus-schema"></a>[7] <a href="https://milvus.io/docs/schema.md" class="bare">https://milvus.io/docs/schema.md</a></p>
</li>
<li>
<p><a id="milvus-manage-collections"></a>[8] <a href="https://milvus.io/docs/manage-collections.md" class="bare">https://milvus.io/docs/manage-collections.md</a></p>
</li>
<li>
<p><a id="milvus-embeddings"></a>[9] <a href="https://milvus.io/docs/embeddings.md" class="bare">https://milvus.io/docs/embeddings.md</a></p>
</li>
</ul>
</div>
</div>
</div>
<style>
  .utterances {
      max-width: 100%;
  }
</style>
<script src="https://utteranc.es/client.js"
        repo="looogos/utterances"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

</div>
</article>
    </main>
    <footer class="c-footer">
  <div class="c-footer-license">
    <span>Article licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span>
  </div>
  
  <details class="c-footer-extralinks" open>
    <summary class="c-footer-extralinks-summary">Extral Links</summary>
    <div class="c-footer-extralinks-content">
      
      <a href="https://jekyllrb.com/">Jekyll</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://shopify.github.io/liquid/">Liquid</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://docs.asciidoctor.org/">Asciidoctor</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://github.com/qqbuby/">GitHub</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="/feed.xml">RSS</a>
      
      
    </div>
  </details>
  
</footer>

    <script src="/assets/js/nav.js" defer></script>
    <script src="/assets/js/heading-anchors.js" defer></script>
    <!-- https://cdn.jsdelivr.net/gh/lurongkai/anti-baidu/js/anti-baidu-latest.min.js -->    
    <script type="text/javascript" src="/js/anti-baidu.min.js" charset="UTF-8"></script>
  </body>
</html>
