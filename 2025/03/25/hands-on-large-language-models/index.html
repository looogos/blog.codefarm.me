<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Hands-On Large Language Models | CODE FARM</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Hands-On Large Language Models" />
<meta property="og:locale" content="en" />
<meta name="description" content="1. Language AI 2. Tokens and Embeddings 2.1. LLM Tokenization 2.2. Token Embeddings 2.3. Text Embeddings 3. Large Language Models 3.1. Inputs and Outputs 3.2. Components 3.3. Probability Distribution (Sampling/Decoding) 3.4. Parallel Token Processing and Context Size 3.5. Keys and Values Caching 3.6. Transformer Block 4. Text Classification 4.1. Representation Models 4.1.1. Task-Specific Model 4.1.2. Embedding model 4.2. Generative Models 4.2.1. Text-to-Text Transfer Transformer 4.2.2. ChatGPT for Classification 5. Text Clustering and Topic Modeling 5.1. ArXiv’s Articles: Computation and Language 5.2. A Common Pipeline for Text Clustering 5.2.1. Embedding Documents 5.2.2. Reducing the Dimensionality of Embeddings 5.2.3. Cluster the Reduced Embeddings 5.2.4. Inspecting the Clusters 5.3. From Text Clustering to Topic Modeling 5.3.1. BERTopic: A Modular Topic Modeling Framework 6. Prompt Engineering 6.1. Using Text Generation Models 6.1.1. Prompt Template 6.1.2. Controlling Model Output 6.2. Prompt Engineering 6.3. Instruction-Based Prompting 6.4. Advanced Prompt Engineering 6.4.1. Prompt Components 6.4.2. In-Context Learning: Providing Examples 6.4.3. Chain Prompting: Breaking up the Problem 6.5. Reasoning with Generative Models 6.5.1. Chain-of-Thought: Think Before Answering 6.5.2. Self-Consistency: Sampling Outputs 6.5.3. Tree-of-Thought: Exploring Intermediate Steps 6.6. Output Verification 6.6.1. Providing Examples 6.6.2. Grammar: Constrained Sampling 7. Advanced Text Generation Techniques and Tools 7.1. Model I/O: Loading Quantized Models with LangChain 7.2. Chains: Extending the Capabilities of LLMs 7.2.1. A Single Link in the Chain: Prompt Template 7.2.2. A Chain with Multiple Prompts 7.3. Memory: Helping LLMs to Remember Conversations 7.3.1. Conversation Buffer 7.3.2. Windowed Conversation Buffer 7.3.3. Conversation Summary 7.4. Agents: Creating a System of LLMs Appendix A: LangChain 7.A.1. Chat Models and Messages 7.A.2. Prompt Templates 7.A.3. Structured Outputs 7.A.4. Output Parsers 7.A.5. Embedding, Vector Stores, and Retrievers 7.A.6. Document Loaders 7.A.7. Text Splitters 7.A.8. Tools 7.A.9. Chat History 7.A.10. Memory 7.A.11. LangChain Expression Language (LCEL) 8. Semantic Search and Retrieval-Augmented Generation 8.1. Semantic Search with Language Models 8.1.1. Dense Retrieval 8.1.2. Reranking 8.2. Retrieval-Augmented Generation (RAG) 9. Multimodal Large Language Models 9.1. Vision Transformer (ViT) 9.2. Multimodal Embedding Models 9.3. Multimodal Text Generation Models 9.3.1. BLIP-2: Bridging the Modality Gap 9.3.2. Preprocessing Multimodal Inputs 9.3.3. Use Case 1: Image Captioning 9.3.4. Use Case 2: Multimodal Chat-Based Prompting 10. Creating and Fine-Tuning Text Embedding Models 10.1. Contrastive Learning 10.2. Sentence Transformers (SBERT) 10.3. Creating an Embedding Model References 1. Language AI Google Colab offers free, cloud-based GPU and TPU access for accelerated computation, subject to usage limits, and requires changing the runtime type to GPU to enable it. Artificial Intelligence (AI) is the science and engineering of creating intelligent machines, particularly intelligent computer programs, that can perform tasks similar to human intelligence. Language AI is a subfield of AI focused on developing technologies that can understand, process, and generate human language, which is often used interchangeably with Natural Language Processing (NLP). Figure 1. A peek into the history of Language AI. Figure 2. Language AI is capable of many tasks by processing textual input. The Bag-of-Words, a representation model, converts text to numerical vectors by tokenizing it—splitting sentences into individual words or subwords (tokens)—creating a vocabulary, and counting token occurrences to form a vector representation (the &#39;bag of words&#39;). Figure 3. A bag-of-words is created by counting individual words. These values are referred to as vector representations. Word2vec introduced dense vector embeddings, a significant improvement over Bag-of-Words, by using neural networks to capture the semantic meaning of words based on their context within large datasets, allowing for the measurement of semantic similarity. Figure 4. Embeddings of words that are similar will be close to each other in dimensional space. Figure 5. Embeddings can be created for different types of input. Attention-based Transformer models, replacing RNNs which struggled with long sentences, enabled parallel processing and context-aware language representation by using stacked encoders and decoders to focus on relevant input, revolutionizing language AI. Figure 6. Using word2vec embeddings, a context embedding is generated that represents the entire sequence. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder. Figure 7. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder. Figure 8. The encoder block revolves around self-attention to generate intermediate representations. Figure 9. The decoder has an additional attention layer that attends to the output of the encoder. Encoder-only models (a.k.a., representation models) like Bidirectional Encoder Representations from Transformers(BERT) excel at language representation through masked language modeling, while decoder-only models (a.k.a., generative models) like Generative Pre-trained Transformer (GPT) focus on text generation and are the foundation for large language models. Figure 10. The architecture of a BERT base model with 12 encoders. Figure 11. The architecture of a GPT-1. It uses a decoder-only architecture and removes the encoder-attention block. Generative LLMs function as sequence-to-sequence machines, initially designed for text completion, but their capability to be fine-tuned into chatbots or instruct models that can follow user prompts revealed their true potential. Figure 12. Generative LLMs take in some input and try to complete it. With instruct models, this is more than just autocomplete and attempts to answer the question. The context length, or window, represents the maximum number of tokens the model can process, enabling the generative LLM to handle larger documents, and the current length expands as the model generates new tokens due to its autoregressive nature. Figure 13. The context length is the maximum context an LLM can handle. LLMs differ from traditional machine learning by using a two-step training process: pretraining, for general language learning, and fine-tuning (or post-training), to adapt the pretrained (foundation/base) model for specific tasks. Figure 14. Compared to traditional machine learning, LLM training takes a multistep approach. Closed-source LLMs, like GPT-4 and Claude, are models that do not have their weights and architecture shared with the public, which are accessed via APIs, and offer high performance with managed hosting, but are costly and limit user control; open LLMs, such as Llama, share their architecture, enabling local use, fine-tuning, and privacy, but require powerful hardware and expertise. The main source for finding and downloading LLMs is the Hugging Face Hub. Hugging Face is the organization behind the well-known Transformers package, which for years has driven the development of language models in general. # If a connection to the Hugging Face URL (https://huggingface.co/) fails, try to set the HF_ENDPOINT environment variable to the mirror URL. import os os.environ[&quot;HF_ENDPOINT&quot;] = &quot;https://hf-mirror.com&quot; Hugging Face, the organization behind the Transformers package, is the primary source for finding and downloading LLMs, built upon the Transformer framework. import os from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # HF_ENDPOINT controls the base URL used by the transformers library # to download models and other resources from the Hugging Face Hub. os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # create a pipeline pipe = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=True, ) # the prompt (user input / query) messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a funny joke about chickens.&quot;}] # generate output output = pipe(messages) print(output[0][&quot;generated_text&quot;]) Why did the chicken join the band? Because he heard they had the &quot;cluck-loudest&quot; performers around! # clear memory and empty the VRAM import gc import torch # attempt to delete the model, tokenizer, and pipeline objects from memory del model, tokenizer, pipe # flush memory gc.collect() if torch.cuda.is_available(): # if a GPU is available, empty the CUDA cache to free up GPU memory torch.cuda.empty_cache() 2. Tokens and Embeddings Tokens and embeddings are two of the central concepts of using large language models (LLMs). Figure 15. Language models deal with text in small chunks called tokens. For the lan‐ guage model to compute language, it needs to turn tokens into numeric representations called embeddings. 2.1. LLM Tokenization import os import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # HF_ENDPOINT controls the base URL used by the transformers library # to download models and other resources from the Hugging Face Hub. os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) prompt = &#39;&lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;&#39; # tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=&#39;pt&#39;).input_ids.to(dev) print(f&#39;input_ids: {input_ids}&#39;) # generate the text output_ids = model.generate(input_ids=input_ids, max_new_tokens=20) print(f&#39;output_ids: {output_ids}&#39;) # print the output print(tokenizer.decode(output_ids[0])) input_ids: tensor([[101950, 29, 16465, 448, 3719, 39950, 6396, 316, 32145, 395, 290, 62374, 66241, 80785, 403, 13, 115474, 1495, 480, 12570, 13, 200019]]) output_ids: tensor([[101950, 29, 16465, 448, 3719, 39950, 6396, 316, 32145, 395, 290, 62374, 66241, 80785, 403, 13, 115474, 1495, 480, 12570, 13, 200019, 18174, 25, 336, 2768, 512, 6537, 10384, 395, 290, 193145, 147276, 403, 279, 36210, 32145, 4464, 40, 5498, 495, 3719]]) &lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;Subject: Sincere Apologies for the Gardening Mishap Dear Sarah, I hope this email Tokens, the units into which text prompts are broken for model input, also form the model&#8217;s output. Figure 16. A tokenizer encodes input prompts into token ID lists for the language model and decodes the model&#8217;s output token IDs back into words or tokens. Each ID corresponds to a specific token (character, word, or subword) in the tokenizer&#8217;s vocabulary. The tokenizer&#8217;s vocabulary acts as a lookup table, allowing the model to convert between text and these integer representations. for id in [101950, 29, 16465, 448, 3719, 39950]: print(tokenizer.decode(id)) # &lt;s # &gt; # Write # an # email # apolog for id in [18174, 25, 336, 2768, 512]: print(tokenizer.decode(id) # Subject # : # S # inc # ere Tokenization is determined by three major design decisions: the tokenizer algorithm (e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size, special tokens, capitalization, treatment of capitalization and different languages), and the dataset the tokenizer is trained on (a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset). Tokenization methods vary in granularity, from word-level to byte-level, with subword tokenization offering a balance of vocabulary expressiveness and efficiency, making it the most common approach in modern language models. 2.2. Token Embeddings Text --&gt; Tokens --&gt; Token IDs --&gt; Embeddings (Vectors) A tokenizer, once trained, becomes intrinsically linked to its language model during the model&#8217;s training; consequently, a pretrained language model cannot function with a different tokenizer without retraining, as their vocabularies and tokenization schemes are aligned. An embedding is a dense, numerical vector representation of a token (like a word or subword) that captures its semantic meaning within a high-dimensional space, enabling language models to understand and process relationships between words. A language model stores static embedding vectors for each token in its vocabulary, but also generates contextualized word embeddings, dynamically representing a token based on its context instead of a single, fixed vector. A language model holds an embedding vector associated with each token in its tokenizer. Figure 17. A language model holds an embedding vector associated with each token in its tokenizer. A language model operates on raw, static embeddings as its input and produces contextual text embeddings. Figure 18. A language model operates on raw, static embeddings as its input and produces contextual text embeddings. from transformers import AutoModel, AutoTokenizer # load a tokenizer tokenizer = AutoTokenizer.from_pretrained(&#39;microsoft/deberta-base&#39;) # load a language model model = AutoModel.from_pretrained(&#39;microsoft/deberta-v3-xsmall&#39;) # tokenize the sentence: convert text to token IDs tokens = tokenizer(&#39;Hello world&#39;, return_tensors=&#39;pt&#39;) # print the decoded tokens to show tokenization for token_id in tokens[&#39;input_ids&#39;][0]: print(tokenizer.decode(token_id)) print(&#39;\n&#39;) # process the token IDs through the model to get contextualized embeddings output = model(**tokens)[0] # show the shape of the embedding result print(f&#39;{output.shape}\n&#39;) # output contains the contextualized embedding vectors print(output) [CLS] Hello world [SEP] torch.Size([1, 4, 384]) tensor([[[-3.4816, 0.0861, -0.1819, ..., -0.0612, -0.3911, 0.3017], [ 0.1898, 0.3208, -0.2315, ..., 0.3714, 0.2478, 0.8048], [ 0.2071, 0.5036, -0.0485, ..., 1.2175, -0.2292, 0.8582], [-3.4278, 0.0645, -0.1427, ..., 0.0658, -0.4367, 0.3834]]], grad_fn=&lt;NativeLayerNormBackward0&gt;) 2.3. Text Embeddings Text embeddings are single, dense vectors that represent the semantic meaning of entire sentences, paragraphs, or documents, in contrast to token embeddings, which represent individual words or subwords. from sentence_transformers import SentenceTransformer # load model model = SentenceTransformer(&#39;sentence-transformers/all-MiniLM-L6-v2&#39;) # convert text to text embeddings embeddings = model.encode(&quot;Best movie ever!&quot;) print(embeddings.shape) # (384,) Input Sequence Length: https://www.sbert.net/ For transformer models like BERT, RoBERTa, DistilBERT etc., the runtime and memory requirement grows quadratic with the input length. This limits transformers to inputs of certain lengths. A common value for BERT-based models are 512 tokens, which corresponds to about 300-400 words (for English). Each model has a maximum sequence length under model.max_seq_length, which is the maximal number of tokens that can be processed. Longer texts will be truncated to the first model.max_seq_length tokens: from sentence_transformers import SentenceTransformer model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;) print(&quot;Max Sequence Length:&quot;, model.max_seq_length) # =&gt; Max Sequence Length: 256 # Change the length to 200 model.max_seq_length = 200 print(&quot;Max Sequence Length:&quot;, model.max_seq_length) # =&gt; Max Sequence Length: 200 3. Large Language Models import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # create a pipeline generator = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=50, do_sample=False, ) 3.1. Inputs and Outputs The most common picture of understanding the behavior of a Transformer LLM is to think of it as a software system that takes in text and generates text in response. Once a large enough text-in-text-out model is trained on a large enough high-quality dataset, it becomes able to generate impressive and useful outputs. Figure 19. At a high level of abstraction, Transformer LLMs take a text prompt and output generated text. The model does not generate the text all in one operation; it actually generates one token at a time. Figure 20. Transformer LLMs generate one token at a time, not the entire text at once. Each token generation step is one forward pass through the model (that’s machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph). After each token generation, the input prompt for the next generation step is tweaked by appending the output token to the end of the input prompt. Figure 21. An output token is appended to the prompt, then this new text is presented to the model again for another forward pass to generate the next token. Text generation LLMs are called autoregressive models because they generate text sequentially, using prior outputs as input, unlike text representation models like BERT, which process the entire input at once. 3.2. Components A language model consists of a tokenizer, a stack of Transformer blocks for processing, and an LM head that converts the processed information into probability scores for the next token. Figure 22. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head. The model has a vector representation associated with each of these tokens in the vocabulary (token embeddings). Figure 23. The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings. For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probability distribution for the next token. Figure 24. At the end of the forward pass, the model predicts a probability score for each token in the vocabulary. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) print(model) Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(200064, 3072, padding_idx=199999) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear(in_features=3072, out_features=3072, bias=False) (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False) ) (mlp): Phi3MLP( (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False) (down_proj): Linear(in_features=8192, out_features=3072, bias=False) (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (resid_attn_dropout): Dropout(p=0.0, inplace=False) (resid_mlp_dropout): Dropout(p=0.0, inplace=False) ) ) (norm): Phi3RMSNorm((3072,), eps=1e-05) (rotary_emb): Phi3RotaryEmbedding() ) (lm_head): Linear(in_features=3072, out_features=200064, bias=False) ) 3.3. Probability Distribution (Sampling/Decoding) Language models use a probability distribution to determine the next token, which is called the decoding strategy. The easiest strategy would be to always pick the token with the highest probability score, which is called greedy decoding (equivalent to setting the temperature to zero in an LLM). In practice, this doesn’t tend to lead to the best outputs for most use cases. A better approach is to introduce randomness by sampling from the probability distribution, sometimes choosing the second or third highest probability token. 3.4. Parallel Token Processing and Context Size Transformers excel at parallel processing, unlike earlier architectures, which is evident in how they handle token generation. Each input token is processed simultaneously through its own computation path or stream. Figure 25. Each token is processed through its own stream of computation (with some interaction between them in attention steps). A model with 4K context length or context size can only process 4K tokens and would only have 4K of these streams. Each of the token streams starts with an input vector (the embedding vector and some positional information). Figure 26. Each processing stream takes a vector as input and produces a final resulting vector of the same size (often referred to as the model dimension). At the end of the stream, another vector emerges as the result of the model’s processing. For text generation, only the output result of the last stream is used to predict the next token. That output vector is the only input into the LM head as it calculates the probabilities of the next token. 3.5. Keys and Values Caching Transformer models use a key/value (KV) cache to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), speeding up text generation by avoiding redundant calculations. Figure 27. When generating text, it’s important to cache the computation results of previous tokens instead of repeating the same calculation over and over again. In Hugging Face Transformers, cache is enabled by default, and can be disabled it by setting use_cache to False. prompt = &#39;Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&#39; input_ids = tokenizer(prompt, return_tensors=&#39;pt&#39;).input_ids.to(dev) generation_output = model.generate( input_ids=input_ids, max_new_tokens=100, use_cache=False, ) 3.6. Transformer Block Transformer LLMs are composed of a series Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs) and each block processes its inputs, then passes the results of its processing to the next block. Figure 28. The bulk of the Transformer LLM processing happens inside a series of Transformer blocks, each handing the result of its processing as input to the subsequent block. A Transformer block is made up of two successive components: Figure 29. A Transformer block is made up of a self-attention layer and a feedforward neural network. The attention layer is mainly concerned with incorporating relevant information from other input tokens and positions The feedforward layer houses the majority of the model’s processing capacity The feedforward network in a Transformer model stores learned information, such as &#39;The Shawshank&#39; and &#39;Redemption,&#39; and enables interpolation and generalization for generating text on unseen inputs. Figure 30. The feedforward neural network component of a Transformer block likely does the majority of the model’s memorization and interpolation. The attention layer in a Transformer model enables context awareness, crucial for language understanding beyond simple memorization. Figure 31. The self-attention layer incorporates relevant information from previous positions that help process the current token. 4. Text Classification A common task in natural language processing is classification, where the goal is to train a model to assign a label or class to input text, a technique widely used in applications like sentiment analysis and intent detection, significantly impacted by both representative and generative language models. Figure 32. Although both representation and generative models can be used for classification, their approaches differ. The Hugging Face Hub is a collaborative platform for machine learning resources (models, datasets, applications), and the datasets package can be used to load datasets. The dataset is split into train (for training), test (for final evaluation), and validation (for intermediate generalization checks, especially during hyperparameter tuning). from datasets import load_dataset # load data data = load_dataset(&quot;rotten_tomatoes&quot;) # the well-known &#39;rotten_tomatoes&#39; dataset data DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 8530 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 1066 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 1066 }) }) 4.1. Representation Models Classification with pretrained representation models generally comes in two flavors, either using a task-specific model or an embedding model. Figure 33. A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings. A task-specific model is a representation model, such as BERT, trained for a specific task, like sentiment analysis. An embedding model generates general-purpose embeddings that can be used for a variety of tasks not limited to classification, like semantic search. Figure 34. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings. 4.1.1. Task-Specific Model from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&quot;rotten_tomatoes&quot;) # determine the device to use for computation (GPU if available, otherwise CPU) import torch dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; from transformers import pipeline # specify the path to the pre-trained Twitter-RoBERTa-base for Sentiment Analysis model model_path = &quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot; # load the pre-trained sentiment analysis model into a pipeline for easy inference pipe = pipeline( model=model_path, tokenizer=model_path, return_all_scores=True, # return the scores for all sentiment labels device=dev, # specify the device to run the pipeline on ) import numpy as np from tqdm import tqdm # for progress bar during inference from transformers.pipelines.pt_utils import KeyDataset # utility to feed data to the pipeline # run inference on the test dataset y_pred = [] # list to store the predicted sentiment labels for output in tqdm( # iterate through the &#39;text&#39; column of the test dataset pipe(KeyDataset(data[&quot;test&quot;], &quot;text&quot;)), total=len(data[&quot;test&quot;]) ): # extract the negative sentiment score negative_score = output[0][&quot;score&quot;] # extract the positive sentiment score (assuming labels are ordered: negative, neutral, positive) positive_score = output[2][&quot;score&quot;] # predict the sentiment based on the highest score (0 for negative, 1 for positive) assignment = np.argmax([negative_score, positive_score]) # add the predicted label to the list y_pred.append(assignment) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&quot;Negative Review&quot;, &quot;Positive Review&quot;] ) print(performance) # evaluate the performance of the sentiment analysis model on the test set evaluate_performance(data[&quot;test&quot;][&quot;label&quot;], y_pred) # compare the true labels with the predicted labels precision recall f1-score support Negative Review 0.76 0.88 0.81 533 Positive Review 0.86 0.72 0.78 533 accuracy 0.80 1066 macro avg 0.81 0.80 0.80 1066 weighted avg 0.81 0.80 0.80 1066 The above generated classification report shows four such methods: precision, recall, accuracy, and the F1 score. Precision measures how many of the items found are relevant, which indicates the accuracy of the relevant results. Recall refers to how many relevant classes were found, which indicates its ability to find all relevant results. Accuracy refers to how many correct predictions the model makes out of all predictions, which indicates the overall correctness of the model. The F1 score balances both precision and recall to create a model’s overall performance. A confusion matrix visualizes the performance of a classification model by showing the counts of four prediction outcomes: True Positives, True Negatives, False Positives, and False Negatives, which serves as the basis for calculating various metrics to evaluate the model&#8217;s quality. Figure 35. The confusion matrix describes four types of predictions. Figure 36. The classification report describes several metrics for evaluating a model’s performance. 4.1.2. Embedding model Without fine-tuning a representation model, a general-purpose embedding model can generate features that are then fed into a separate, trainable classifier (like logistic regression, which can be trained efficiently on a CPU), creating a two-step classification approach. A major benefit of this separation is avoiding the costly fine-tuning of the embedding model, instead, a classifier, such as logistic regression, can be trained efficiently on the CPU. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&quot;rotten_tomatoes&quot;) # load the SentenceTransformer model for generating text embeddings from sentence_transformers import SentenceTransformer model = SentenceTransformer(&quot;sentence-transformers/all-mpnet-base-v2&quot;) # convert the text data from the train and test splits into embeddings train_embeddings = model.encode(data[&quot;train&quot;][&quot;text&quot;], show_progress_bar=True) test_embeddings = model.encode(data[&quot;test&quot;][&quot;text&quot;], show_progress_bar=True) from sklearn.linear_model import LogisticRegression # train a logistic regression classifier on the generated training embeddings # initialize the logistic regression model with a random state for reproducibility clf = LogisticRegression(random_state=42) # train the classifier using the training embeddings and their corresponding labels clf.fit(train_embeddings, data[&quot;train&quot;][&quot;label&quot;]) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&quot;Negative Review&quot;, &quot;Positive Review&quot;] ) print(performance) # predict the sentiment labels for the test embeddings using the trained classifier y_pred = clf.predict(test_embeddings) # evaluate the performance of the classifier on the test set evaluate_performance(data[&quot;test&quot;][&quot;label&quot;], y_pred) precision recall f1-score support Negative Review 0.85 0.86 0.85 533 Positive Review 0.86 0.85 0.85 533 accuracy 0.85 1066 macro avg 0.85 0.85 0.85 1066 weighted avg 0.85 0.85 0.85 1066 Zero-shot classification can be used on unlabeled data by leveraging the model&#8217;s pre-existing knowledge to predict labels based solely on their definitions. In zero-shot classification, without any labeled examples, the model determines the relationship between input text and predefined candidate labels. Figure 37. In zero-shot classification, we have no labeled data, only the labels them‐ selves. The zero-shot model decides how the input is related to the candidate labels. Zero-shot classification generates target labels without labeled data by describing and embedding labels (e.g., &quot;negative movie review&quot;) and documents. Figure 38. To embed the labels, we first need to give them a description, such as “a negative movie review.” This can then be embedded through sentence-transformers. To assign labels to documents in zero-shot classification, cosine similarity, representing the cosine of the angle between the embedding vectors, can be applied to document-label embedding pairs. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) from sentence_transformers import SentenceTransformer # load model model = SentenceTransformer(&#39;sentence-transformers/all-mpnet-base-v2&#39;) # convert text to embeddings train_embeddings = model.encode(data[&#39;train&#39;][&#39;text&#39;], show_progress_bar=True) test_embeddings = model.encode(data[&#39;test&#39;][&#39;text&#39;], show_progress_bar=True) # create embeddings for our labels label_embeddings = model.encode([&#39;A negative review&#39;, &#39;A positive review&#39;]) import numpy as np from sklearn.metrics.pairwise import cosine_similarity # find the best matching label for each document using cosine similarity sim_matrix = cosine_similarity(test_embeddings, label_embeddings) # get the index of the label with the highest similarity score for each test embedding y_pred = np.argmax(sim_matrix, axis=1) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) precision recall f1-score support Negative Review 0.78 0.77 0.78 533 Positive Review 0.77 0.79 0.78 533 accuracy 0.78 1066 macro avg 0.78 0.78 0.78 1066 weighted avg 0.78 0.78 0.78 1066 From Wikipedia, the free encyclopedia In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval [−1, 1]. import numpy as np # import the NumPy library for numerical operations A = np.array([1, 2, 3]) # create a NumPy array named A B = np.array([4, 5, 6]) # create a NumPy array named B # calculate the cosine similarity using the formula: (A dot B) / (||A|| * ||B||) dot_product = np.dot(A, B) # calculate the dot product of A and B norm_A = np.linalg.norm(A) # calculate the Euclidean norm (magnitude) of A norm_B = np.linalg.norm(B) # calculate the Euclidean norm (magnitude) of B cosine_similarity = dot_product / (norm_A * norm_B) # calculate the cosine similarity print(cosine_similarity) # 0.9746318461970762 4.2. Generative Models Text classification with generative language models (like GPT) involves feeding input text to the model and having it generate text as output, in contrast to task-specific models that directly output a class label. Figure 39. A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens. Generative models are generally trained on a wide variety of tasks and usually don&#8217;t inherently know how to handle specific tasks like classifying a movie review without explicit instructions. Prompt engineering is the skill of crafting effective instructions, or prompts, to guide generative AI models towards producing desired and high-quality outputs for specific tasks, like text classification, which often involves iterative refinement of these prompts based on the model&#8217;s responses. Figure 40. Prompt engineering allows prompts to be updated to improve the output generated by the model. 4.2.1. Text-to-Text Transfer Transformer Text-to-Text Transfer Transformer or T5, like the original Transformer, is a generative encoder-decoder sequence-to-sequence model, contrasting with encoder-only BERT and decoder-only GPT. Figure 41. The T5 architecture is similar to the original Transformer model, a decoder- encoder architecture. In the first step of training, namely pretraining, encoder-decoder models like T5 are initially trained with a masked language modeling objective that masks sets of tokens (or token spans), differing from BERT&#8217;s individual token masking approach. Figure 42. In the first step of training, namely pretraining, the T5 model needs to predict masks that could contain multiple tokens. In the second step of training, namely fine-tuning the base model, instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously. Figure 43. By converting specific tasks to textual instructions, the T5 model can be trained on a variety of tasks during fine-tuning. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) import torch # determine the device to use for computation (GPU if available, otherwise CPU) dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; from transformers import pipeline # specify the path to the pre-trained FLAN-T5-small model for text-to-text generation model_path = &#39;google/flan-t5-small&#39; # load the pre-trained text-to-text generation model into a pipeline for easy inference pipe = pipeline( &#39;text2text-generation&#39;, model=model_path, device=dev, ) # prepare our data by creating a prompt and combining it with the text prompt = &#39;Is the following sentence positive or negative? &#39; # apply the prompt to each example in the dataset&#39;s &#39;text&#39; column to create a new &#39;t5&#39; column data = data.map(lambda example: {&#39;t5&#39;: prompt + example[&#39;text&#39;]}) # data # uncomment to inspect the modified dataset from tqdm import tqdm # for progress bar during inference from transformers.pipelines.pt_utils import ( KeyDataset, ) # utility to feed data to the pipeline # Run inference y_pred = [] # iterate through the test dataset using the pipeline for text generation for output in tqdm( pipe(KeyDataset(data[&#39;test&#39;], &#39;t5&#39;)), total=len(data[&#39;test&#39;]) ): # extract the generated text from the pipeline&#39;s output text = output[0][&#39;generated_text&#39;] # classify the generated text as 0 (negative) if it equals &#39;negative&#39;, otherwise 1 (positive) y_pred.append(0 if text == &#39;negative&#39; else 1) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) # evaluate the performance of the model by comparing the true labels with the predicted labels evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) precision recall f1-score support Negative Review 0.83 0.85 0.84 533 Positive Review 0.85 0.83 0.84 533 accuracy 0.84 1066 macro avg 0.84 0.84 0.84 1066 weighted avg 0.84 0.84 0.84 1066 4.2.2. ChatGPT for Classification OpenAI shared an overview of the training procedure that involved an important component, namely preference tuning. OpenAI first manually created the desired output to an input prompt (instruction data) and used that data to create a first variant of its model. Figure 44. Manually labeled data consisting of an instruction (prompt) and output was used to perform fine-tuning (instruction-tuning). OpenAI used the resulting model to generate multiple outputs that were manually ranked from best to worst. Figure 45. Manually ranked preference data was used to generate the final model, ChatGPT. import openai # create client for interacting with OpenAI API client = openai.OpenAI(api_key=&#39;YOUR_KEY_HERE&#39;) def chatgpt_generation(prompt, document, model=&#39;gpt-3.5-turbo-0125&#39;): &#39;&#39;&#39;Generate an output based on a prompt and an input document using ChatGPT.&#39;&#39;&#39; # define the message structure for the OpenAI API messages = [ {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: prompt.replace(&#39;[DOCUMENT]&#39;, document)}, ] # call the OpenAI Chat Completions API to get a response chat_completion = client.chat.completions.create( messages=messages, model=model, temperature=0 # temperature=0 for deterministic output ) # return the content of the first choice&#39;s message return chat_completion.choices[0].message.content # define a prompt template as a base for sentiment classification prompt = &#39;&#39;&#39;Predict whether the following document is a positive or negative movie review: [DOCUMENT] If it is positive return 1 and if it is negative return 0. Do not give any other answers. &#39;&#39;&#39; # predict the target for a single document using GPT document = &#39;unpretentious , charming , quirky , original&#39; chatgpt_generation(prompt, document) from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) from tqdm import tqdm # generate predictions for all documents in the test set predictions = [ chatgpt_generation(prompt, doc) for doc in tqdm(data[&#39;test&#39;][&#39;text&#39;]) ] # convert the string predictions (&#39;0&#39; or &#39;1&#39;) to integers y_pred = [int(pred) for pred in predictions] from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) # evaluate the performance of ChatGPT on the test set evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) 5. Text Clustering and Topic Modeling Although supervised techniques, such as classification, have reigned supreme over the last few years in the industry, the potential of unsupervised techniques such as text clustering cannot be understated. Text clustering aims to group similar texts based on their semantic content, meaning, and relationships. Figure 46. Clustering unstructured textual data. Text clustering is also applied in topic modeling to uncover abstract topics within large textual datasets. Figure 47. Topic modeling is a way to give meaning to clusters of textual documents. 5.1. ArXiv’s Articles: Computation and Language ArXiv is an open-access platform for scholarly articles, mostly in the fields of computer science, mathematics, and physics. from datasets import load_dataset # load the &#39;arxiv_nlp&#39; dataset from Hugging Face Datasets library dataset = load_dataset(&quot;maartengr/arxiv_nlp&quot;)[&quot;train&quot;] # extract metadata abstracts = dataset[&quot;Abstracts&quot;] titles = dataset[&quot;Titles&quot;] 5.2. A Common Pipeline for Text Clustering Text clustering enables the discovery of both known and unknown data patterns, providing an intuitive understanding of tasks like classification and their complexity, making it valuable beyond just exploratory data analysis. Although there are many methods for text clustering, from graph-based neural networks to centroid-based clustering techniques, a common pipeline that has gained popularity involves three steps and algorithms: Convert the input documents to embeddings with an embedding model. Figure 48. Step 1: We convert documents to embeddings using an embedding model. Reduce the dimensionality of embeddings with a dimensionality reduction model. Figure 49. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction. Find groups of semantically similar documents with a cluster model. Figure 50. Step 3: We cluster the documents using the embeddings with reduced dimensionality. 5.2.1. Embedding Documents from sentence_transformers import SentenceTransformer # create an embedding model using a pre-trained Sentence Transformer model embedding_model = SentenceTransformer(&#39;thenlper/gte-small&#39;) (1) # generate embeddings for each abstract in the &#39;abstracts&#39; list embeddings = embedding_model.encode(abstracts, show_progress_bar=True) # check the dimensions (shape) of the resulting embeddings embeddings.shape # (44949, 384) (2) 1 The thenlper/gte-small model is a more recent model that outperforms the previous model on clustering tasks and due to its small size is even faster for inference. 2 The embeddings.shape of (44949, 384) shows that there are 44,949 abstract embeddings, each with a dimensionality of 384. 5.2.2. Reducing the Dimensionality of Embeddings Reducing the dimensionality of embeddings is essential before clustering high-dimensional data to simplify the representation and enhance clustering effectiveness. Dimensionality reduction is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions. Figure 51. Dimensionality reduction allows data in high-dimensional space to be compressed to a lower-dimensional representation. Well-known methods for dimensionality reduction are Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP). from umap import UMAP # reduce the input embeddings from 384 dimensions to 5 dimensions using UMAP umap_model = UMAP( # generally, values between 5 and 10 work well to capture high-dimensional global structures. n_components=5, # the number of dimensions to reduce to min_dist=0.0, # the effective minimum distance between embedded points metric=&#39;cosine&#39;, # the metric to use to compute distances in high dimensional space random_state=42, # for reproducibility of the embedding ) # fit and then transform the embeddings to the lower-dimensional space reduced_embeddings = umap_model.fit_transform(embeddings) 5.2.3. Cluster the Reduced Embeddings While k-means, a centroid-based algorithm needing a predefined number of clusters, is common, density-based algorithms are preferable when the number of clusters is unknown as they automatically determine the clusters and don&#8217;t require all data points to belong to one. Figure 52. The clustering algorithm not only impacts how clusters are generated but also how they are viewed. A common density-based model is Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). from hdbscan import HDBSCAN # initialize and fit the HDBSCAN clustering model hdbscan_model = HDBSCAN( # the minimum number of samples in a group for it to be considered a cluster min_cluster_size=50, # the metric to use when calculating pairwise distances between data points metric=&#39;euclidean&#39;, # the method used to select clusters from the hierarchy (&#39;eom&#39; stands for Excess of Mass) cluster_selection_method=&#39;eom&#39; ).fit(reduced_embeddings) # fit the HDBSCAN model to the reduced dimensionality embeddings # extract the cluster labels assigned to each data point (-1 indicates noise) clusters = hdbscan_model.labels_ # How many clusters did we generate? (excluding the noise cluster labeled -1) num_clusters = len(set(clusters)) - (1 if -1 in clusters else 0) 5.2.4. Inspecting the Clusters To inspect each cluster manually and explore the assigned documents to get an understanding of its content. import numpy as np # print first three documents in cluster 0 cluster = 0 for index in np.where(clusters == cluster)[0][:3]: print(abstracts[index][:300] + &quot;... \n&quot;) To visualize clustering approximation results without manual review, further reduce document embeddings to two dimensions for plotting on an 2D plane. import pandas as pd from umap import UMAP import matplotlib.pyplot as plt # reduce 384-dimensional embeddings to two dimensions for easier visualization reduced_embeddings = UMAP( n_components=2, min_dist=0.0, metric=&quot;cosine&quot;, random_state=42, ).fit_transform(embeddings) # create dataframe df = pd.DataFrame(reduced_embeddings, columns=[&quot;x&quot;, &quot;y&quot;]) df[&quot;title&quot;] = titles df[&quot;cluster&quot;] = [str(c) for c in clusters] # select outliers (cluster -1) and non-outliers (clusters) to_plot = df.loc[df.cluster != &quot;-1&quot;, :] outliers = df.loc[df.cluster == &quot;-1&quot;, :] # plot outliers and non-outliers separately plt.scatter(outliers.x, outliers.y, alpha=0.05, s=2, c=&quot;grey&quot;, label=&quot;Outliers&quot;) plt.scatter( to_plot.x, to_plot.y, c=to_plot.cluster.astype(int), alpha=0.6, s=2, cmap=&quot;tab20b&quot;, label=&quot;Clusters&quot;, ) plt.axis(&quot;off&quot;) plt.legend() # Add a legend to distinguish outliers and clusters plt.title(&quot;Visualization of Clustered Abstracts&quot;) # Add a title for context plt.show() Figure 53. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization. 5.3. From Text Clustering to Topic Modeling Text clustering is a powerful tool for finding structure among large collections of documents, whereas topic modeling is the process of discovering underlying themes or latent topics within a collection of textual data, which typically involves finding a set of keywords or phrases that best represent and capture the meaning of the topic. Figure 54. Traditionally, topics are represented by a number of keywords but can take other forms. Instead of labeling a topic as “sign language,” these techniques use keywords such as “sign,” “language,” and “translation” to describe the topic. As such, this does not give a single label to a topic and instead requires the user to understand the meaning of the topic through those keywords. 5.3.1. BERTopic: A Modular Topic Modeling Framework BERTopic is a topic modeling technique that leverages clusters of semantically similar texts to extract various types of topic representations. Figure 55. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation. First, similar to text clustering, it embeds documents, reduces their dimensionality, and then clusters these embeddings to group semantically similar texts. .The first part of BERTopic’s pipeline is to create clusters of semantically similar documents. Second, it models word distributions using a bag-of-words approach, counting word frequencies within documents to help extract the most frequent terms. The bag-of-words approach does exactly what its name implies: it counts the number of times each word appears in a document, which can then be used to extract the most frequent words within that document. Figure 56. A bag-of-words counts the number of times each word appears inside a document. Figure 57. Generating c-TF by counting the frequency of words per cluster instead of per document. 6. Prompt Engineering Prompt engineering is the art and science of crafting effective prompts to guide large language models (LLMs) and other generative AI systems to produce desired and high-quality outputs. It involves understanding how these models interpret and respond to different phrasings, instructions, and contexts within a prompt to achieve specific goals, such as generating creative text, answering questions accurately, or performing tasks effectively. 6.1. Using Text Generation Models import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer model_path = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( model_path, device_map=dev, torch_dtype=&#39;auto&#39;, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_path) # create a pipeline pipe = pipeline( &#39;text-generation&#39;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False, ) # prompt messages = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Create a funny joke about chickens.&#39;}] # generate the output output = pipe(messages) print(output[0][&#39;generated_text&#39;]) 6.1.1. Prompt Template Under the hood, transformers.pipeline first converts the messages into a specific prompt template which was used during the training of the model. # apply prompt template prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False) print(prompt) &lt;s&gt;&lt;|user|&gt; Create a funny joke about chickens.&lt;|end|&gt; &lt;|assistant|&gt; Figure 58. The template Phi-3 expects when interacting with the model. 6.1.2. Controlling Model Output Each time an LLM needs to generate a token, it assigns a likelihood number to each possible token to generate different responses for the exact same prompt. Figure 59. The model chooses the next token to generate based on their likelihood scores. The temperature controls the randomness or creativity of the text generated; a higher temperature increases creativity by making less probable tokens more likely, while a temperature of 0 results in deterministic output by always selecting the most probable token. # using a high temperature output = pipe(messages, do_sample=True, temperature=1) print(output[0][&quot;generated_text&quot;]) Figure 60. A higher temperature increases the likelihood that less probable tokens are generated and vice versa. The top-p, or nucleus sampling, is a technique that controls the subset of tokens (the nucleus) an LLM considers for generation by including tokens until their cumulative probability reaches a specified threshold. For instance, if top_p is set to 0.1, the model will consider tokens until their cumulative probability reaches 10%, and if top_p is set to 1, all tokens will be considered. # using a high top_p output = pipe(messages, do_sample=True, top_p=1) print(output[0][&quot;generated_text&quot;]) Figure 61. A higher top_p increases the number of tokens that can be selected to generate and vice versa. The top_k parameter directly limits the number of most probable tokens an LLM considers; setting it to 100 restricts the selection to only the top 100 tokens. Table 1. Use case examples when selecting values for temperature and top_p. Example use case temperature top_p Description Brainstorming session High High High randomness with large pool of potential tokens. The results will be highly diverse, often leading to very creative and unexpected results. Email generation Low Low Deterministic output with high probable predicted tokens. This results in predictable, focused, and conservative outputs. Creative writing High Low High randomness with a small pool of potential tokens. This combination produces creative outputs but still remains coherent. Translation Low High Deterministic output with high probable predicted tokens. Produces coherent output with a wider range of vocabulary, leading to outputs with linguistic variety. 6.2. Prompt Engineering Prompt engineering is the iterative process of designing effective prompts, including questions, statements, or instructions, to elicit useful and relevant outputs from LLMs through experimentation and optimization. A prompt is the input provided to a large language model to elicit a desired response, which generally consists of multiple components such as instructions, data, and output indicators, and can be as complex as needed. Figure 62. A basic example of a prompt. No instruction is given so the LLM will simply try to complete the sentence. Figure 63. Two components of a basic instruction prompt: the instruction itself and the data it refers to. Figure 64. Extending the prompt with an output indicator that allows for a specific output. 6.3. Instruction-Based Prompting Instruction-based prompting is a method of prompting where the primary goal is to have the LLM answer a specific question or resolve a certain task by providing it with specific instructions. Figure 65. Prompt examples of common use cases. Notice how within a use case, the structure and location of the instruction can be changed. Each of these tasks requires different prompting formats and more specifically, asking different questions of the LLM. A non-exhaustive list of the prompting techniques includes: Specificity Accurately describe the desired output, for example, instead of &quot;Write a product description,&quot; ask &quot;Write a product description in under two sentences using a formal tone.&quot; Specificity is arguably the most important aspect; by restricting and specifying what the model should generate, there is a smaller chance of it generating something unrelated to a use case. Hallucination LLMs may generate incorrect information confidently, which is referred to as hallucination. To reduce its impact, ask the LLM to only generate an answer if it knows the answer, and to respond with &quot;I don’t know&quot; if it does not know the answer. Order Either begin or end the prompt with the instruction. Especially with long prompts, information in the middle is often forgotten. LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect). 6.4. Advanced Prompt Engineering While creating a good prompt might initially seem straightforward—just ask a specific question, be accurate, and add examples—prompting can quickly become complex and is often an underestimated aspect of effectively using LLMs. 6.4.1. Prompt Components A prompt generally consists of multiple components, such as instruction, data, and output indicators, and other advanced components that can quickly make a prompt quite complex. Figure 66. An example of a complex prompt with many components. Figure 67. Iterating over modular components is a vital part of prompt engineering. # prompt components persona = &#39;You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.\n&#39; instruction = &#39;Summarize the key findings of the paper provided.\n&#39; context = &#39;Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.\n&#39; data_format = &#39;Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.\n&#39; audience = &#39;The summary is designed for busy researchers that quickly need to grasp the newest trends in Large Language Models.\n&#39; tone = &#39;The tone should be professional and clear.\n&#39; text = &#39;MY TEXT TO SUMMARIZE&#39; data = f&#39;Text to summarize: {text}&#39; # the full prompt - remove and add pieces to view its impact on the generated output query = persona + instruction + context + data_format + audience + tone + data 6.4.2. In-Context Learning: Providing Examples In-context learning (ICL) is a prompting technique that demonstrates the desired task to an LLM through direct examples, rather than solely describing it to provide the model with context to learn from within the prompt. Zero-shot prompting does not leverage examples, one-shot prompts use a single example, and few-shot prompts use two or more examples. Figure 68. An example of a complex prompt with many components. # use a single example of using the made-up word in a sentence one_shot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;A \&#39;Gigamuru\&#39; is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:&#39;, }, { &#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.&#39;, }, { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;To \&#39;screeg\&#39; something is to swing a sword at it. An example of a sentence that uses the word screeg is:&#39;, }, ] print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False)) &lt;|user|&gt;A &#39;Gigamuru&#39; is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:&lt;|end|&gt;&lt;|assistant|&gt;I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.&lt;|end|&gt;&lt;|user|&gt;To &#39;screeg&#39; something is to swing a sword at it. An example of a sentence that uses the word screeg is:&lt;|end|&gt;&lt;|endoftext|&gt; # generate the output outputs = pipe(one_shot_prompt) print(outputs[0][&quot;generated_text&quot;]) In the medieval fantasy novel, the knight would screeg his enemies with his gleaming sword. 6.4.3. Chain Prompting: Breaking up the Problem Prompt chaining is a technique that addresses complex tasks by breaking them down across multiple prompts, where the output of one prompt serves as the input for the subsequent prompt, creating a sequence of interactions that collectively solve the problem. Figure 69. Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch. # create name and slogan for a product product_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a name and slogan for a chatbot that leverages LLMs.&quot;, } ] outputs = pipe(product_prompt) product_description = outputs[0][&quot;generated_text&quot;] print(product_description) # based on a name and slogan for a product, generate a sales pitch sales_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Generate a very short sales pitch for the following product: &#39;{product_description}&#39;&quot;, } ] outputs = pipe(sales_prompt) sales_pitch = outputs[0][&quot;generated_text&quot;] print(sales_pitch) Name: LexiBot Slogan: &quot;Unlock the Power of Language with LexiBot – Your AI Conversation Partner!&quot; Discover the future of communication with LexiBot – your AI conversation partner. Say goodbye to language barriers and hello to seamless, intelligent interactions. LexiBot is here to unlock the power of language, making every conversation more engaging and productive. Embrace the power of AI with LexiBot today! 6.5. Reasoning with Generative Models Reasoning is a core component of human intelligence and is often compared to the emergent behavior of LLMs that often resembles reasoning (through memorization of training data and pattern matching, rather than true reasoning). Human reasoning can be broadly categorized into two systems. System 1 thinking represents an automatic, intuitive, and near-instantaneous process, which shares similarities with generative models that automatically generate tokens without any self-reflective behavior. System 2 thinking, in contrast, is a conscious, slow, and logical process, akin to brainstorming and self-reflection. The system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking, would be emulated by giving a generative model the ability to mimic a form of self-reflection. 6.5.1. Chain-of-Thought: Think Before Answering Chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps (&quot;thoughts&quot;) before giving a final answer. Although chain-of-thought is a great method for enhancing the output of a generative model, it does require one or more examples of reasoning in the prompt, which the user might not have access to. Figure 70. Chain-of-thought prompting uses reasoning examples to persuade the generative model to use reasoning in its answer. # answering with chain-of-thought cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?&quot;, }, { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.&quot;, }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?&quot;, }, ] # generate the output outputs = pipe(cot_prompt) print(outputs[0][&quot;generated_text&quot;]) The cafeteria started with 23 apples. They used 20, so they had 23 - 20 = 3 apples left. Then they bought 6 more, so they now have 3 + 6 = 9 apples. The answer is 9. Instead of providing examples, zero-shot chain-of-thought allows a generative model to provide reasoning without explicit examples by directly prompting it for its thought process. Although the prompt “Let’s think step by step” can improve the output, you are not constrained by this exact formulation. Alterna‐ tives exist like “Take a deep breath and think step-by-step” and “Let’s work through this problem step-by-step.” Figure 71. Chain-of-thought prompting without using examples. Instead, it uses the phrase “Let’s think step-by-step” to prime reasoning in its answer. # zero-shot chain-of-thought prompt zeroshot_cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let&#39;s think step-by-step.&quot;, } ] # generate the output outputs = pipe(zeroshot_cot_prompt) print(outputs[0][&quot;generated_text&quot;]) Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 new apples to the 3 apples they have left: 3 apples + 6 apples = 9 apples. So, the cafeteria now has 9 apples. 6.5.2. Self-Consistency: Sampling Outputs Self-consistency is a technique that reduces randomness in generative models by prompting them multiple times with the same input, using varied sampling parameters like temperature and top_p to enhance diversity, and selecting the majority result as the final answer for robustness. Figure 72. By sampling from multiple reasoning paths, we can use majority voting to extract the most likely answer. # zero-shot chain-of-thought prompt zeroshot_cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let&#39;s think step-by-step.&quot;, } ] # self-consistency settings num_samples = 3 temperature = [0.3, 0.5, 0.7] top_p = [0.8, 0.85, 0.9] # extract final numerical answers def extract_answer(text): numbers = re.findall(r&quot;\d+&quot;, text) # find all numbers in the output return ( numbers[-1] if numbers else None ) # take the last number as the final answer # generate multiple answers answers = [] for i in range(num_samples): outputs = pipe( zeroshot_cot_prompt, do_sample=True, temperature=temperature[i % len(temperature)], top_p=top_p[i % len(top_p)], ) response = outputs[0][&quot;generated_text&quot;].strip() print(f&#39;\n{response}&#39; final_answer = extract_answer(response) if final_answer: answers.append(final_answer) # perform majority voting on numerical answers most_common_answer, count = Counter(answers).most_common(1)[0] print(&quot;\ngenerated answers:&quot;) for i, ans in enumerate(answers, 1): print(f&quot;{i}. {ans}&quot;) print(f&quot;\nfinal answer (majority vote): {most_common_answer}&quot;) Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 apples to the 3 apples they have left gives: 3 apples + 6 apples = 9 apples. So, the cafeteria Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 new apples to the 3 apples they have left, they now have: 3 apples + 6 apples = 9 apples. Sure, let&#39;s break it down step by step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. - 23 apples - 20 apples = 3 apples remaining. 3. They then buy 6 more apples. - 3 apples + 6 apples = 9 apples. So, after these transactions, the cafeteria has 9 apples. generated answers: 1. 9 2. 9 3. 9 final answer (majority vote): 9 6.5.3. Tree-of-Thought: Exploring Intermediate Steps Tree-of-Thought (ToT) is a problem-solving technique structuring reasoning as a decision tree that explores multiple potential solutions at each step, evaluates them, and branches forward with the most promising, similar to brainstorming, to enhance the final outcome. Figure 73. By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned. Tree-of-Thought excels at tasks requiring exploration of multiple paths, such as creative writing, but its reliance on numerous generative model calls can be slow. A more efficient approach involves prompting the model to simulate a multi-expert discussion to reach a consensus, mimicking the ToT framework with a single call. # zero-shot tree-of-thought prompt zeroshot_tot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &quot;Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they&#39;re wrong at any point then they leave. The question is &#39;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?&#39; Make sure to discuss the results.&quot;, } ] # generate the output outputs = pipe(zeroshot_tot_prompt) print(outputs[0][&#39;generated_text&#39;]) **Expert 1:** Step 1: Start with the initial number of apples, which is 23. **Expert 2:** Step 1: Subtract the apples used for lunch, which is 20, from the initial 23 apples. This leaves 3 apples. **Expert 3:** Step 1: Add the 6 apples that were bought to the remaining 3 apples. This results in 9 apples. **Discussion:** All three experts agree on the final result. The cafeteria started with 23 apples, used 20 for lunch, leaving them with 3 apples. Then, they bought 6 more apples, bringing the total to 9 apples. Therefore, the cafeteria now has 9 apples. 6.6. Output Verification Systems and applications built with generative models might eventually end up in production. When that happens, it is important to verify and control the output of the model to prevent breaking the application and to create a robust generative AI application. By default, most generative models create free-form text without adhering to specific structures other than those defined by natural language. Some use cases require their output to be structured in certain formats, like JSON. Even allowing the model to generate structured output, it still has the capability to freely generate its content. For instance, when a model is asked to output either one of two choices, it should not come up with a third. Some open source generative models have no guardrails and will generate outputs that do not consider safety or ethical considerations. For instance, use cases might require the output to be free of profanity, personally identifiable information (PII), bias, cultural stereotypes, etc. Many use cases require the output to adhere to certain standards or performance. The aim is to double-check whether the generated information is factually accurate, coherent, or free from hallucination. Generally, there are three ways of controlling the output of a generative model: Examples: Provide a number of examples of the expected output. Grammar: Control the token selection process. Fine-tuning: Tune a model on data that contains the expected output. 6.6.1. Providing Examples A simple and straightforward method to fix the output is to provide the generative model with examples of what the output should look like. The few-shot learning is a helpful technique that guides the output of the generative model, which can be generalized to guide the structure of the output as well. An important note here is that it is still up to the model whether it will adhere to your suggested format or not. Some models are better than others at following instructions. # zero-shot learning: providing no in-context examples zeroshot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Create a character profile for an RPG game in JSON format.&#39;, } ] # generate the output outputs = pipe(zeroshot_prompt) print(outputs[0][&#39;generated_text&#39;]) # one-shot learning: providing a single in-context example of the desired output structure one_shot_template = &#39;&#39;&#39;Create a short character profile for an RPG game. Make sure to only use this format: { &quot;description&quot;: &quot;A SHORT DESCRIPTION&quot;, &quot;name&quot;: &quot;THE CHARACTER&#39;S NAME&quot;, &quot;armor&quot;: &quot;ONE PIECE OF ARMOR&quot;, &quot;weapon&quot;: &quot;ONE OR MORE WEAPONS&quot; } &#39;&#39;&#39; one_shot_prompt = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: one_shot_template}] # generate the output outputs = pipe(one_shot_prompt) print(outputs[0][&#39;generated_text&#39;]) { &quot;name&quot;: &quot;Eldrin Shadowbane&quot;, &quot;class&quot;: &quot;Rogue&quot;, &quot;level&quot;: 10, &quot;race&quot;: &quot;Elf&quot;, &quot;background&quot;: &quot;Eldrin was born into a noble family in the elven city of Luminara. He was trained in the arts of stealth and combat from a young age. However, Eldrin always felt a deep connection to the shadows and the mysteries of the night. He left his family to become a rogue { &quot;description&quot;: &quot;A skilled archer with a mysterious past, known for their agility and precision.&quot;, &quot;name&quot;: &quot;Lyra Swiftarrow&quot;, &quot;armor&quot;: &quot;Leather bracers and a lightweight leather tunic&quot;, &quot;weapon&quot;: &quot;Longbow, throwing knives&quot; } 6.6.2. Grammar: Constrained Sampling Few-shot learning has a significant disadvantage: explicitly preventing certain output is not possible. Although the model is guided and given instructions, it might still not follow them completely. Grammar-constrained sampling is a technique used during the token generation process of a Large Language Model (LLM) that enforces adherence to predefined grammars or rules when selecting the next token. Instead, packages have been rapidly developed to constrain and validate the output of generative models, like Guidance, Guardrails, and LMQL, which leverage generative models to validate their own output. Figure 74. The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails. Figure 75. Use an LLM to generate only the pieces of information we do not know beforehand. Figure 76. Constrain the token selection to only three possible tokens: “positive,” “neutral,” and “negative.” Like transformers, llama-cpp-python is a library, generally used to efficiently load and use compressed models (quantization) in the GGUF format but can also be used to apply a JSON grammar. from llama_cpp.llama import Llama # load the Phi-3 language model using the llama-cpp-python library llm = Llama.from_pretrained( repo_id=&quot;microsoft/Phi-3-mini-4k-instruct-gguf&quot;, filename=&quot;*fp16.gguf&quot;, n_gpu_layers=-1, n_ctx=2048, verbose=False, ) # generate output using the loaded language model for a chat completion task output = llm.create_chat_completion( messages=[ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a warrior for an RPG in JSON for mat.&quot;, }, ], response_format={&quot;type&quot;: &quot;json_object&quot;}, # specify the response_format as a JSON temperature=0, )[&#39;choices&#39;][0][&#39;message&#39;][&quot;content&quot;] import json # check whether the output actually is JSON json_output = json.dumps(json.loads(output), indent=4) print(json_output) { &quot;warrior&quot;: { &quot;name&quot;: &quot;Aldarion the Brave&quot;, &quot;class&quot;: &quot;Warrior&quot;, &quot;level&quot;: 10, &quot;attributes&quot;: { &quot;strength&quot;: 18, &quot;dexterity&quot;: 10, &quot;constitution&quot;: 16, &quot;intelligence&quot;: 8, &quot;wisdom&quot;: 10, &quot;charisma&quot;: 12 }, 7. Advanced Text Generation Techniques and Tools LangChain is a framework for developing applications powered by large language models (LLMs), which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. Figure 77. LangChain is a complete framework for using LLMs. It has modular compo‐ nents that can be chained together to allow for complex LLM systems. Hugging Face models can be run locally through the HuggingFacePipeline class. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer model_id = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_id) # create a pipeline pipe = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=True, ) from langchain_huggingface.llms import HuggingFacePipeline llm = HuggingFacePipeline(pipeline=pipe) 7.1. Model I/O: Loading Quantized Models with LangChain A GGUF model represents a compressed version of its original counterpart through a method called quantization, which reduces the number of bits needed to represent the parameters of an LLM. Figure 78. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the lowered accuracy when we halve the number of bits. Bits, a series of 0s and 1s, represent values through binary encoding; more bits allow for a wider range of values but demand greater memory for storage. Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information. Quantization comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original. Like rounding the time to the nearest minute (&quot;14:16&quot;) instead of including seconds (&quot;14:16 and 12 seconds&quot;), quantization reduces the precision of a value without losing essential information. As a rule of thumb, look for at least 4-bit quantized models. These models have a good balance between compression and accuracy. Although it is possible to use 3-bit or even 2-bit quantized mod‐ els, the performance degradation becomes noticeable and it would instead be preferable to choose a smaller model with a higher precision. To download a specific bit-variant file (e.g., fp16) of the microsoft/Phi-3-mini-4k-instruct-gguf model, which includes multiple files with different bit-variants (see the &#39;Files and versions&#39; tab). # download from the primary Hugging Face URL: wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf # alternatively, download from the HF mirror: wget https://hf-mirror.com/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf Use Llama.cpp together with LangChain to load the GGUF file, and generate output. # !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf # !pip install llama-cpp-python langchain_communit from langchain_community.llms import LlamaCpp # initialize the LlamaCpp language model integration from Langchain llm = LlamaCpp( # path to the downloaded GGUF model file (ensure this file exists!) model_path=&quot;Phi-3-mini-4k-instruct-fp16.gguf&quot;, n_gpu_layers=-1, max_tokens=500, n_ctx=2048, seed=42, verbose=False, ) # invoke the language model with a prompt. output = llm.invoke(&quot;Hi! My name is Maarten. What is 1 + 1?&quot;) # no/meanless output! Phi-3 requires a specific prompt template. print(output) 7.2. Chains: Extending the Capabilities of LLMs In Langchain, a &quot;chain&quot; is a core concept that goes beyond running LLMs in isolation, which involves connecting an LLM with other components like prompts, tools, or even other chains, to enhance its capabilities and create more complex systems. Figure 79. A single chain connects some modular component, like a prompt template or external memory, to the LLM. 7.2.1. A Single Link in the Chain: Prompt Template Figure 80. By chaining a prompt template with an LLM, we only need to define the input prompts. The template will be constructed for you. By chaining a prompt template with an LLM to get the output, only the user and system prompts need to be defined for each interaction, eliminating the need to repeatedly define the full prompt template. Figure 81. An example of a single chain using Phi-3’s template. The template for Phi-3 is comprised of four main components: &lt;s&gt; to indicate when the prompt starts &lt;|user|&gt; to indicate the start of the user’s prompt &lt;|assistant|&gt; to indicate the start of the model’s output &lt;|end|&gt; to indicate the end of either the prompt or the model’s output Figure 82. The prompt template Phi-3 expects. from langchain_core.prompts import PromptTemplate # create a prompt template with a placeholder for the user&#39;s input template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt; {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate( template=template, input_variables=[&quot;input_prompt&quot;], ) # create a simple chain with the prompt template and the language model basic_chain = prompt | llm # invoke the chain with the input for the prompt template output = basic_chain.invoke( { &quot;input_prompt&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;, } ) # the &#39;output&#39; variable now contains the generated text print(output) Hello Maarten! The answer to 1 + 1 is 2. 7.2.2. A Chain with Multiple Prompts Figure 83. With sequential chains, the output of a prompt is used as the input for the next prompt. A multiple prompt chain, or sequential chain, processes a complex task by dividing it into a series of smaller, sequential subtasks, where each subtask utilizes a distinct prompt and LLM call, with the output from one step feeding directly into the input of the subsequent step. Figure 84. An example to generate a story that has three components: a title, a description of the main character, a summary of the story. The output of the title prompt is used as the input of the character prompt. To generate the story, the output of all previous prompts is used. import json from langchain_core.prompts import PromptTemplate from langchain_core.runnables import RunnablePassthrough, RunnableLambda from langchain.schema import StrOutputParser from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&#39;qwen2.5:0.5b-instruct&#39;, temperature=0.7, max_tokens=100, timeout=30, max_retries=2, base_url=&#39;http://localhost:11434/v1&#39;, # Ollama API api_key=&#39;API-KEY&#39;, verbose=True, ) title_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Create a title for a story about {summary}.&quot; &quot;Only return the title.&quot; &quot;&lt;|end|&gt; &lt;|assistant|&gt;&quot; ) character_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Describe the main character of a story about {summary} with the title {title}. &quot; &quot;Use only two sentences.&quot; &quot;&lt;|end|&gt;&lt;|assistant|&gt;&quot; ) story_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Create a story about {summary} with the title {title}.&quot; &quot;The main character is: {character}. &quot; &quot;Only return the story and it cannot be longer than one paragraph.&quot; &quot;&lt;|end|&gt;&lt;|assistant|&gt;&quot; ) # LCEL-style chain using Runnables title_chain = ( {&quot;summary&quot;: RunnablePassthrough()} | title_prompt | llm | StrOutputParser() ) character_chain = ( {&quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain} | character_prompt | llm | StrOutputParser() ) story_chain = ( { &quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain, &quot;character&quot;: character_chain, } | story_prompt | llm | StrOutputParser() ) aggregate_chain = RunnableLambda( lambda inputs: { &quot;summary&quot;: inputs[&quot;summary&quot;], &quot;title&quot;: inputs[&quot;title&quot;], &quot;character&quot;: inputs[&quot;character&quot;], &quot;story&quot;: inputs[&quot;story&quot;], } ) final_chain = { &quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain, &quot;character&quot;: character_chain, &quot;story&quot;: story_chain, } | aggregate_chain output = final_chain.invoke({&quot;summary&quot;: &quot;a girl that lost her mother&quot;}) print(json.dumps(output, indent=2)) { &quot;summary&quot;: { &quot;summary&quot;: &quot;a girl that lost her mother&quot; }, &quot;title&quot;: &quot;\&quot;Lost Mother Girl\&quot;&quot;, &quot;character&quot;: &quot;In the story, the main character named Lily, who was born to an ordinary family, unexpectedly finds herself the daughter of a rich individual after losing her mother. She navigates this new reality with courage and strength, learning valuable lessons about empathy, perseverance, and the power of resilience.&quot;, &quot;story&quot;: &quot;In the quiet village where Linxue lived, her mother had been gone for many years. As an only child, she often felt distant from the other children in the village. One day, 7.3. Memory: Helping LLMs to Remember Conversations Memory can be added to the LLM chain using methods like conversation buffers and conversation summaries to make chat models stateful to remember previous conversations. 7.3.1. Conversation Buffer In Langchain, ConversationBufferMemory provides an intuitive way to give LLMs memory by updating the prompt to include the full chat history. Figure 85. We can remind an LLM of what previously happened by simply appending the entire conversation history to the input prompt. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke({&quot;input&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;}) {&#39;input&#39;: &#39;Hi! My name is Maarten. What is 1 + 1?&#39;, &#39;chat_history&#39;: &#39;&#39;, &#39;text&#39;: &#39;Nice to meet you, Maarten!\n\nThe answer to 1 + 1 is... 2!&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) {&#39;input&#39;: &#39;What is my name?&#39;, &#39;chat_history&#39;: &#39;Human: Hi! My name is Maarten. What is 1 + 1?\nAI: Nice to meet you, Maarten!\n\nThe answer to 1 + 1 is... 2!&#39;, &#39;text&#39;: &#39;Nice to meet you too, Maarten! Your name is indeed Maarten. Would you like to ask another question or have a conversation?&#39;} 7.3.2. Windowed Conversation Buffer In LangChain, ConversationBufferWindowMemory decides how many the last k conversations are passed to the input prompt. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=2, memory_key=&quot;chat_history&quot;) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke( input=&quot;Hi! My name is Maarten and I am 33 years old. What is 1 + 1?&quot; ) llm_chain.invoke(input=&quot;What is 3 + 3?&quot;) llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) llm_chain.invoke({&quot;input&quot;: &quot;What is my age?&quot;}) 7.3.3. Conversation Summary In LangChain, ConversationSummaryMemory summarizes the entire conversation history (typically using an external LLM) before providing it to the input prompt. Figure 86. Instead of passing the conversation history directly to the prompt, we use another LLM to summarize it first. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationSummaryMemory # prepare a summarization template as the summarization prompt summary_prompt_template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Summarize the conversations and update with the new lines. Current summary: {summary} new lines of conversation: {new_lines} New summary:&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; summary_prompt = PromptTemplate.from_template(template=summary_prompt_template) memory = ConversationSummaryMemory( llm=llm, memory_key=&quot;chat_history&quot;, prompt=summary_prompt ) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke({&quot;input&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;}) {&#39;input&#39;: &#39;Hi! My name is Maarten. What is 1 + 1?&#39;, &#39;chat_history&#39;: &#39;&#39;, &#39;text&#39;: &#39;Hi Maarten!\n\nThe answer to 1 + 1 is 2.&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) {&#39;input&#39;: &#39;What is my name?&#39;, &#39;chat_history&#39;: &quot;Here is the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n\nNew lines of conversation:\nHuman: That&#39;s correct, what&#39;s 2 * 2?\nAI: Let me calculate... The answer to 2 * 2 is 4.&quot;, &#39;text&#39;: &#39;Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What was the first question I asked?&quot;}) {&#39;input&#39;: &#39;What was the first question I asked?&#39;, &#39;chat_history&#39;: &#39;Here\&#39;s the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n* Human: That\&#39;s correct, what\&#39;s 2 * 2?\n* AI: Let me calculate... The answer to 2 * 2 is 4.\n* Human: What is my name?\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?&#39;, &#39;text&#39;: &#39;The first question you asked was: &quot;what\&#39;s 1 + 1?&quot;&#39;} # check what the summary is thus far memory.load_memory_variables({}) {&#39;chat_history&#39;: &#39;Here is the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n* Human: That\&#39;s correct, what\&#39;s 2 * 2?\n* AI: Let me calculate... The answer to 2 * 2 is 4.\n* Human: What is my name?\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?\n* Human: What was the first question I asked?\n* AI: The first question you asked was: &quot;what\&#39;s 1 + 1?&quot;&#39;} 7.4. Agents: Creating a System of LLMs Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions. ReAct (Reasoning and Acting) is a cognitive framework for language models that interleaves reasoning (&quot;Thoughts&quot;) and acting (&quot;Actions&quot;) with observations, allowing the model to dynamically plan, execute, and learn from its interactions with external tools or environments to solve complex tasks. Figure 87. An example of a ReAct prompt template. Figure 88. An example of two cycles in a ReAct pipeline. from langchain_openai import ChatOpenAI # an LLM that is powerful enough to properly follow complex instructions llm = ChatOpenAI( model=&quot;mistral:7b-instruct&quot;, # &quot;llama3.1:8b&quot;, # &quot;llama3.2:1b&quot;, temperature=0.7, max_tokens=100, base_url=&quot;http://localhost:11434/v1&quot;, api_key=&quot;API-KEY&quot;, verbose=True, ) from langchain_core.prompts import PromptTemplate # create the ReAct template react_template = &quot;&quot;&quot;Answer the following questions as best you can. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Agents: Creating a System of LLMs Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: {input} Thought:{agent_scratchpad}&quot;&quot;&quot; prompt = PromptTemplate( template=react_template, input_variables=[&quot;tools&quot;, &quot;tool_names&quot;, &quot;input&quot;, &quot;agent_scratchpad&quot;], ) from langchain.agents import load_tools, Tool from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults search = DuckDuckGoSearchResults() search_tool = Tool( name=&quot;duckduck&quot;, description=&quot;A web search engine. Use this to as a search engine for general queries.&quot;, func=search.run, ) tools = load_tools([&quot;llm-math&quot;], llm=llm) tools.append(search_tool) from langchain.agents import AgentExecutor, create_react_agent agent = create_react_agent(llm, tools, prompt) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=5, ) agent_executor.invoke( { &quot;input&quot;: &quot;What is 123 + 456?&quot; } ) &gt; Entering new AgentExecutor chain... To solve this, I will use the Calculator tool. The input for the calculator will be the equation &quot;123 + 456&quot;. Action: Calculator Action Input: &quot;123 + 456&quot;Answer: 579 I now know the final answer. Final Answer: The result of the calculation (123 + 456) is 579. &gt; Finished chain. {&#39;input&#39;: &#39;What is 123 + 456?&#39;, &#39;output&#39;: &#39;The result of the calculation (123 + 456) is 579.&#39;} agent_executor.invoke( { &quot;input&quot;: &quot;What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.&quot; } ) &gt; Entering new AgentExecutor chain... I need to find the current price of a MacBook Pro and then convert that price from USD to EUR using the given exchange rate. Agents: Calculator, duckduck Action: duckduck Action Input: What is the current price of a MacBook Pro in USD?snippet: Apple resellers are hosting a variety of MacBook Pro sales that discount current M4, M4 Pro and M4 Max 14-inch and 16-inch models, in addition to blowout bargains on M3 models. Apple offers two ..., title: Best MacBook Pro Deals for March 2025 | Save up to $1,200 - AppleInsider, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: The newly launched M4 Pro and M4 Max 14-inch MacBook Pros have shown notable performance improvements over their M1, M2, and M3 counterparts, especially in single-core scores. In recent benchmarks, the M4 Pro 14-inch MacBook Pro achieved a single-core score of approximately 3,850, surpassing the M3 Pro&#39;s single-core score by about 15-20%., title: Apple 14″ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/14-macbook-pro/, snippet: Apple MacBook Pro 14&quot; (M4/512GB): was $1,599 now $1,399 at Amazon. The M4-based MacBook Pro M4 is pretty close to being the perfect laptop. You get fantastic performance from the M4 chip, useful ..., title: Epic Apple MacBook sale is live — shop the best deals from $629 right ..., link: https://www.tomsguide.com/sales-events/epic-apple-macbook-sale-is-live-shop-the-best-deals-from-usd629-right-now, snippet: The M4 Max MacBook Pro is Apple&#39;s most powerful option, and both the silver and space black options are on sale. ... List price Best price (current) Best price (all-time) M2 MacBook Air (13-inch ..., title: Best MacBook Deals: Save on Apple&#39;s Latest Laptops and Previous-Gen ..., link: https://www.cnet.com/deals/best-macbook-deals/ The current price of a MacBook Pro in USD can be found from the search results. Let me filter the results a bit more specifically to find the price. Agents: duckduck Action: duckduck Action Input: What is the price of a new 14-inch MacBook Pro (M4/512GB) in USD?snippet: - 14″ M4 MacBook Pro (16GB/1TB/Gray): $1599, $200 off MSRP - 14″ M4 MacBook Pro (24GB/1TB/Gray): $1799, $200 off MSRP. These are currently the lowest prices available for new M4-powered 14″ MacBook Pros among the Apple retailers we track. For the latest sales and prices, keep an eye on our 14-inch MacBook Pro Price Tracker, updated daily., title: 14-inch M4 MacBook Pros on sale today for $150-$200 off MSRP, link: https://www.macprices.net/2025/01/14/14-inch-m4-macbook-pros-on-sale-today-for-150-200-off-msrp/, snippet: Every M4 Pro and M4 Max model is also on sale at up to $300 off in our Mac Price Guide. Prices start at $1,699. Here are a few top picks from the MacBook Pro sale: 14-inch M4, 16GB, 512GB, Space ..., title: Apple M4 MacBook Pro Drops to $1,399, Free Next Day Shipping - AppleInsider, link: https://appleinsider.com/articles/24/12/25/snag-an-m4-macbook-pro-14-inch-for-1399-with-free-next-day-delivery, snippet: The M4 Pro MacBook Pro 14-inch has hit a new record low price of $1,699, with units in stock with free store pickup as early as today. But don&#39;t delay, as the deal ends on Christmas Eve., title: Apple MacBook Pro 14-inch M4 Pro Drops to Best $1,699 Price - AppleInsider, link: https://appleinsider.com/articles/24/12/24/apples-14-inch-macbook-pro-with-m4-pro-chip-plunges-to-record-low-1699-today-only, snippet: Right now the 14-inch MacBook Pro is available with a discount that slashes its price to the lowest yet, and you won&#39;t want to miss out. Amazon is now selling the M4 MacBook Pro for just $1,398 ..., title: Apple&#39;s Latest M4 14-inch MacBook Pro Is Now Yours for Its Best-Ever Price, link: https://www.cnet.com/deals/apples-latest-m4-14-inch-macbook-pro-is-now-yours-for-its-best-ever-price/ The current price of a new 14-inch MacBook Pro (M4/512GB) in USD is $1399. To find the cost in EUR, we can use the given exchange rate of 0.85 EUR for 1 USD. So, the cost of the MacBook Pro in EUR would be 1399 * 0.85 = €1176.21. Final Answer: The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR. &gt; Finished chain. {&#39;input&#39;: &#39;What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.&#39;, &#39;output&#39;: &#39;The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR.&#39;} Appendix A: LangChain LangChain is a framework that consists of a number of packages, which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. langchain-core is a lightweight package containing base abstractions and interfaces for core Langchain components like chat models, vector stores, and tools, without including any third-party integrations and with minimal dependencies. langchain is the main package containing generic chains and retrieval strategies that form an application&#8217;s cognitive architecture, independent of specific third-party integrations. Integrations are a list of lightweight packages (e.g., langchain-openai, langchain-anthropic) that contain specific integrations and are co-maintained for proper versioning. langchain-community is a package containing third-party integrations for various components (chat models, vector stores, tools, etc.), maintained by the Langchain community, with all dependencies being optional to ensure a lightweight package. langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. langserve is a package to deploy LangChain chains as REST APIs that makes it easy to get a production ready API up and running. LangSmith is a developer platform for debugging, testing, evaluating, and monitoring LLM applications. 7.A.1. Chat Models and Messages Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario. LangChain provides a consistent interface for working with chat models from different providers that takes a list of messages as input and returns a message as output while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs. LangChain supports two message formats to interact with chat models: LangChain Message Format: LangChain&#8217;s own message format, which is used by default and is used internally by LangChain. OpenAI&#8217;s Message Format: OpenAI&#8217;s message format. Messages are the unit of communication in chat models, which are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation. Each message has a role (e.g., &quot;user&quot;, &quot;assistant&quot;) and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider. LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider. LangChain messages are Python objects that subclass from a BaseMessage. SystemMessage: corresponds to system role HumanMessage: corresponds to user role AIMessage: corresponds to assistant role AIMessageChunk: corresponds to assistant role, used for streaming responses ToolMessage: corresponds to tool role When invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. model.invoke(&quot;Hello, how are you?&quot;) from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&quot;gpt-4o&quot;, temperature=0, max_tokens=100, timeout=30, max_retries=2, ) llm.invoke(&#39;What is LangChain?&#39;) 7.A.2. Prompt Templates Prompt Templates are responsible for formatting user input into a format that can be passed to a language model, take as input a dictionary, where each key represents a variable in the prompt template to fill in, and output a PromptValue. from langchain_core.prompts import PromptTemplate prompt_template = PromptTemplate.from_template(&quot;Tell me a joke about {topic}&quot;) prompt = prompt_template.format(**{&quot;topic&quot;: &quot;cats&quot;}) print(prompt) # Tell me a joke about cats from langchain_core.prompts import ChatPromptTemplate prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), (&quot;user&quot;, &quot;Tell me a joke about {topic}&quot;) ]) prompt = prompt_template.format(**{&quot;topic&quot;: &quot;cats&quot;}) print(prompt) # System: You are a helpful assistant # Human: Tell me a joke about cats from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.messages import HumanMessage prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), MessagesPlaceholder(&quot;msgs&quot;) ]) prompt = prompt_template.format(**{&quot;msgs&quot;: [HumanMessage(content=&quot;hi!&quot;)]}) print(prompt) # System: You are a helpful assistant # Human: hi! # alternatively prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), (&quot;placeholder&quot;, &quot;{msgs}&quot;) # &lt;-- This is the changed part ]) prompt = prompt_template.format(**{&quot;msgs&quot;: [HumanMessage(content=&quot;hi!&quot;)]}) print(prompt) # System: You are a helpful assistant # Human: hi! 7.A.3. Structured Outputs Structured outputs are a concept where language models are instructed to respond in a structured format, rather than in direct natural language, which is useful in scenarios where the output needs to be machine-readable, such as storing output in a database and ensure that the output conforms to the database schema. LangChain provides a method, with_structured_output(), that automates the process of binding the schema to the model and parsing the output. from pydantic import BaseModel, Field class ResponseFormatter(BaseModel): &quot;&quot;&quot;Always use this tool to structure your response to the user.&quot;&quot;&quot; answer: str = Field(description=&quot;The answer to the user&#39;s question&quot;) followup_question: str = Field(description=&quot;A followup question the user could ask&quot;) llm_with_structure = llm.with_structured_output(ResponseFormatter) structured_output = llm_with_structure.invoke( &quot;What is the powerhouse of the cell?&quot;, verbose=True ) structured_output ResponseFormatter(answer=&#39;The powerhouse of the cell is the mitochondria.&#39;, followup_question=&#39;What is the organelle that powers the cell?&#39;) While one approach is to include defined schema in the prompt and ask nicely for the model to use it, it is not recommended. from langchain.output_parsers.structured import ResponseSchema, StructuredOutputParser response_schemas = [ ResponseSchema( name=&quot;answer&quot;, description=&quot;The answer to the user&#39;s question&quot;, type=&quot;string&quot;, ), ResponseSchema( name=&quot;followup_question&quot;, description=&quot;A followup question the user could ask&quot;, type=&quot;string&quot;, ), ] parser = StructuredOutputParser.from_response_schemas(response_schemas) format_instructions = parser.get_format_instructions() from langchain.prompts import PromptTemplate prompt = PromptTemplate( template=&quot;{query}\n{format_instructions}\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: format_instructions}, ) print(prompt.format(**{&quot;query&quot;: &quot;What is the powerhouse of the cell?&quot;})) What is the powerhouse of the cell? The output should be a markdown code snippet formatted in the following schema, including the leading and trailing &quot;```json&quot; and &quot;```&quot;: ```json { &quot;answer&quot;: string // The answer to the user&#39;s question &quot;followup_question&quot;: string // A followup question the user could ask } ``` chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;What is the powerhouse of the cell?&quot;}) output {&#39;answer&#39;: &#39;The powerhouse of the cell is the nucleus.&#39;, &#39;followup_question&#39;: &#39;What does the nucleus play a crucial role in?&#39;} 7.A.4. Output Parsers Output Parsers are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks, which are useful when using LLMs to generate structured data, or to normalize output from chat models and LLMs. # parse text from message objects from langchain_core.output_parsers import StrOutputParser chain = llm | StrOutputParser() output = chain.invoke(&#39;What is 2 + 2 ?&#39;) print(output) # 2 + 2 equals 4. # use output parsers to parse an LLM response into structured format from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import PromptTemplate from pydantic import BaseModel, Field, model_validator class Joke(BaseModel): setup: str = Field(description=&quot;question to set up a joke&quot;) punchline: str = Field(description=&quot;answer to resolve the joke&quot;) parser = PydanticOutputParser(pydantic_object=Joke) prompt = PromptTemplate( template=&quot;Answer the user query.\n{format_instructions}\n{query}\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}, ) chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;Tell me a joke.&quot;}) print(output.model_dump_json(indent=2)) # { # &quot;setup&quot;: &quot;Why did the tomato turn red?&quot;, # &quot;punchline&quot;: &quot;Because it saw the salad dressing!&quot; # } # parse JSON output from langchain_core.output_parsers import JsonOutputParser from langchain_core.prompts import PromptTemplate from pydantic import BaseModel, Field class Joke(BaseModel): setup: str = Field(description=&quot;question to set up a joke&quot;) punchline: str = Field(description=&quot;answer to resolve the joke&quot;) parser = JsonOutputParser(pydantic_object=Joke) instructions = parser.get_format_instructions() print(f&#39;\n{instructions}\n---------------&#39;) prompt = PromptTemplate( template=&quot;Answer the user query.\n{format_instructions}\n{query}\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}, ) chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;Tell me a joke.&quot;}) print(output) # The output should be formatted as a JSON instance that conforms to the JSON schema below. # # As an example, for the schema {&quot;properties&quot;: {&quot;foo&quot;: {&quot;title&quot;: &quot;Foo&quot;, &quot;description&quot;: &quot;a list of strings&quot;, &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}}}, &quot;required&quot;: [&quot;foo&quot;]} # the object {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]} is a well-formatted instance of the schema. The object {&quot;properties&quot;: {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]}} is not well-formatted. # # Here is the output schema: # ``` # {&quot;properties&quot;: {&quot;setup&quot;: {&quot;description&quot;: &quot;question to set up a joke&quot;, &quot;title&quot;: &quot;Setup&quot;, &quot;type&quot;: &quot;string&quot;}, &quot;punchline&quot;: {&quot;description&quot;: &quot;answer to resolve the joke&quot;, &quot;title&quot;: &quot;Punchline&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;setup&quot;, &quot;punchline&quot;]} # ``` # --------------- # {&#39;setup&#39;: &#39;Why did the tomato turn red?&#39;, &#39;punchline&#39;: &#39;Because it saw the salad dressing!&#39;} 7.A.5. Embedding, Vector Stores, and Retrievers Embedding models are machine learning models that transform human language or multimodal data (text, audio, images, video - not currently fully supported by Langchain) into numerical vector representations (embeddings), which are fixed-length arrays capturing the semantic meaning of the input, enabling machines to understand and compare data based on conceptual similarity, not just keywords. (1) Embed text as a vector: Embeddings transform text into a numerical vector representation. (2) Measure similarity: Embedding vectors can be compared using simple mathematical operations. LangChain provides a universal interface for working with embedding models, providing standard methods for common operations, and simplifies interaction with various embedding providers through two central methods: embed_documents: For embedding multiple texts (documents) embed_query: For embedding a single text (query) # for embedding multiple texts (documents) from langchain_openai import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings() embeddings = embeddings_model.embed_documents( [ &quot;Hi there!&quot;, &quot;Oh, hello!&quot;, &quot;What&#39;s your name?&quot;, &quot;My friends call me World&quot;, &quot;Hello World!&quot; ] ) len(embeddings), len(embeddings[0]) (5, 1536) # for embedding a single text (query) query_embedding = embeddings_model.embed_query(&quot;What is the meaning of life?&quot;) # measure similarity import numpy as np def cosine_similarity(vec1, vec2): dot_product = np.dot(vec1, vec2) norm_vec1 = np.linalg.norm(vec1) norm_vec2 = np.linalg.norm(vec2) return dot_product / (norm_vec1 * norm_vec2) similarity = cosine_similarity(query_result, document_result) print(&quot;Cosine Similarity:&quot;, similarity) # hugging face embeddings from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) query_embedding = embeddings.embed_query(&quot;Hello, world!&quot;) print(len(query_embedding)) # 384 Vector stores are databases that can efficiently store and retrieve embeddings, which are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches. LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations. The key methods are: add_documents: Add a list of texts to the vector store. delete: Delete a list of documents from the vector store. similarity_search: Search for similar documents to a given query. from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) from langchain_core.vectorstores import InMemoryVectorStore # initialize with an embedding model vector_store = InMemoryVectorStore(embedding=embeddings) # add documents from langchain_core.documents import Document document_1 = Document( page_content=&quot;I had chocalate chip pancakes and scrambled eggs for breakfast this morning.&quot;, metadata={&quot;source&quot;: &quot;tweet&quot;}, ) document_2 = Document( page_content=&quot;The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.&quot;, metadata={&quot;source&quot;: &quot;news&quot;}, ) documents = [document_1, document_2] vector_store.add_documents(documents=documents) # [&#39;df0f6926-c824-4114-a2c5-2b19d9d8740c&#39;, &#39;fa105761-9dd6-4c1c-860a-28e3e4ba181a&#39;] # provide IDs for the documents to the vector store vector_store.add_documents(documents=documents, ids=[&quot;doc1&quot;, &quot;doc2&quot;]) # [&#39;doc1&#39;, &#39;doc2&#39;] # delete documents vector_store.delete(ids=[&quot;doc1&quot;]) # similarity search query = &quot;my query&quot; docs = vectorstore.similarity_search(query) print(docs[0].page_content) Retrievers in Langchain are components that provide a unified way to interact with various retrieval systems, including vector stores, graph databases, and relational databases, and take a natural language query as input to return a list of relevant documents. LangChain provides a uniform interface for interacting with different types of retrieval systems that accepts a query and return documents. A Langchain retriever is a runnable, which is a standard interface for Langchain components, and it has a few common methods, including invoke, that are used to interact with it. docs = retriever.invoke(query) Lost in the Middle is the phenomenon where Large Language Models (LLMs) have difficulty effectively using information located in the middle of a long input context, often performing better when relevant details are at the beginning or end. Documents retrieved from vector stores are typically returned in descending order of relevance, often measured by cosine similarity of embeddings. To mitigate the &quot;lost in the middle&quot; effect, re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. The LongContextReorder document transformer implements the re-ordering procedure. from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) from langchain_core.vectorstores import InMemoryVectorStore texts = [ &quot;Basquetball is a great sport.&quot;, &quot;Fly me to the moon is one of my favourite songs.&quot;, &quot;The Celtics are my favourite team.&quot;, &quot;This is a document about the Boston Celtics&quot;, &quot;I simply love going to the movies&quot;, &quot;The Boston Celtics won the game by 20 points&quot;, &quot;This is just a random text.&quot;, &quot;Elden Ring is one of the best games in the last 15 years.&quot;, &quot;L. Kornet is one of the best Celtics players.&quot;, &quot;Larry Bird was an iconic NBA player.&quot;, ] vector_store = InMemoryVectorStore.from_texts(texts, embedding=embeddings) from langchain_core.runnables import chain from langchain_core.documents import Document # create a retriever @chain def retriever(query: str) -&gt; list[Document]: docs, scores = zip(*vector_store.similarity_search_with_score(query, k=10)) for doc, score in zip(docs, scores): doc.metadata[&quot;score&quot;] = score return docs docs = retriever.invoke(query) max_score_length = max(len(f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;) for doc in docs) for doc in docs: score_str = f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;.rjust(max_score_length) print(f&quot;- {score_str}: {doc.page_content}&quot;) - 0.675469: This is a document about the Boston Celtics - 0.638917: The Celtics are my favourite team. - 0.552694: L. Kornet is one of the best Celtics players. - 0.460651: The Boston Celtics won the game by 20 points - 0.320224: Larry Bird was an iconic NBA player. - 0.244521: Elden Ring is one of the best games in the last 15 years. - 0.231564: Basquetball is a great sport. - 0.106447: I simply love going to the movies - 0.059917: Fly me to the moon is one of my favourite songs. - 0.034081: This is just a random text. from langchain_community.document_transformers import LongContextReorder # Reorder the documents: # Less relevant document will be at the middle of the list and more # relevant elements at beginning / end. reordering = LongContextReorder() reordered_docs = reordering.transform_documents(docs) # Confirm that the 4 relevant documents are at beginning and end. for doc in reordered_docs: score_str = f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;.rjust(max_score_length) print(f&quot;- {score_str}: {doc.page_content}&quot;) - 0.638917: The Celtics are my favourite team. - 0.460651: The Boston Celtics won the game by 20 points - 0.244521: Elden Ring is one of the best games in the last 15 years. - 0.106447: I simply love going to the movies - 0.034081: This is just a random text. - 0.059917: Fly me to the moon is one of my favourite songs. - 0.231564: Basquetball is a great sport. - 0.320224: Larry Bird was an iconic NBA player. - 0.552694: L. Kornet is one of the best Celtics players. - 0.675469: This is a document about the Boston Celtics 7.A.6. Document Loaders Document Loaders are responsible for loading documents from a variety of sources. # simple and fast text extraction from langchain_community.document_loaders import PyPDFLoader file_path = &quot;./books/llm-book.pdf&quot; loader = PyPDFLoader(file_path) pages = [] for page in loader.lazy_load(): pages.append(page) print(f&quot;{pages[0].metadata}\n&quot;) print(pages[0].page_content) {&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} Hands-On Large Language Models Language Understanding and Generation Jay Alammar &amp; Maarten Grootendorst # vector search over PDFs from langchain_core.vectorstores import InMemoryVectorStore from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) vector_store = InMemoryVectorStore.from_documents(pages, embeddings) docs = vector_store.similarity_search(&quot;What is Prompt Engineering?&quot;, k=2) for doc in docs: print(f&#39;Page {doc.metadata[&quot;page&quot;]}: {doc.page_content[:300]}\n&#39;) Page 194: Intro to Prompt Engineering An essential part of working with text-generative LLMs is prompt engineering. By carefully designing our prompts we can guide the LLM to generate desired responses. Whether the prompts are questions, statements, or instructions, the main goal of prompt engineering is to e Page 219: Summary In this chapter, we explored the basics of using generative models through prompt engineering and output verification. We focused on the creativity and potential com‐ plexity that comes with prompt engineering. These components of a prompt are key in generating and optimizing output appropri 7.A.7. Text Splitters Text splitters split documents into smaller, manageable chunks for use in downstream applications, particularly retrieval systems, to handle non-uniform document lengths, overcome model limitations, improve representation quality, enhance retrieval precision, and optimize computational resources. Text splitting approaches include length-based methods (token or character), text-structure based methods (like recursive splitting that respects paragraphs and sentences), document-structure based methods (leveraging formats like Markdown or HTML), and semantic meaning based methods (analyzing content for significant meaning shifts). from langchain_text_splitters import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=100, chunk_overlap=20, length_function=len, is_separator_regex=False, ) with open(&quot;state_of_the_union.txt&quot;) as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) print(texts[0]) print(texts[1]) Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. from langchain_community.document_loaders.text import TextLoader loader = TextLoader(&quot;state_of_the_union.txt&quot;) documents = loader.load() split_documents = text_splitter.split_documents(documents) print(split_documents[0]) print(split_documents[1]) page_content=&#39;Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and&#39; metadata={&#39;source&#39;: &#39;state_of_the_union.txt&#39;} page_content=&#39;of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.&#39; metadata={&#39;source&#39;: &#39;state_of_the_union.txt&#39;} from langchain_community.document_loaders import PyPDFLoader loader = PyPDFLoader(&quot;./books/llm-book.pdf&quot;) documents = loader.load() split_documents = text_splitter.split_documents(documents) print(split_documents[0]) print(split_documents[1]) page_content=&#39;Hands-On Large Language Models Language Understanding and Generation Jay Alammar &amp;&#39; metadata={&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} page_content=&#39;Jay Alammar &amp; Maarten Grootendorst&#39; metadata={&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} 7.A.8. Tools LangChain&#8217;s tool abstraction links a Python function to a schema defining its name, description, and expected arguments, which chat models that support tool calling (or function calling) can use to request the execution of a specific function with specific inputs A key principle of tool calling is that the model decides when to use a tool based on the input&#8217;s relevance. # tool creation @tool def multiply(a: int, b: int) -&gt; int: &quot;&quot;&quot;Multiply a and b.&quot;&quot;&quot; return a * b tools = [multiply] # tool binding llm_with_tools = llm.bind_tools(tools) # tool calling output = llm_with_tools.invoke(&quot;What is 2 multiplied by 3?&quot;) output.content, output.tool_calls (&#39;&#39;, [{&#39;name&#39;: &#39;multiply&#39;, &#39;args&#39;: {&#39;a&#39;: 2, &#39;b&#39;: 3}, &#39;id&#39;: &#39;call_zerallda&#39;, &#39;type&#39;: &#39;tool_call&#39;}]) # model doesn&#39;t always need to call a tool output = llm_with_tools.invoke(&quot;Hello world!&quot;) output.content, output.tool_calls (&#39;Hello! How can I assist you today?&#39;, []) 7.A.9. Chat History Chat history is sequence of messages, each of which is associated with a specific role, such as user, assistant, system, or tool, a record of the conversation between the user and the chat model, which is used to maintain context and state throughout the conversation. A full conversation often starts with a system message that sets the context for the conversation, and follows a combination of two alternating message patterns: user and assistant, representing a back-and-forth conversation, or assistant and tool, representing an &quot;agentic&quot; workflow where the assistant invokes tools for specific tasks. All models have finite context windows, and trim_messages can be used to reduce the size of a chat history to a specified token count or specified message count. from langchain_core.messages import ( AIMessage, HumanMessage, SystemMessage, trim_messages, ) messages = [ SystemMessage(&quot;you&#39;re a good assistant, you always respond with a joke.&quot;), HumanMessage(&quot;i wonder why it&#39;s called langchain&quot;), AIMessage( &#39;Well, I guess they thought &quot;WordRope&quot; and &quot;SentenceString&quot; just didn\&#39;t have the same ring to it!&#39; ), HumanMessage(&quot;and who is harrison chasing anyways&quot;), AIMessage( &quot;Hmmm let me think.\n\nWhy, he&#39;s probably chasing after the last cup of coffee in the office!&quot; ), HumanMessage(&quot;what do you call a speechless parrot&quot;), ] # trimming based on token count from langchain_core.messages.utils import count_tokens_approximately trim_messages( messages, strategy=&quot;last&quot;, token_counter=count_tokens_approximately, max_tokens=45, start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, allow_partial=False, ) SystemMessage(content=&quot;you&#39;re a good assistant, you always respond with a joke.&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;what do you call a speechless parrot&#39;, additional_kwargs={}, response_metadata={})] # trimming based on message count trim_messages( messages, strategy=&quot;last&quot;, token_counter=len, max_tokens=5, # message count start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, ) [SystemMessage(content=&quot;you&#39;re a good assistant, you always respond with a joke.&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;and who is harrison chasing anyways&#39;, additional_kwargs={}, response_metadata={}), AIMessage(content=&quot;Hmmm let me think.\n\nWhy, he&#39;s probably chasing after the last cup of coffee in the office!&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;what do you call a speechless parrot&#39;, additional_kwargs={}, response_metadata={})] # using a chat model as a token counter from langchain_openai import ChatOpenAI trim_messages( messages, max_tokens=45, strategy=&quot;first&quot;, token_counter=ChatOpenAI(model=&quot;gpt-4o&quot;), ) # chaining from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=&quot;gpt-4o&quot;) trimmer = trim_messages( token_counter=llm, strategy=&quot;last&quot;, max_tokens=45, start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, ) chain = trimmer | llm chain.invoke(messages) from langchain_core.chat_history import InMemoryChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory chat_history = InMemoryChatMessageHistory(messages=messages[:-1]) def dummy_get_session_history(session_id): if session_id != &quot;1&quot;: return InMemoryChatMessageHistory() return chat_history trimmer = trim_messages( max_tokens=45, strategy=&quot;last&quot;, token_counter=llm, include_system=True, start_on=&quot;human&quot;, ) chain = trimmer | llm chain_with_history = RunnableWithMessageHistory( chain, dummy_get_session_history ) chain_with_history.invoke( [HumanMessage(&quot;what do you call a speechless parrot&quot;)], config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;1&quot;}}, ) 7.A.10. Memory Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. Short-term memory, or thread-scoped memory, can be recalled at any time from within a single conversational thread with a user. Long-term memory is shared across conversational threads, and can be recalled at any time and in any thread. 7.A.11. LangChain Expression Language (LCEL) The LangChain Expression Language (LCEL) uses a declarative approach, similar to a Unix pipe, to build new Runnable components from existing ones, where a Runnable created with LCEL is often referred to as a &quot;chain&quot; and fully implements the Runnable interface. from langchain_core.vectorstores import InMemoryVectorStore from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) vectorstore = InMemoryVectorStore.from_texts( [&quot;harrison worked at kensho&quot;], embedding=embeddings, ) retriever = vectorstore.as_retriever() from langchain_core.prompts import ChatPromptTemplate template = &quot;&quot;&quot;Answer the question based only on the following context: {context} Question: {question} &quot;&quot;&quot; prompt = ChatPromptTemplate.from_template(template) from langchain_core.runnables import RunnablePassthrough prompt_chain = { &quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough(), } | prompt prompt_text = prompt_chain.invoke(&quot;where did harrison work?&quot;).to_string() print(prompt_text) Human: Answer the question based only on the following context: [Document(id=&#39;d03a67c7-a031-43aa-a27c-6411f9dd0dba&#39;, metadata={}, page_content=&#39;harrison worked at kensho&#39;)] Question: where did harrison work? from langchain_core.output_parsers import StrOutputParser from langchain_openai import ChatOpenAI llm = ChatOpenAI() retrieval_chain = ( {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()} | prompt | llm | StrOutputParser() ) output = retrieval_chain.invoke(&quot;where did harrison work?&quot;) print(output) Harrison worked at Kensho. In LCEL chains, the two main composition primitives are RunnableSequence and RunnableParallel. RunnableSequence is a composition primitive to chain multiple runnables sequentially, with the output of one runnable serving as the input to the next. from langchain_core.runnables import RunnableSequence chain = RunnableSequence([runnable1, runnable2]) final_output = chain.invoke(some_input) corresponds to the following: output1 = runnable1.invoke(some_input) final_output = runnable2.invoke(output1) RunnableParallel is a composition primitive to run multiple runnables concurrently, with the same input provided to each. from langchain_core.runnables import RunnableParallel chain = RunnableParallel({ &quot;key1&quot;: runnable1, &quot;key2&quot;: runnable2, }) final_output = chain.invoke(some_input) { &quot;key1&quot;: runnable1.invoke(some_input), &quot;key2&quot;: runnable2.invoke(some_input), } The | (pipe) operator have been overloaded to create a RunnableSequence from two Runnables. chain = runnable1 | runnable2 is Equivalent to: chain = RunnableSequence([runnable1, runnable2]) is Equivalent to: chain = runnable1.pipe(runnable2) LCEL applies automatic type coercion to make it easier to compose chains. Inside an LCEL expression, a dictionary is automatically converted to a RunnableParallel. mapping = { &quot;key1&quot;: runnable1, &quot;key2&quot;: runnable2, } chain = mapping | runnable3 is automatically converted to the following: chain = RunnableSequence([RunnableParallel(mapping), runnable3]) Inside an LCEL expression, a function is automatically converted to a RunnableLambda. def some_func(x): return x chain = some_func | runnable1 is automatically converted to the following: chain = RunnableSequence([RunnableLambda(some_func), runnable1]) A dict object defines data routing in LCEL by mapping keys to Runnables, functions, or static values, while RunnablePassthrough duplicates data across the pipeline as a data conduit to orchestrate chain flow. chain = ( {&quot;input&quot;: RunnablePassthrough()} # capture initial input | { &quot;output&quot;: llm_chain, # generate LLM output &quot;input&quot;: RunnablePassthrough() # maintain original input } ) # output: {&quot;output&quot;: &quot;LLM&#39;s answer&quot;, &quot;input&quot;: &quot;user&#39;s question&quot;} 8. Semantic Search and Retrieval-Augmented Generation Dense retrieval, reranking, and Retrieval-Augmented Generation (RAG) represent three significant strategies for enhancing search using language models. Dense retrieval systems rely on the concept of embeddings, and turn the search problem into retrieving the nearest neighbors of the search query (after both the query and the documents are converted into embeddings). Figure 89. Dense retrieval is one of the key types of semantic search, relying on the similarity of text embeddings to retrieve relevant results. A reranking language model is one of multiple steps in search system pipelines and is tasked with scoring the relevance of a subset of results against the query; the order of results is then changed based on these scores. Figure 90. Rerankers, the second key type of semantic search, take a search query and a collection of results, and reorder them by relevance, often resulting in vastly improved results. An RAG (Retrieval-Augmented Generation) system is a text generation system that incorporates search capabilities to reduce hallucinations, increase factuality, and/or ground the generation model on a specific dataset. Figure 91. A RAG system formulates an answer to a question and (preferably) cites its information sources. 8.1. Semantic Search with Language Models An embedding is a numeric representation of text, where each text is intuitively represented as a point (or a vector), and texts with similar meaning are close to each other in the high multi-dimensional embedding space. 8.1.1. Dense Retrieval Figure 92. Dense retrieval relies on the property that search queries will be close to their relevant results. # dense retrieval with FAISS from sentence_transformers import SentenceTransformer import faiss text = &quot;&quot;&quot; Artificial intelligence was founded as an academic discipline in 1956. Alan Turing was the first person to conduct substantial research in AI. Born in Maida Vale, London, Turing was raised in southern England. &quot;&quot;&quot; sentences = text.split(&quot;.&quot;) sentences = [s.strip() for s in sentences if s.strip()] model = SentenceTransformer(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) # embedding the text chunks. xb = model.encode(sentences) # building the search index. d = xb.shape[1] index = faiss.IndexFlatL2(d) index.add(xb) # search the index q = &quot;Who is Alan Turing?&quot; xq = model.encode([q]) distances, indices = index.search(xq, 3) print(f&quot;Q: {q}&quot;) for i in range(len(indices[0])): sentence = sentences[indices[0][i]] distance = distances[0][i] print(f&quot; Sentence: {sentence}&quot;) print(f&quot; Distance: {distance:.4f}&quot;) Q: Who is Alan Turing? Sentence: Alan Turing was the first person to conduct substantial research in AI Distance: 0.4903 Sentence: Born in Maida Vale, London, Turing was raised in southern England Distance: 1.0674 Sentence: Artificial intelligence was founded as an academic discipline in 1956 Distance: 1.4276 # keyword search with BM25 import string import numpy as np from rank_bm25 import BM25Okapi from sklearn.feature_extraction import _stop_words from tqdm import tqdm def bm25_tokenizer(text: str): tokenized_doc = [] for token in text.lower().split(): token = token.strip(string.punctuation) if len(token) &gt; 0 and token not in _stop_words.ENGLISH_STOP_WORDS: tokenized_doc.append(token) return tokenized_doc tokenized_corpus = [] text = &quot;&quot;&quot; Artificial intelligence was founded as an academic discipline in 1956. Alan Turing was the first person to conduct substantial research in AI. Born in Maida Vale, London, Turing was raised in southern England. &quot;&quot;&quot; texts = text.split(&#39;.&#39;) for passage in tqdm(texts): tokenized_corpus.append(bm25_tokenizer(passage)) bm25 = BM25Okapi(tokenized_corpus) def keyword_search(q: str, k=3, n=3): print(&quot;Input question:&quot;, q) bm25_scores = bm25.get_scores(bm25_tokenizer(q)) top_n = np.argpartition(bm25_scores, -n)[-n:] bm25_hits = [ {&#39;corpus_id&#39;: idx, &#39;score&#39;: bm25_scores[idx]} for idx in top_n ] bm25_hits = sorted(bm25_hits, key=lambda x: x[&#39;score&#39;], reverse=True) print(&quot;Top-3 lexical search (BM25) hits&quot;) for hit in bm25_hits[0:k]: print( &quot;\t{:.3f}\t{}&quot;.format( hit[&#39;score&#39;], texts[hit[&#39;corpus_id&#39;]].replace(&quot;\n&quot;, &quot; &quot;) ) ) q = &quot;Who is Alan Turing?&quot; keyword_search(q=q, k=3, n=len(texts)) Input question: Who is Alan Turing? Top-3 lexical search (BM25) hits 0.737 Alan Turing was the first person to conduct substantial research in AI 0.000 Artificial intelligence was founded as an academic discipline in 1956 0.000 Born in Maida Vale, London, Turing was raised in southern England It’s useful to be aware of some of the drawbacks of dense retrieval and how to address them. Lack of Answer in Retrieved Texts Dense retrieval always returns results based on semantic similarity, even if none of the texts actually contain the answer to the query. A potential solution is to implement a distance threshold to filter out results that are not sufficiently relevant. User feedback (click-through rates and satisfaction) can also help improve the system over time. Difficulty with Exact Phrase Matches Dense retrieval, relying on semantic similarity, may not perform well when a user is looking for an exact match of a specific phrase. In such cases, traditional keyword matching is more effective, suggesting the use of hybrid search systems that combine both approaches. Domain Specificity Dense retrieval models trained on data from one domain (e.g., internet and Wikipedia) may not generalize well to other, unseen domains (e.g., legal texts) without sufficient training data from that new domain. Handling Multi-Sentence Answers Dense retrieval systems face the challenge of how to best chunk long texts into embeddings. A key design parameter is deciding the optimal way to divide documents, as answers to some questions may span multiple sentences, and models have context size limitations. Chunking strategies include embedding per document (which can lose information) or embedding multiple chunks per document (which offers better coverage). Various chunking methods exist, such as by sentence, paragraph, or overlapping segments to retain context, with the best approach depending on the text and query types. Scalability and Efficiency While simple nearest neighbor search with tools like NumPy works for smaller datasets, for millions of vectors, optimized approximate nearest neighbor (ANN) search libraries like FAISS or Annoy are necessary for efficient retrieval. Vector databases like Weaviate or Pinecone offer additional functionalities like adding/deleting vectors without rebuilding the index and advanced filtering options. Need for Fine-Tuning To optimize dense retrieval for specific tasks, fine-tuning the embedding models with relevant query-result pairs (including negative examples) is crucial. This process aims to bring embeddings of relevant queries and results closer together in the vector space while pushing irrelevant ones further apart. 8.1.2. Reranking A reranker takes in the search query and a number of search results, and returns the optimal ordering of these documents so the most relevant ones to the query are higher in ranking. Figure 93. LLM rerankers operate as part of a search pipeline with the goal of reordering a number of shortlisted search results by relevance. Figure 94. A reranker assigns a relevance score to each document by looking at the document and the query at the same time. For the retrieval, either lexical search, e.g. with a vector engine like Elasticsearch, or dense retrieval with a SentenceTransformer (a.k.a. bi-encoder) can be used. However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, a re-ranker based on a CrossEncoder that scores the relevancy of all shortlisted candidates for the given search query can be used to output a ranked list. from sentence_transformers import SentenceTransformer bi_encoder = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;) corpus = [ &quot;A man is eating food.&quot;, &quot;A man is eating a piece of bread.&quot;, &quot;The girl is carrying a baby.&quot;, &quot;A man is riding a horse.&quot;, &quot;A woman is playing violin.&quot;, &quot;Two men pushed carts through the woods.&quot;, &quot;A man is riding a white horse on an enclosed ground.&quot;, &quot;A monkey is playing drums.&quot;, &quot;A cheetah is running behind its prey.&quot;, ] corpus_embeddings = bi_encoder.encode(corpus, convert_to_tensor=True) query = &quot;A man is eating pasta.&quot; query_embedding = bi_encoder.encode(query, convert_to_tensor=True) top_N = min(10, len(corpus)) similarity_scores = bi_encoder.similarity(query_embedding, corpus_embeddings)[0] import torch scores, indices = torch.topk(similarity_scores, k=top_N) documents = [] for score, index in zip(scores, indices): document = corpus[index] print(f&quot;({score:.4f})&quot;, document) documents.append(document) (0.7035) A man is eating food. (0.5272) A man is eating a piece of bread. (0.1889) A man is riding a horse. (0.1047) A man is riding a white horse on an enclosed ground. (0.0980) A cheetah is running behind its prey. (0.0819) A monkey is playing drums. (0.0336) A woman is playing violin. (-0.0594) Two men pushed carts through the woods. (-0.0898) The girl is carrying a baby. from sentence_transformers import CrossEncoder cross_encoder = CrossEncoder(&quot;cross-encoder/ms-marco-MiniLM-L-6-v2&quot;) top_K = min(5, top_N) ranking = cross_encoder.rank( query, documents, top_k=top_K, return_documents=True, ) for r in ranking: print(f&quot;({r[&#39;score&#39;]:.4f})&quot;, r[&quot;text&quot;]) (1.9005) A man is eating food. (1.4804) A man is eating a piece of bread. (-7.0890) A man is riding a horse. (-8.9042) A man is riding a white horse on an enclosed ground. (-10.7628) A monkey is playing drums. 8.2. Retrieval-Augmented Generation (RAG) RAG systems incorporate search capabilities in addition to generation capabilities to enhance factuality and reduce hallucinations. Figure 95. A basic RAG pipeline is made up of a search step followed by a grounded generation step where the LLM is prompted with the question and the information retrieved from the search step. Figure 96. Generative search formulates answers and summaries at the end of a search pipeline while citing its sources (returned by the previous steps in the search system). Figure 97. Find the most relevant information to an input prompt by comparing the similarities between embeddings. The most relevant information is added to the prompt before giving it to the LLM. from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&quot;mistral:7b-instruct&quot;, api_key=&#39;APK-KEY&#39;, base_url=&quot;http://localhost:11434/v1&quot;, # Ollama ) from langchain_text_splitters import HTMLHeaderTextSplitter headers_to_split_on = [ (&quot;h1&quot;, &quot;Header 1&quot;), (&quot;h2&quot;, &quot;Header 2&quot;), (&quot;h3&quot;, &quot;Header 3&quot;), (&quot;h4&quot;, &quot;Header 4&quot;), ] html_splitter = HTMLHeaderTextSplitter(headers_to_split_on) url = &quot;https://plato.stanford.edu/entries/goedel/&quot; documents = html_splitter.split_text_from_url(url) from langchain_community.vectorstores import FAISS from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) db = FAISS.from_documents(documents, embeddings) from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot; Relevant information: {context} Provide a concise answer the following question using the relevant information provided above: {question} &quot;&quot;&quot; prompt = PromptTemplate.from_template(template=template) from langchain.chains.retrieval_qa.base import RetrievalQA rag = RetrievalQA.from_chain_type( llm=llm, chain_type=&quot;stuff&quot;, retriever=db.as_retriever(), chain_type_kwargs={&quot;prompt&quot;: prompt}, verbose=True, ) rag.invoke(&quot;Who is Kurt Gödel?&quot;) {&#39;query&#39;: &#39;Who is Kurt Gödel?&#39;, &#39;result&#39;: &quot; Kurt Gödel was an Austrian mathematician and logician. He is best known for his work on the incompleteness theorems, which were established in 1930 and prove that any sufficiently rich formal axiomatic system contains either statements that cannot be proven or disproven within the system itself. Some of Gödel&#39;s other notable contributions include his proof of the consistency of the continuum hypothesis using large cardinals, and his work on undecidable propositions in number theory, which led to the concept of Gödel numbers for representing mathematical statements in a formal system. Throughout his life, Gödel also explored philosophical questions related to logic, mathematics, and metaphysics, including questions about realism, the foundations of mathematics, set theory, and the nature of time and truth.&quot;} 9. Multimodal Large Language Models A multimodal model is a type of artificial intelligence model capable of processing and reasoning across different modalities, where a modality refers to a distinct type of data such as text, images, audio, video, or sensor data. Figure 98. Models that are able to deal with different types (or modalities) of data, such as images, audio, video, or sensors, are said to be multimodal. It’s possible for a model to accept a modality as input yet not be able to generate in that modality. 9.1. Vision Transformer (ViT) Vision Transformer (ViT) is a method that adapts the Transformer architecture to the field of computer vision, particularly for image recognition tasks, by treating an image as a sequence of flattened image patches which are then linearly embedded and processed by the Transformer encoder in a manner similar to textual tokens, allowing it to capture global relationships in the image more directly than the local receptive fields of convolutional neural networks (CNNs). Figure 99. The main algorithm behind ViT. After patching the images and linearly projecting them, the patch embeddings are passed to the encoder and treated as if they were textual tokens. 9.2. Multimodal Embedding Models A multimodal embedding model is a type of model that can create numerical representations (embeddings) for multiple modalities, such as text and imagery, within the same vector space, allowing for direct comparison of representations from different modalities based on their semantic content. Figure 100. Despite having coming from different modalities, embeddings with similar meaning will be close to each other in vector space. Contrastive Language-Image Pre-training (CLIP) is an embedding model to compute embeddings of both images and texts. Figure 101. In the first step of training CLIP, both images and text are embedded using an image and text encoder, respectively. Figure 102. In the second step of training CLIP, the similarity between the sentence and image embedding is calculated using cosine similarity. Figure 103. In the third step of training CLIP, the text and image encoders are updated to match what the intended similarity should be (called contrastive learning). This updates the embeddings such that they are closer in vector space if the inputs are similar. from urllib.request import urlopen from PIL import Image # load an AI-generated image of a puppy playing in the snow from a URL puppy_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/puppy.png&quot; ) # open the image from the URL and convert it to RGB format image = Image.open(urlopen(puppy_path)).convert(&quot;RGB&quot;) # define a text caption for the image caption = &quot;a puppy playing in the snow&quot; Figure 104. An AI-generated image of a puppy playing in the snow. from transformers import CLIPTokenizer, CLIPProcessor, CLIPModel model_id = &quot;openai/clip-vit-base-patch32&quot; # load the tokenizer associated with the CLIP model to preprocess text clip_tokenizer = CLIPTokenizer.from_pretrained(model_id, use_fast=True) # load the processor associated with the CLIP model to preprocess images and text clip_processor = CLIPProcessor.from_pretrained(model_id, use_fast=True) # load the main CLIP model for generating text and image embeddings model = CLIPModel.from_pretrained(model_id) # tokenize the input caption into numerical representations inputs = clip_tokenizer(caption, return_tensors=&quot;pt&quot;) inputs {&#39;input_ids&#39;: tensor([[49406, 320, 6829, 1629, 530, 518, 2583, 49407]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} # convert the token IDs back to the corresponding text tokens clip_tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0]) [&#39;&lt;|startoftext|&gt;&#39;, &#39;a&lt;/w&gt;&#39;, &#39;puppy&lt;/w&gt;&#39;, &#39;playing&lt;/w&gt;&#39;, &#39;in&lt;/w&gt;&#39;, &#39;the&lt;/w&gt;&#39;, &#39;snow&lt;/w&gt;&#39;, &#39;&lt;|endoftext|&gt;&#39;] # create a text embedding vector representing the semantic meaning of the caption text_embedding = model.get_text_features(**inputs) text_embedding.shape # (batch_size, embedding_dimension) torch.Size([1, 512]) # preprocess the image to match the input requirements of the CLIP model image_inputs = clip_processor(text=None, images=image, return_tensors=&quot;pt&quot;) image_pixel_values = image_inputs[&quot;pixel_values&quot;] image_pixel_values.shape # (batch_size, num_channels, height, width) torch.Size([1, 3, 224, 224]) import torch import numpy as np import matplotlib.pyplot as plt # prepare the preprocessed image tensor for visualization img = image_pixel_values.squeeze(0) # remove the batch dimension img = img.permute(*torch.arange(img.ndim - 1, -1, -1)) # transpose dimensions for correct visualization order (C, H, W -&gt; H, W, C) img = np.einsum(&quot;ijk-&gt;jik&quot;, img) # visualize the preprocessed image plt.imshow(img) # turn off axis labels and ticks plt.axis(&quot;off&quot;) Figure 105. The preprocessed input image by CLIP. # create the image embedding vector representing the visual content of the image image_embedding = model.get_image_features(image_pixel_values) image_embedding.shape # (batch_size, embedding_dimension): same as that of the text embedding torch.Size([1, 512]) # normalize the text and image embeddings text_embedding /= text_embedding.norm(dim=-1, keepdim=True) image_embedding /= image_embedding.norm(dim=-1, keepdim=True) # calculate the cosine similarity score text_embedding = text_embedding.detach().cpu().numpy() # move the text embedding to CPU and convert to NumPy array image_embedding = image_embedding.detach().cpu().numpy() # move the image embedding to CPU and convert to NumPy array score = np.dot(text_embedding, image_embedding.T) score array([[0.33146894]], dtype=float32) sentence-transformers implements a few CLIP-based models that make it much easier to create embeddings. It only takes a few lines of code: from urllib.request import urlopen from PIL import Image puppy_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/puppy.png&quot; ) image = Image.open(urlopen(puppy_path)).convert(&quot;RGB&quot;) caption = &quot;a puppy playing in the snow&quot; from sentence_transformers import SentenceTransformer, util model = SentenceTransformer(&quot;sentence-transformers/clip-ViT-B-32&quot;) image_embeddings = model.encode([image]) text_embeddings = model.encode([caption]) sim_matrix = util.cos_sim(image_embeddings, text_embeddings) sim_matrix # tensor([[0.3315]]) 9.3. Multimodal Text Generation Models BLIP-2 (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 2) is a multimodal text generation model designed to introduce vision capabilities to existing, pre-trained language models (LLMs) without requiring end-to-end training from scratch. Figure 106. The Querying Transformer is the bridge between vision (ViT) and text (LLM) that is the only trainable component of the pipeline. 9.3.1. BLIP-2: Bridging the Modality Gap BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), connecting a frozen (non-trainable) pre-trained image encoder like a Vision Transformer and a frozen pre-trained LLM. The Q-Former is trained in two stages, one for each modality to make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM to give information about the image in a similar manner to the context providing an LLM when prompting. Figure 107. In step 1, representation learning is applied to learn representations for vision and language simultaneously. In step 2, these representations are converted to soft visual prompts to feed the LLM. In step 1, image-document pairs are used to train the Q-Former to represent both images and text, which are generally captions of images similar tranning CLIP. Figure 108. In step 1, the output of the frozen ViT is used together with its caption and trained on three contrastive-like tasks to learn visual-text representations. The images are fed to the frozen ViT to extract vision embeddings, which are used as the input of Q-Former’s ViT, and the captions are used as the input of Q-Former’s Text Transformer. The Q-Former is then trained on three tasks: image-text contrastive learning that attempts to align pairs of image and text embeddings such that they maximize their mutual information, image-text matching that predicts whether an image and text pair is positive (matched) or negative (unmatched), and image-grounded text generation that generates text based on information extracted from the input image. In step 2, the learnable embeddings containing aligned visual and textual information in the same dimensional space from the Q-Former are projected to match the LLM&#8217;s input format and then serve as soft visual prompts, conditioning the LLM on the visual representations. Figure 109. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer. The projected embeddings serve as a soft visual prompt. 9.3.2. Preprocessing Multimodal Inputs from urllib.request import urlopen from PIL import Image # load image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) Figure 110. An orange supercar driving on the road at sunset. import torch from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=torch.float16, device_map=dev, ) model.vision_model # vision transformer in the loaded BLIP-2 model. Blip2VisionModel( (embeddings): Blip2VisionEmbeddings( (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14)) ) (encoder): Blip2Encoder( (layers): ModuleList( (0-38): 39 x Blip2EncoderLayer( (self_attn): Blip2Attention( (dropout): Dropout(p=0.0, inplace=False) (qkv): Linear(in_features=1408, out_features=4224, bias=True) (projection): Linear(in_features=1408, out_features=1408, bias=True) ) (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) (mlp): Blip2MLP( (activation_fn): GELUActivation() (fc1): Linear(in_features=1408, out_features=6144, bias=True) (fc2): Linear(in_features=6144, out_features=1408, bias=True) ) (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) ) ) ) (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) ) model.language_model # text generative model in the loaded BLIP-2 model. OPTForCausalLM( (model): OPTModel( (decoder): OPTDecoder( (embed_tokens): Embedding(50304, 2560, padding_idx=1) (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560) (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) (layers): ModuleList( (0-31): 32 x OPTDecoderLayer( (self_attn): OPTSdpaAttention( (k_proj): Linear(in_features=2560, out_features=2560, bias=True) (v_proj): Linear(in_features=2560, out_features=2560, bias=True) (q_proj): Linear(in_features=2560, out_features=2560, bias=True) (out_proj): Linear(in_features=2560, out_features=2560, bias=True) ) (activation_fn): ReLU() (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=2560, out_features=10240, bias=True) (fc2): Linear(in_features=10240, out_features=2560, bias=True) (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) ) ) ) ) (lm_head): Linear(in_features=2560, out_features=50304, bias=False) ) # preprocess the image image_inputs = blip_processor(image, return_tensors=&quot;pt&quot;).to(dev, torch.float16) image_pixel_values = image_inputs[&quot;pixel_values&quot;] image_pixel_values.shape # a 224 × 224-sized image torch.Size([1, 3, 224, 224]) # tokenizer used to tokenize the input text blip_processor.tokenizer GPT2TokenizerFast(name_or_path=&#39;Salesforce/blip2-opt-2.7b&#39;, vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;bos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;}, clean_up_tokenization_spaces=False, added_tokens_decoder={ 1: AddedToken(&quot;&lt;pad&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 2: AddedToken(&quot;&lt;/s&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 50265: AddedToken(&quot;&lt;image&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), } ) # preprocess the text text = &quot;Her vocalization was remarkably melodic&quot; token_ids = blip_processor(image, text=text, return_tensors=&quot;pt&quot;) token_ids = token_ids.to(dev, torch.float16)[&quot;input_ids&quot;][0] # convert input ids back to tokens tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids) tokens [&#39;&lt;/s&gt;&#39;, &#39;Her&#39;, &#39;Ġvocal&#39;, &#39;ization&#39;, &#39;Ġwas&#39;, &#39;Ġremarkably&#39;, &#39;Ġmel&#39;, &#39;odic&#39;] # replace the space token with an underscore tokens = [token.replace(&quot;Ġ&quot;, &quot;_&quot;) for token in tokens] tokens [&#39;&lt;/s&gt;&#39;, &#39;Her&#39;, &#39;_vocal&#39;, &#39;ization&#39;, &#39;_was&#39;, &#39;_remarkably&#39;, &#39;_mel&#39;, &#39;odic&#39;] 9.3.3. Use Case 1: Image Captioning from urllib.request import urlopen import torch from PIL import Image from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; dtype = torch.float16 if torch.cuda.is_available() else torch.float32 model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=dtype, device_map=dev, ) # load an AI-generated image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) # convert an image into inputs and preprocess it inputs = blip_processor(image, return_tensors=&quot;pt&quot;).to(dev, dtype) # {&#39;pixel_values&#39;: tensor([[[[-1.0039, -1.0039, -0.9893, ..., -0.0842, -0.0988, -0.0842], # generate image ids to be passed to the decoder (LLM) generated_ids = model.generate(**inputs, max_new_tokens=20) # generate text from the image ids generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text an orange supercar driving on the road at sunset 9.3.4. Use Case 2: Multimodal Chat-Based Prompting from urllib.request import urlopen import torch from PIL import Image from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; dtype = torch.float16 if torch.cuda.is_available() else torch.float32 model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=dtype, device_map=dev, ) # load an AI-generated image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) # visual question answering prompt = &quot;Question: Write down what you see in this picture. Answer:&quot; # process both the image and the prompt inputs = blip_processor(image, text=prompt, return_tensors=&quot;pt&quot;).to(dev, dtype) # generate text generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset # chat-like prompting: a follow-up question prompt = ( &quot;Question: Write down what you see in this picture. Answer: A sports &quot; &quot;car driving on the road at sunset. Question: What would it cost me to &quot; &quot;drive that car? Answer:&quot; ) # Generate output inputs = blip_processor(image, text=prompt, return_tensors=&quot;pt&quot;).to(dev, dtype) generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer: $1,000,000 10. Creating and Fine-Tuning Text Embedding Models Embedding models are Large Language Models (LLMs) used to convert unstructured textual data (like documents, sentences, or phrases) into dense numerical representations called embeddings. The primary goal of these models is to accurately capture the semantic meaning of the text, such that texts with similar meanings have embeddings that are close to each other in a high-dimensional vector space, while texts with different meanings have dissimilar embeddings. Figure 111. The idea of semantic similarity is that we expect textual data with similar meanings to be closer to each other in n-dimensional space (two dimensions are illustra‐ ted here). Embedding models can also be trained or fine-tuned for other purposes, such as capturing sentiment similarity, by guiding the model with appropriate training examples. Figure 112. In addition to semantic similarity, an embedding model can be trained to focus on sentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to positive reviews (green). 10.1. Contrastive Learning Contrastive learning is a self-supervised or supervised machine learning technique that aims to learn representations of data by contrasting similar (&quot;positive&quot;) and dissimilar (&quot;negative&quot;) examples (Why P and not Q?) to create an embedding space where similar data points are located close to each other, while dissimilar data points are far apart, which is effective in various domains, including computer vision and natural language processing, for tasks like representation learning, similarity search, and few-shot learning. Reporter: “Why did you rob a bank?” Robber: “Because that is where the money is.” Reporter (alternatively): “Why did you rob a bank (P) instead of obeying the law (Q)?” 10.2. Sentence Transformers (SBERT) A cross-encoder is a Transformer-based model that processes two sentences together to directly predict their similarity score via a classification head, but it&#8217;s computationally expensive for large-scale pairwise comparisons and doesn&#8217;t typically generate individual sentence embeddings. Figure 113. The architecture of a cross-encoder. Both sentences are concatenated, separated with a &lt;SEP&gt; token, and fed to the model simultaneously. The authors of sentence-transformers addressed the limitations of cross-encoders (slow speed, no embeddings) by developing a fast alternative that generates semantically comparable, fixed-size embeddings by using a Siamese architecture, also known as a bi-encoder or SBERT, with two identical BERT models (sharing weights) that process sentences independently and then apply mean pooling to the final layer. Figure 114. The architecture of the original sentence-transformers model, which leverages a Siamese network, also called a bi-encoder. 10.3. Creating an Embedding Model Natural Language Inference (NLI) datasets, used in pretraining embedding models, classify premise-hypothesis pairs as entailment (similar meaning), contradiction (opposite meaning), or neutral. Figure 115. We can leverage the structure of NLI datasets to generate negative examples (contradiction) and positive examples (entailments) for contrastive learning. Entailments serve as positive examples for contrastive learning (similar pairs), while contradictions serve as negative examples (dissimilar pairs). The Multi-Genre Natural Language Inference (MNLI) corpus from the General Language Understanding Evaluation (GLUE) benchmark contains annotated sentence pairs with these relationships, and is a common source for generating such contrastive training data. A subset of MNLI is often used for faster experimentation, though larger, quality datasets are generally preferred for stable training. from datasets import load_dataset # Load MNLI dataset from GLUE # 0 = entailment, 1 = neutral, 2 = contradiction train_dataset = load_dataset( &quot;glue&quot;, # load a dataset from the GLUE benchmark &quot;mnli&quot;, # load the MNLI dataset split=&quot;train&quot;, # load the training split ).select(range(50_000)) train_dataset = train_dataset.remove_columns(&quot;idx&quot;) train_dataset[2] {&#39;premise&#39;: &#39;One of our number will carry out your instructions minutely.&#39;, &#39;hypothesis&#39;: &#39;A member of my team will execute your orders with immense precision.&#39;, &#39;label&#39;: 0} # train model from sentence_transformers import SentenceTransformer # use a base model model = SentenceTransformer(&quot;google-bert/bert-base-uncased&quot;) from sentence_transformers import losses # define the softmax loss function. train_loss = losses.SoftmaxLoss( model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=3, ) from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # create an embedding similarity evaluator for STSB val_sts = load_dataset(&quot;glue&quot;, &quot;stsb&quot;, split=&quot;validation&quot;) evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[&quot;sentence1&quot;], sentences2=val_sts[&quot;sentence2&quot;], scores=[score / 5 for score in val_sts[&quot;label&quot;]], main_similarity=&quot;cosine&quot;, ) from sentence_transformers.training_args import ( SentenceTransformerTrainingArguments, ) args = SentenceTransformerTrainingArguments( output_dir=&quot;base_embedding_model&quot;, num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32, warmup_steps=100, fp16=True, eval_steps=100, logging_steps=100, ) from sentence_transformers.trainer import SentenceTransformerTrainer # train embedding model trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator, ) trainer.train() # evaluate the trained model evaluator(model) References [1] Jay Alammar, Maarten Grootendorst Hands-On Large Language Models: Language Understanding and Generation. O&#8217;Reilly Media; 1st edition (October 15, 2024)" />
<meta property="og:description" content="1. Language AI 2. Tokens and Embeddings 2.1. LLM Tokenization 2.2. Token Embeddings 2.3. Text Embeddings 3. Large Language Models 3.1. Inputs and Outputs 3.2. Components 3.3. Probability Distribution (Sampling/Decoding) 3.4. Parallel Token Processing and Context Size 3.5. Keys and Values Caching 3.6. Transformer Block 4. Text Classification 4.1. Representation Models 4.1.1. Task-Specific Model 4.1.2. Embedding model 4.2. Generative Models 4.2.1. Text-to-Text Transfer Transformer 4.2.2. ChatGPT for Classification 5. Text Clustering and Topic Modeling 5.1. ArXiv’s Articles: Computation and Language 5.2. A Common Pipeline for Text Clustering 5.2.1. Embedding Documents 5.2.2. Reducing the Dimensionality of Embeddings 5.2.3. Cluster the Reduced Embeddings 5.2.4. Inspecting the Clusters 5.3. From Text Clustering to Topic Modeling 5.3.1. BERTopic: A Modular Topic Modeling Framework 6. Prompt Engineering 6.1. Using Text Generation Models 6.1.1. Prompt Template 6.1.2. Controlling Model Output 6.2. Prompt Engineering 6.3. Instruction-Based Prompting 6.4. Advanced Prompt Engineering 6.4.1. Prompt Components 6.4.2. In-Context Learning: Providing Examples 6.4.3. Chain Prompting: Breaking up the Problem 6.5. Reasoning with Generative Models 6.5.1. Chain-of-Thought: Think Before Answering 6.5.2. Self-Consistency: Sampling Outputs 6.5.3. Tree-of-Thought: Exploring Intermediate Steps 6.6. Output Verification 6.6.1. Providing Examples 6.6.2. Grammar: Constrained Sampling 7. Advanced Text Generation Techniques and Tools 7.1. Model I/O: Loading Quantized Models with LangChain 7.2. Chains: Extending the Capabilities of LLMs 7.2.1. A Single Link in the Chain: Prompt Template 7.2.2. A Chain with Multiple Prompts 7.3. Memory: Helping LLMs to Remember Conversations 7.3.1. Conversation Buffer 7.3.2. Windowed Conversation Buffer 7.3.3. Conversation Summary 7.4. Agents: Creating a System of LLMs Appendix A: LangChain 7.A.1. Chat Models and Messages 7.A.2. Prompt Templates 7.A.3. Structured Outputs 7.A.4. Output Parsers 7.A.5. Embedding, Vector Stores, and Retrievers 7.A.6. Document Loaders 7.A.7. Text Splitters 7.A.8. Tools 7.A.9. Chat History 7.A.10. Memory 7.A.11. LangChain Expression Language (LCEL) 8. Semantic Search and Retrieval-Augmented Generation 8.1. Semantic Search with Language Models 8.1.1. Dense Retrieval 8.1.2. Reranking 8.2. Retrieval-Augmented Generation (RAG) 9. Multimodal Large Language Models 9.1. Vision Transformer (ViT) 9.2. Multimodal Embedding Models 9.3. Multimodal Text Generation Models 9.3.1. BLIP-2: Bridging the Modality Gap 9.3.2. Preprocessing Multimodal Inputs 9.3.3. Use Case 1: Image Captioning 9.3.4. Use Case 2: Multimodal Chat-Based Prompting 10. Creating and Fine-Tuning Text Embedding Models 10.1. Contrastive Learning 10.2. Sentence Transformers (SBERT) 10.3. Creating an Embedding Model References 1. Language AI Google Colab offers free, cloud-based GPU and TPU access for accelerated computation, subject to usage limits, and requires changing the runtime type to GPU to enable it. Artificial Intelligence (AI) is the science and engineering of creating intelligent machines, particularly intelligent computer programs, that can perform tasks similar to human intelligence. Language AI is a subfield of AI focused on developing technologies that can understand, process, and generate human language, which is often used interchangeably with Natural Language Processing (NLP). Figure 1. A peek into the history of Language AI. Figure 2. Language AI is capable of many tasks by processing textual input. The Bag-of-Words, a representation model, converts text to numerical vectors by tokenizing it—splitting sentences into individual words or subwords (tokens)—creating a vocabulary, and counting token occurrences to form a vector representation (the &#39;bag of words&#39;). Figure 3. A bag-of-words is created by counting individual words. These values are referred to as vector representations. Word2vec introduced dense vector embeddings, a significant improvement over Bag-of-Words, by using neural networks to capture the semantic meaning of words based on their context within large datasets, allowing for the measurement of semantic similarity. Figure 4. Embeddings of words that are similar will be close to each other in dimensional space. Figure 5. Embeddings can be created for different types of input. Attention-based Transformer models, replacing RNNs which struggled with long sentences, enabled parallel processing and context-aware language representation by using stacked encoders and decoders to focus on relevant input, revolutionizing language AI. Figure 6. Using word2vec embeddings, a context embedding is generated that represents the entire sequence. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder. Figure 7. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder. Figure 8. The encoder block revolves around self-attention to generate intermediate representations. Figure 9. The decoder has an additional attention layer that attends to the output of the encoder. Encoder-only models (a.k.a., representation models) like Bidirectional Encoder Representations from Transformers(BERT) excel at language representation through masked language modeling, while decoder-only models (a.k.a., generative models) like Generative Pre-trained Transformer (GPT) focus on text generation and are the foundation for large language models. Figure 10. The architecture of a BERT base model with 12 encoders. Figure 11. The architecture of a GPT-1. It uses a decoder-only architecture and removes the encoder-attention block. Generative LLMs function as sequence-to-sequence machines, initially designed for text completion, but their capability to be fine-tuned into chatbots or instruct models that can follow user prompts revealed their true potential. Figure 12. Generative LLMs take in some input and try to complete it. With instruct models, this is more than just autocomplete and attempts to answer the question. The context length, or window, represents the maximum number of tokens the model can process, enabling the generative LLM to handle larger documents, and the current length expands as the model generates new tokens due to its autoregressive nature. Figure 13. The context length is the maximum context an LLM can handle. LLMs differ from traditional machine learning by using a two-step training process: pretraining, for general language learning, and fine-tuning (or post-training), to adapt the pretrained (foundation/base) model for specific tasks. Figure 14. Compared to traditional machine learning, LLM training takes a multistep approach. Closed-source LLMs, like GPT-4 and Claude, are models that do not have their weights and architecture shared with the public, which are accessed via APIs, and offer high performance with managed hosting, but are costly and limit user control; open LLMs, such as Llama, share their architecture, enabling local use, fine-tuning, and privacy, but require powerful hardware and expertise. The main source for finding and downloading LLMs is the Hugging Face Hub. Hugging Face is the organization behind the well-known Transformers package, which for years has driven the development of language models in general. # If a connection to the Hugging Face URL (https://huggingface.co/) fails, try to set the HF_ENDPOINT environment variable to the mirror URL. import os os.environ[&quot;HF_ENDPOINT&quot;] = &quot;https://hf-mirror.com&quot; Hugging Face, the organization behind the Transformers package, is the primary source for finding and downloading LLMs, built upon the Transformer framework. import os from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # HF_ENDPOINT controls the base URL used by the transformers library # to download models and other resources from the Hugging Face Hub. os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # create a pipeline pipe = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=True, ) # the prompt (user input / query) messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a funny joke about chickens.&quot;}] # generate output output = pipe(messages) print(output[0][&quot;generated_text&quot;]) Why did the chicken join the band? Because he heard they had the &quot;cluck-loudest&quot; performers around! # clear memory and empty the VRAM import gc import torch # attempt to delete the model, tokenizer, and pipeline objects from memory del model, tokenizer, pipe # flush memory gc.collect() if torch.cuda.is_available(): # if a GPU is available, empty the CUDA cache to free up GPU memory torch.cuda.empty_cache() 2. Tokens and Embeddings Tokens and embeddings are two of the central concepts of using large language models (LLMs). Figure 15. Language models deal with text in small chunks called tokens. For the lan‐ guage model to compute language, it needs to turn tokens into numeric representations called embeddings. 2.1. LLM Tokenization import os import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # HF_ENDPOINT controls the base URL used by the transformers library # to download models and other resources from the Hugging Face Hub. os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) prompt = &#39;&lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;&#39; # tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=&#39;pt&#39;).input_ids.to(dev) print(f&#39;input_ids: {input_ids}&#39;) # generate the text output_ids = model.generate(input_ids=input_ids, max_new_tokens=20) print(f&#39;output_ids: {output_ids}&#39;) # print the output print(tokenizer.decode(output_ids[0])) input_ids: tensor([[101950, 29, 16465, 448, 3719, 39950, 6396, 316, 32145, 395, 290, 62374, 66241, 80785, 403, 13, 115474, 1495, 480, 12570, 13, 200019]]) output_ids: tensor([[101950, 29, 16465, 448, 3719, 39950, 6396, 316, 32145, 395, 290, 62374, 66241, 80785, 403, 13, 115474, 1495, 480, 12570, 13, 200019, 18174, 25, 336, 2768, 512, 6537, 10384, 395, 290, 193145, 147276, 403, 279, 36210, 32145, 4464, 40, 5498, 495, 3719]]) &lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;Subject: Sincere Apologies for the Gardening Mishap Dear Sarah, I hope this email Tokens, the units into which text prompts are broken for model input, also form the model&#8217;s output. Figure 16. A tokenizer encodes input prompts into token ID lists for the language model and decodes the model&#8217;s output token IDs back into words or tokens. Each ID corresponds to a specific token (character, word, or subword) in the tokenizer&#8217;s vocabulary. The tokenizer&#8217;s vocabulary acts as a lookup table, allowing the model to convert between text and these integer representations. for id in [101950, 29, 16465, 448, 3719, 39950]: print(tokenizer.decode(id)) # &lt;s # &gt; # Write # an # email # apolog for id in [18174, 25, 336, 2768, 512]: print(tokenizer.decode(id) # Subject # : # S # inc # ere Tokenization is determined by three major design decisions: the tokenizer algorithm (e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size, special tokens, capitalization, treatment of capitalization and different languages), and the dataset the tokenizer is trained on (a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset). Tokenization methods vary in granularity, from word-level to byte-level, with subword tokenization offering a balance of vocabulary expressiveness and efficiency, making it the most common approach in modern language models. 2.2. Token Embeddings Text --&gt; Tokens --&gt; Token IDs --&gt; Embeddings (Vectors) A tokenizer, once trained, becomes intrinsically linked to its language model during the model&#8217;s training; consequently, a pretrained language model cannot function with a different tokenizer without retraining, as their vocabularies and tokenization schemes are aligned. An embedding is a dense, numerical vector representation of a token (like a word or subword) that captures its semantic meaning within a high-dimensional space, enabling language models to understand and process relationships between words. A language model stores static embedding vectors for each token in its vocabulary, but also generates contextualized word embeddings, dynamically representing a token based on its context instead of a single, fixed vector. A language model holds an embedding vector associated with each token in its tokenizer. Figure 17. A language model holds an embedding vector associated with each token in its tokenizer. A language model operates on raw, static embeddings as its input and produces contextual text embeddings. Figure 18. A language model operates on raw, static embeddings as its input and produces contextual text embeddings. from transformers import AutoModel, AutoTokenizer # load a tokenizer tokenizer = AutoTokenizer.from_pretrained(&#39;microsoft/deberta-base&#39;) # load a language model model = AutoModel.from_pretrained(&#39;microsoft/deberta-v3-xsmall&#39;) # tokenize the sentence: convert text to token IDs tokens = tokenizer(&#39;Hello world&#39;, return_tensors=&#39;pt&#39;) # print the decoded tokens to show tokenization for token_id in tokens[&#39;input_ids&#39;][0]: print(tokenizer.decode(token_id)) print(&#39;\n&#39;) # process the token IDs through the model to get contextualized embeddings output = model(**tokens)[0] # show the shape of the embedding result print(f&#39;{output.shape}\n&#39;) # output contains the contextualized embedding vectors print(output) [CLS] Hello world [SEP] torch.Size([1, 4, 384]) tensor([[[-3.4816, 0.0861, -0.1819, ..., -0.0612, -0.3911, 0.3017], [ 0.1898, 0.3208, -0.2315, ..., 0.3714, 0.2478, 0.8048], [ 0.2071, 0.5036, -0.0485, ..., 1.2175, -0.2292, 0.8582], [-3.4278, 0.0645, -0.1427, ..., 0.0658, -0.4367, 0.3834]]], grad_fn=&lt;NativeLayerNormBackward0&gt;) 2.3. Text Embeddings Text embeddings are single, dense vectors that represent the semantic meaning of entire sentences, paragraphs, or documents, in contrast to token embeddings, which represent individual words or subwords. from sentence_transformers import SentenceTransformer # load model model = SentenceTransformer(&#39;sentence-transformers/all-MiniLM-L6-v2&#39;) # convert text to text embeddings embeddings = model.encode(&quot;Best movie ever!&quot;) print(embeddings.shape) # (384,) Input Sequence Length: https://www.sbert.net/ For transformer models like BERT, RoBERTa, DistilBERT etc., the runtime and memory requirement grows quadratic with the input length. This limits transformers to inputs of certain lengths. A common value for BERT-based models are 512 tokens, which corresponds to about 300-400 words (for English). Each model has a maximum sequence length under model.max_seq_length, which is the maximal number of tokens that can be processed. Longer texts will be truncated to the first model.max_seq_length tokens: from sentence_transformers import SentenceTransformer model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;) print(&quot;Max Sequence Length:&quot;, model.max_seq_length) # =&gt; Max Sequence Length: 256 # Change the length to 200 model.max_seq_length = 200 print(&quot;Max Sequence Length:&quot;, model.max_seq_length) # =&gt; Max Sequence Length: 200 3. Large Language Models import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # create a pipeline generator = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=50, do_sample=False, ) 3.1. Inputs and Outputs The most common picture of understanding the behavior of a Transformer LLM is to think of it as a software system that takes in text and generates text in response. Once a large enough text-in-text-out model is trained on a large enough high-quality dataset, it becomes able to generate impressive and useful outputs. Figure 19. At a high level of abstraction, Transformer LLMs take a text prompt and output generated text. The model does not generate the text all in one operation; it actually generates one token at a time. Figure 20. Transformer LLMs generate one token at a time, not the entire text at once. Each token generation step is one forward pass through the model (that’s machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph). After each token generation, the input prompt for the next generation step is tweaked by appending the output token to the end of the input prompt. Figure 21. An output token is appended to the prompt, then this new text is presented to the model again for another forward pass to generate the next token. Text generation LLMs are called autoregressive models because they generate text sequentially, using prior outputs as input, unlike text representation models like BERT, which process the entire input at once. 3.2. Components A language model consists of a tokenizer, a stack of Transformer blocks for processing, and an LM head that converts the processed information into probability scores for the next token. Figure 22. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head. The model has a vector representation associated with each of these tokens in the vocabulary (token embeddings). Figure 23. The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings. For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probability distribution for the next token. Figure 24. At the end of the forward pass, the model predicts a probability score for each token in the vocabulary. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) print(model) Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(200064, 3072, padding_idx=199999) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear(in_features=3072, out_features=3072, bias=False) (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False) ) (mlp): Phi3MLP( (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False) (down_proj): Linear(in_features=8192, out_features=3072, bias=False) (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (resid_attn_dropout): Dropout(p=0.0, inplace=False) (resid_mlp_dropout): Dropout(p=0.0, inplace=False) ) ) (norm): Phi3RMSNorm((3072,), eps=1e-05) (rotary_emb): Phi3RotaryEmbedding() ) (lm_head): Linear(in_features=3072, out_features=200064, bias=False) ) 3.3. Probability Distribution (Sampling/Decoding) Language models use a probability distribution to determine the next token, which is called the decoding strategy. The easiest strategy would be to always pick the token with the highest probability score, which is called greedy decoding (equivalent to setting the temperature to zero in an LLM). In practice, this doesn’t tend to lead to the best outputs for most use cases. A better approach is to introduce randomness by sampling from the probability distribution, sometimes choosing the second or third highest probability token. 3.4. Parallel Token Processing and Context Size Transformers excel at parallel processing, unlike earlier architectures, which is evident in how they handle token generation. Each input token is processed simultaneously through its own computation path or stream. Figure 25. Each token is processed through its own stream of computation (with some interaction between them in attention steps). A model with 4K context length or context size can only process 4K tokens and would only have 4K of these streams. Each of the token streams starts with an input vector (the embedding vector and some positional information). Figure 26. Each processing stream takes a vector as input and produces a final resulting vector of the same size (often referred to as the model dimension). At the end of the stream, another vector emerges as the result of the model’s processing. For text generation, only the output result of the last stream is used to predict the next token. That output vector is the only input into the LM head as it calculates the probabilities of the next token. 3.5. Keys and Values Caching Transformer models use a key/value (KV) cache to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), speeding up text generation by avoiding redundant calculations. Figure 27. When generating text, it’s important to cache the computation results of previous tokens instead of repeating the same calculation over and over again. In Hugging Face Transformers, cache is enabled by default, and can be disabled it by setting use_cache to False. prompt = &#39;Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&#39; input_ids = tokenizer(prompt, return_tensors=&#39;pt&#39;).input_ids.to(dev) generation_output = model.generate( input_ids=input_ids, max_new_tokens=100, use_cache=False, ) 3.6. Transformer Block Transformer LLMs are composed of a series Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs) and each block processes its inputs, then passes the results of its processing to the next block. Figure 28. The bulk of the Transformer LLM processing happens inside a series of Transformer blocks, each handing the result of its processing as input to the subsequent block. A Transformer block is made up of two successive components: Figure 29. A Transformer block is made up of a self-attention layer and a feedforward neural network. The attention layer is mainly concerned with incorporating relevant information from other input tokens and positions The feedforward layer houses the majority of the model’s processing capacity The feedforward network in a Transformer model stores learned information, such as &#39;The Shawshank&#39; and &#39;Redemption,&#39; and enables interpolation and generalization for generating text on unseen inputs. Figure 30. The feedforward neural network component of a Transformer block likely does the majority of the model’s memorization and interpolation. The attention layer in a Transformer model enables context awareness, crucial for language understanding beyond simple memorization. Figure 31. The self-attention layer incorporates relevant information from previous positions that help process the current token. 4. Text Classification A common task in natural language processing is classification, where the goal is to train a model to assign a label or class to input text, a technique widely used in applications like sentiment analysis and intent detection, significantly impacted by both representative and generative language models. Figure 32. Although both representation and generative models can be used for classification, their approaches differ. The Hugging Face Hub is a collaborative platform for machine learning resources (models, datasets, applications), and the datasets package can be used to load datasets. The dataset is split into train (for training), test (for final evaluation), and validation (for intermediate generalization checks, especially during hyperparameter tuning). from datasets import load_dataset # load data data = load_dataset(&quot;rotten_tomatoes&quot;) # the well-known &#39;rotten_tomatoes&#39; dataset data DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 8530 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 1066 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 1066 }) }) 4.1. Representation Models Classification with pretrained representation models generally comes in two flavors, either using a task-specific model or an embedding model. Figure 33. A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings. A task-specific model is a representation model, such as BERT, trained for a specific task, like sentiment analysis. An embedding model generates general-purpose embeddings that can be used for a variety of tasks not limited to classification, like semantic search. Figure 34. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings. 4.1.1. Task-Specific Model from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&quot;rotten_tomatoes&quot;) # determine the device to use for computation (GPU if available, otherwise CPU) import torch dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; from transformers import pipeline # specify the path to the pre-trained Twitter-RoBERTa-base for Sentiment Analysis model model_path = &quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot; # load the pre-trained sentiment analysis model into a pipeline for easy inference pipe = pipeline( model=model_path, tokenizer=model_path, return_all_scores=True, # return the scores for all sentiment labels device=dev, # specify the device to run the pipeline on ) import numpy as np from tqdm import tqdm # for progress bar during inference from transformers.pipelines.pt_utils import KeyDataset # utility to feed data to the pipeline # run inference on the test dataset y_pred = [] # list to store the predicted sentiment labels for output in tqdm( # iterate through the &#39;text&#39; column of the test dataset pipe(KeyDataset(data[&quot;test&quot;], &quot;text&quot;)), total=len(data[&quot;test&quot;]) ): # extract the negative sentiment score negative_score = output[0][&quot;score&quot;] # extract the positive sentiment score (assuming labels are ordered: negative, neutral, positive) positive_score = output[2][&quot;score&quot;] # predict the sentiment based on the highest score (0 for negative, 1 for positive) assignment = np.argmax([negative_score, positive_score]) # add the predicted label to the list y_pred.append(assignment) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&quot;Negative Review&quot;, &quot;Positive Review&quot;] ) print(performance) # evaluate the performance of the sentiment analysis model on the test set evaluate_performance(data[&quot;test&quot;][&quot;label&quot;], y_pred) # compare the true labels with the predicted labels precision recall f1-score support Negative Review 0.76 0.88 0.81 533 Positive Review 0.86 0.72 0.78 533 accuracy 0.80 1066 macro avg 0.81 0.80 0.80 1066 weighted avg 0.81 0.80 0.80 1066 The above generated classification report shows four such methods: precision, recall, accuracy, and the F1 score. Precision measures how many of the items found are relevant, which indicates the accuracy of the relevant results. Recall refers to how many relevant classes were found, which indicates its ability to find all relevant results. Accuracy refers to how many correct predictions the model makes out of all predictions, which indicates the overall correctness of the model. The F1 score balances both precision and recall to create a model’s overall performance. A confusion matrix visualizes the performance of a classification model by showing the counts of four prediction outcomes: True Positives, True Negatives, False Positives, and False Negatives, which serves as the basis for calculating various metrics to evaluate the model&#8217;s quality. Figure 35. The confusion matrix describes four types of predictions. Figure 36. The classification report describes several metrics for evaluating a model’s performance. 4.1.2. Embedding model Without fine-tuning a representation model, a general-purpose embedding model can generate features that are then fed into a separate, trainable classifier (like logistic regression, which can be trained efficiently on a CPU), creating a two-step classification approach. A major benefit of this separation is avoiding the costly fine-tuning of the embedding model, instead, a classifier, such as logistic regression, can be trained efficiently on the CPU. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&quot;rotten_tomatoes&quot;) # load the SentenceTransformer model for generating text embeddings from sentence_transformers import SentenceTransformer model = SentenceTransformer(&quot;sentence-transformers/all-mpnet-base-v2&quot;) # convert the text data from the train and test splits into embeddings train_embeddings = model.encode(data[&quot;train&quot;][&quot;text&quot;], show_progress_bar=True) test_embeddings = model.encode(data[&quot;test&quot;][&quot;text&quot;], show_progress_bar=True) from sklearn.linear_model import LogisticRegression # train a logistic regression classifier on the generated training embeddings # initialize the logistic regression model with a random state for reproducibility clf = LogisticRegression(random_state=42) # train the classifier using the training embeddings and their corresponding labels clf.fit(train_embeddings, data[&quot;train&quot;][&quot;label&quot;]) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&quot;Negative Review&quot;, &quot;Positive Review&quot;] ) print(performance) # predict the sentiment labels for the test embeddings using the trained classifier y_pred = clf.predict(test_embeddings) # evaluate the performance of the classifier on the test set evaluate_performance(data[&quot;test&quot;][&quot;label&quot;], y_pred) precision recall f1-score support Negative Review 0.85 0.86 0.85 533 Positive Review 0.86 0.85 0.85 533 accuracy 0.85 1066 macro avg 0.85 0.85 0.85 1066 weighted avg 0.85 0.85 0.85 1066 Zero-shot classification can be used on unlabeled data by leveraging the model&#8217;s pre-existing knowledge to predict labels based solely on their definitions. In zero-shot classification, without any labeled examples, the model determines the relationship between input text and predefined candidate labels. Figure 37. In zero-shot classification, we have no labeled data, only the labels them‐ selves. The zero-shot model decides how the input is related to the candidate labels. Zero-shot classification generates target labels without labeled data by describing and embedding labels (e.g., &quot;negative movie review&quot;) and documents. Figure 38. To embed the labels, we first need to give them a description, such as “a negative movie review.” This can then be embedded through sentence-transformers. To assign labels to documents in zero-shot classification, cosine similarity, representing the cosine of the angle between the embedding vectors, can be applied to document-label embedding pairs. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) from sentence_transformers import SentenceTransformer # load model model = SentenceTransformer(&#39;sentence-transformers/all-mpnet-base-v2&#39;) # convert text to embeddings train_embeddings = model.encode(data[&#39;train&#39;][&#39;text&#39;], show_progress_bar=True) test_embeddings = model.encode(data[&#39;test&#39;][&#39;text&#39;], show_progress_bar=True) # create embeddings for our labels label_embeddings = model.encode([&#39;A negative review&#39;, &#39;A positive review&#39;]) import numpy as np from sklearn.metrics.pairwise import cosine_similarity # find the best matching label for each document using cosine similarity sim_matrix = cosine_similarity(test_embeddings, label_embeddings) # get the index of the label with the highest similarity score for each test embedding y_pred = np.argmax(sim_matrix, axis=1) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) precision recall f1-score support Negative Review 0.78 0.77 0.78 533 Positive Review 0.77 0.79 0.78 533 accuracy 0.78 1066 macro avg 0.78 0.78 0.78 1066 weighted avg 0.78 0.78 0.78 1066 From Wikipedia, the free encyclopedia In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval [−1, 1]. import numpy as np # import the NumPy library for numerical operations A = np.array([1, 2, 3]) # create a NumPy array named A B = np.array([4, 5, 6]) # create a NumPy array named B # calculate the cosine similarity using the formula: (A dot B) / (||A|| * ||B||) dot_product = np.dot(A, B) # calculate the dot product of A and B norm_A = np.linalg.norm(A) # calculate the Euclidean norm (magnitude) of A norm_B = np.linalg.norm(B) # calculate the Euclidean norm (magnitude) of B cosine_similarity = dot_product / (norm_A * norm_B) # calculate the cosine similarity print(cosine_similarity) # 0.9746318461970762 4.2. Generative Models Text classification with generative language models (like GPT) involves feeding input text to the model and having it generate text as output, in contrast to task-specific models that directly output a class label. Figure 39. A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens. Generative models are generally trained on a wide variety of tasks and usually don&#8217;t inherently know how to handle specific tasks like classifying a movie review without explicit instructions. Prompt engineering is the skill of crafting effective instructions, or prompts, to guide generative AI models towards producing desired and high-quality outputs for specific tasks, like text classification, which often involves iterative refinement of these prompts based on the model&#8217;s responses. Figure 40. Prompt engineering allows prompts to be updated to improve the output generated by the model. 4.2.1. Text-to-Text Transfer Transformer Text-to-Text Transfer Transformer or T5, like the original Transformer, is a generative encoder-decoder sequence-to-sequence model, contrasting with encoder-only BERT and decoder-only GPT. Figure 41. The T5 architecture is similar to the original Transformer model, a decoder- encoder architecture. In the first step of training, namely pretraining, encoder-decoder models like T5 are initially trained with a masked language modeling objective that masks sets of tokens (or token spans), differing from BERT&#8217;s individual token masking approach. Figure 42. In the first step of training, namely pretraining, the T5 model needs to predict masks that could contain multiple tokens. In the second step of training, namely fine-tuning the base model, instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously. Figure 43. By converting specific tasks to textual instructions, the T5 model can be trained on a variety of tasks during fine-tuning. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) import torch # determine the device to use for computation (GPU if available, otherwise CPU) dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; from transformers import pipeline # specify the path to the pre-trained FLAN-T5-small model for text-to-text generation model_path = &#39;google/flan-t5-small&#39; # load the pre-trained text-to-text generation model into a pipeline for easy inference pipe = pipeline( &#39;text2text-generation&#39;, model=model_path, device=dev, ) # prepare our data by creating a prompt and combining it with the text prompt = &#39;Is the following sentence positive or negative? &#39; # apply the prompt to each example in the dataset&#39;s &#39;text&#39; column to create a new &#39;t5&#39; column data = data.map(lambda example: {&#39;t5&#39;: prompt + example[&#39;text&#39;]}) # data # uncomment to inspect the modified dataset from tqdm import tqdm # for progress bar during inference from transformers.pipelines.pt_utils import ( KeyDataset, ) # utility to feed data to the pipeline # Run inference y_pred = [] # iterate through the test dataset using the pipeline for text generation for output in tqdm( pipe(KeyDataset(data[&#39;test&#39;], &#39;t5&#39;)), total=len(data[&#39;test&#39;]) ): # extract the generated text from the pipeline&#39;s output text = output[0][&#39;generated_text&#39;] # classify the generated text as 0 (negative) if it equals &#39;negative&#39;, otherwise 1 (positive) y_pred.append(0 if text == &#39;negative&#39; else 1) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) # evaluate the performance of the model by comparing the true labels with the predicted labels evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) precision recall f1-score support Negative Review 0.83 0.85 0.84 533 Positive Review 0.85 0.83 0.84 533 accuracy 0.84 1066 macro avg 0.84 0.84 0.84 1066 weighted avg 0.84 0.84 0.84 1066 4.2.2. ChatGPT for Classification OpenAI shared an overview of the training procedure that involved an important component, namely preference tuning. OpenAI first manually created the desired output to an input prompt (instruction data) and used that data to create a first variant of its model. Figure 44. Manually labeled data consisting of an instruction (prompt) and output was used to perform fine-tuning (instruction-tuning). OpenAI used the resulting model to generate multiple outputs that were manually ranked from best to worst. Figure 45. Manually ranked preference data was used to generate the final model, ChatGPT. import openai # create client for interacting with OpenAI API client = openai.OpenAI(api_key=&#39;YOUR_KEY_HERE&#39;) def chatgpt_generation(prompt, document, model=&#39;gpt-3.5-turbo-0125&#39;): &#39;&#39;&#39;Generate an output based on a prompt and an input document using ChatGPT.&#39;&#39;&#39; # define the message structure for the OpenAI API messages = [ {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: prompt.replace(&#39;[DOCUMENT]&#39;, document)}, ] # call the OpenAI Chat Completions API to get a response chat_completion = client.chat.completions.create( messages=messages, model=model, temperature=0 # temperature=0 for deterministic output ) # return the content of the first choice&#39;s message return chat_completion.choices[0].message.content # define a prompt template as a base for sentiment classification prompt = &#39;&#39;&#39;Predict whether the following document is a positive or negative movie review: [DOCUMENT] If it is positive return 1 and if it is negative return 0. Do not give any other answers. &#39;&#39;&#39; # predict the target for a single document using GPT document = &#39;unpretentious , charming , quirky , original&#39; chatgpt_generation(prompt, document) from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) from tqdm import tqdm # generate predictions for all documents in the test set predictions = [ chatgpt_generation(prompt, doc) for doc in tqdm(data[&#39;test&#39;][&#39;text&#39;]) ] # convert the string predictions (&#39;0&#39; or &#39;1&#39;) to integers y_pred = [int(pred) for pred in predictions] from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) # evaluate the performance of ChatGPT on the test set evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) 5. Text Clustering and Topic Modeling Although supervised techniques, such as classification, have reigned supreme over the last few years in the industry, the potential of unsupervised techniques such as text clustering cannot be understated. Text clustering aims to group similar texts based on their semantic content, meaning, and relationships. Figure 46. Clustering unstructured textual data. Text clustering is also applied in topic modeling to uncover abstract topics within large textual datasets. Figure 47. Topic modeling is a way to give meaning to clusters of textual documents. 5.1. ArXiv’s Articles: Computation and Language ArXiv is an open-access platform for scholarly articles, mostly in the fields of computer science, mathematics, and physics. from datasets import load_dataset # load the &#39;arxiv_nlp&#39; dataset from Hugging Face Datasets library dataset = load_dataset(&quot;maartengr/arxiv_nlp&quot;)[&quot;train&quot;] # extract metadata abstracts = dataset[&quot;Abstracts&quot;] titles = dataset[&quot;Titles&quot;] 5.2. A Common Pipeline for Text Clustering Text clustering enables the discovery of both known and unknown data patterns, providing an intuitive understanding of tasks like classification and their complexity, making it valuable beyond just exploratory data analysis. Although there are many methods for text clustering, from graph-based neural networks to centroid-based clustering techniques, a common pipeline that has gained popularity involves three steps and algorithms: Convert the input documents to embeddings with an embedding model. Figure 48. Step 1: We convert documents to embeddings using an embedding model. Reduce the dimensionality of embeddings with a dimensionality reduction model. Figure 49. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction. Find groups of semantically similar documents with a cluster model. Figure 50. Step 3: We cluster the documents using the embeddings with reduced dimensionality. 5.2.1. Embedding Documents from sentence_transformers import SentenceTransformer # create an embedding model using a pre-trained Sentence Transformer model embedding_model = SentenceTransformer(&#39;thenlper/gte-small&#39;) (1) # generate embeddings for each abstract in the &#39;abstracts&#39; list embeddings = embedding_model.encode(abstracts, show_progress_bar=True) # check the dimensions (shape) of the resulting embeddings embeddings.shape # (44949, 384) (2) 1 The thenlper/gte-small model is a more recent model that outperforms the previous model on clustering tasks and due to its small size is even faster for inference. 2 The embeddings.shape of (44949, 384) shows that there are 44,949 abstract embeddings, each with a dimensionality of 384. 5.2.2. Reducing the Dimensionality of Embeddings Reducing the dimensionality of embeddings is essential before clustering high-dimensional data to simplify the representation and enhance clustering effectiveness. Dimensionality reduction is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions. Figure 51. Dimensionality reduction allows data in high-dimensional space to be compressed to a lower-dimensional representation. Well-known methods for dimensionality reduction are Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP). from umap import UMAP # reduce the input embeddings from 384 dimensions to 5 dimensions using UMAP umap_model = UMAP( # generally, values between 5 and 10 work well to capture high-dimensional global structures. n_components=5, # the number of dimensions to reduce to min_dist=0.0, # the effective minimum distance between embedded points metric=&#39;cosine&#39;, # the metric to use to compute distances in high dimensional space random_state=42, # for reproducibility of the embedding ) # fit and then transform the embeddings to the lower-dimensional space reduced_embeddings = umap_model.fit_transform(embeddings) 5.2.3. Cluster the Reduced Embeddings While k-means, a centroid-based algorithm needing a predefined number of clusters, is common, density-based algorithms are preferable when the number of clusters is unknown as they automatically determine the clusters and don&#8217;t require all data points to belong to one. Figure 52. The clustering algorithm not only impacts how clusters are generated but also how they are viewed. A common density-based model is Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). from hdbscan import HDBSCAN # initialize and fit the HDBSCAN clustering model hdbscan_model = HDBSCAN( # the minimum number of samples in a group for it to be considered a cluster min_cluster_size=50, # the metric to use when calculating pairwise distances between data points metric=&#39;euclidean&#39;, # the method used to select clusters from the hierarchy (&#39;eom&#39; stands for Excess of Mass) cluster_selection_method=&#39;eom&#39; ).fit(reduced_embeddings) # fit the HDBSCAN model to the reduced dimensionality embeddings # extract the cluster labels assigned to each data point (-1 indicates noise) clusters = hdbscan_model.labels_ # How many clusters did we generate? (excluding the noise cluster labeled -1) num_clusters = len(set(clusters)) - (1 if -1 in clusters else 0) 5.2.4. Inspecting the Clusters To inspect each cluster manually and explore the assigned documents to get an understanding of its content. import numpy as np # print first three documents in cluster 0 cluster = 0 for index in np.where(clusters == cluster)[0][:3]: print(abstracts[index][:300] + &quot;... \n&quot;) To visualize clustering approximation results without manual review, further reduce document embeddings to two dimensions for plotting on an 2D plane. import pandas as pd from umap import UMAP import matplotlib.pyplot as plt # reduce 384-dimensional embeddings to two dimensions for easier visualization reduced_embeddings = UMAP( n_components=2, min_dist=0.0, metric=&quot;cosine&quot;, random_state=42, ).fit_transform(embeddings) # create dataframe df = pd.DataFrame(reduced_embeddings, columns=[&quot;x&quot;, &quot;y&quot;]) df[&quot;title&quot;] = titles df[&quot;cluster&quot;] = [str(c) for c in clusters] # select outliers (cluster -1) and non-outliers (clusters) to_plot = df.loc[df.cluster != &quot;-1&quot;, :] outliers = df.loc[df.cluster == &quot;-1&quot;, :] # plot outliers and non-outliers separately plt.scatter(outliers.x, outliers.y, alpha=0.05, s=2, c=&quot;grey&quot;, label=&quot;Outliers&quot;) plt.scatter( to_plot.x, to_plot.y, c=to_plot.cluster.astype(int), alpha=0.6, s=2, cmap=&quot;tab20b&quot;, label=&quot;Clusters&quot;, ) plt.axis(&quot;off&quot;) plt.legend() # Add a legend to distinguish outliers and clusters plt.title(&quot;Visualization of Clustered Abstracts&quot;) # Add a title for context plt.show() Figure 53. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization. 5.3. From Text Clustering to Topic Modeling Text clustering is a powerful tool for finding structure among large collections of documents, whereas topic modeling is the process of discovering underlying themes or latent topics within a collection of textual data, which typically involves finding a set of keywords or phrases that best represent and capture the meaning of the topic. Figure 54. Traditionally, topics are represented by a number of keywords but can take other forms. Instead of labeling a topic as “sign language,” these techniques use keywords such as “sign,” “language,” and “translation” to describe the topic. As such, this does not give a single label to a topic and instead requires the user to understand the meaning of the topic through those keywords. 5.3.1. BERTopic: A Modular Topic Modeling Framework BERTopic is a topic modeling technique that leverages clusters of semantically similar texts to extract various types of topic representations. Figure 55. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation. First, similar to text clustering, it embeds documents, reduces their dimensionality, and then clusters these embeddings to group semantically similar texts. .The first part of BERTopic’s pipeline is to create clusters of semantically similar documents. Second, it models word distributions using a bag-of-words approach, counting word frequencies within documents to help extract the most frequent terms. The bag-of-words approach does exactly what its name implies: it counts the number of times each word appears in a document, which can then be used to extract the most frequent words within that document. Figure 56. A bag-of-words counts the number of times each word appears inside a document. Figure 57. Generating c-TF by counting the frequency of words per cluster instead of per document. 6. Prompt Engineering Prompt engineering is the art and science of crafting effective prompts to guide large language models (LLMs) and other generative AI systems to produce desired and high-quality outputs. It involves understanding how these models interpret and respond to different phrasings, instructions, and contexts within a prompt to achieve specific goals, such as generating creative text, answering questions accurately, or performing tasks effectively. 6.1. Using Text Generation Models import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer model_path = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( model_path, device_map=dev, torch_dtype=&#39;auto&#39;, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_path) # create a pipeline pipe = pipeline( &#39;text-generation&#39;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False, ) # prompt messages = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Create a funny joke about chickens.&#39;}] # generate the output output = pipe(messages) print(output[0][&#39;generated_text&#39;]) 6.1.1. Prompt Template Under the hood, transformers.pipeline first converts the messages into a specific prompt template which was used during the training of the model. # apply prompt template prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False) print(prompt) &lt;s&gt;&lt;|user|&gt; Create a funny joke about chickens.&lt;|end|&gt; &lt;|assistant|&gt; Figure 58. The template Phi-3 expects when interacting with the model. 6.1.2. Controlling Model Output Each time an LLM needs to generate a token, it assigns a likelihood number to each possible token to generate different responses for the exact same prompt. Figure 59. The model chooses the next token to generate based on their likelihood scores. The temperature controls the randomness or creativity of the text generated; a higher temperature increases creativity by making less probable tokens more likely, while a temperature of 0 results in deterministic output by always selecting the most probable token. # using a high temperature output = pipe(messages, do_sample=True, temperature=1) print(output[0][&quot;generated_text&quot;]) Figure 60. A higher temperature increases the likelihood that less probable tokens are generated and vice versa. The top-p, or nucleus sampling, is a technique that controls the subset of tokens (the nucleus) an LLM considers for generation by including tokens until their cumulative probability reaches a specified threshold. For instance, if top_p is set to 0.1, the model will consider tokens until their cumulative probability reaches 10%, and if top_p is set to 1, all tokens will be considered. # using a high top_p output = pipe(messages, do_sample=True, top_p=1) print(output[0][&quot;generated_text&quot;]) Figure 61. A higher top_p increases the number of tokens that can be selected to generate and vice versa. The top_k parameter directly limits the number of most probable tokens an LLM considers; setting it to 100 restricts the selection to only the top 100 tokens. Table 1. Use case examples when selecting values for temperature and top_p. Example use case temperature top_p Description Brainstorming session High High High randomness with large pool of potential tokens. The results will be highly diverse, often leading to very creative and unexpected results. Email generation Low Low Deterministic output with high probable predicted tokens. This results in predictable, focused, and conservative outputs. Creative writing High Low High randomness with a small pool of potential tokens. This combination produces creative outputs but still remains coherent. Translation Low High Deterministic output with high probable predicted tokens. Produces coherent output with a wider range of vocabulary, leading to outputs with linguistic variety. 6.2. Prompt Engineering Prompt engineering is the iterative process of designing effective prompts, including questions, statements, or instructions, to elicit useful and relevant outputs from LLMs through experimentation and optimization. A prompt is the input provided to a large language model to elicit a desired response, which generally consists of multiple components such as instructions, data, and output indicators, and can be as complex as needed. Figure 62. A basic example of a prompt. No instruction is given so the LLM will simply try to complete the sentence. Figure 63. Two components of a basic instruction prompt: the instruction itself and the data it refers to. Figure 64. Extending the prompt with an output indicator that allows for a specific output. 6.3. Instruction-Based Prompting Instruction-based prompting is a method of prompting where the primary goal is to have the LLM answer a specific question or resolve a certain task by providing it with specific instructions. Figure 65. Prompt examples of common use cases. Notice how within a use case, the structure and location of the instruction can be changed. Each of these tasks requires different prompting formats and more specifically, asking different questions of the LLM. A non-exhaustive list of the prompting techniques includes: Specificity Accurately describe the desired output, for example, instead of &quot;Write a product description,&quot; ask &quot;Write a product description in under two sentences using a formal tone.&quot; Specificity is arguably the most important aspect; by restricting and specifying what the model should generate, there is a smaller chance of it generating something unrelated to a use case. Hallucination LLMs may generate incorrect information confidently, which is referred to as hallucination. To reduce its impact, ask the LLM to only generate an answer if it knows the answer, and to respond with &quot;I don’t know&quot; if it does not know the answer. Order Either begin or end the prompt with the instruction. Especially with long prompts, information in the middle is often forgotten. LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect). 6.4. Advanced Prompt Engineering While creating a good prompt might initially seem straightforward—just ask a specific question, be accurate, and add examples—prompting can quickly become complex and is often an underestimated aspect of effectively using LLMs. 6.4.1. Prompt Components A prompt generally consists of multiple components, such as instruction, data, and output indicators, and other advanced components that can quickly make a prompt quite complex. Figure 66. An example of a complex prompt with many components. Figure 67. Iterating over modular components is a vital part of prompt engineering. # prompt components persona = &#39;You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.\n&#39; instruction = &#39;Summarize the key findings of the paper provided.\n&#39; context = &#39;Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.\n&#39; data_format = &#39;Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.\n&#39; audience = &#39;The summary is designed for busy researchers that quickly need to grasp the newest trends in Large Language Models.\n&#39; tone = &#39;The tone should be professional and clear.\n&#39; text = &#39;MY TEXT TO SUMMARIZE&#39; data = f&#39;Text to summarize: {text}&#39; # the full prompt - remove and add pieces to view its impact on the generated output query = persona + instruction + context + data_format + audience + tone + data 6.4.2. In-Context Learning: Providing Examples In-context learning (ICL) is a prompting technique that demonstrates the desired task to an LLM through direct examples, rather than solely describing it to provide the model with context to learn from within the prompt. Zero-shot prompting does not leverage examples, one-shot prompts use a single example, and few-shot prompts use two or more examples. Figure 68. An example of a complex prompt with many components. # use a single example of using the made-up word in a sentence one_shot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;A \&#39;Gigamuru\&#39; is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:&#39;, }, { &#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.&#39;, }, { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;To \&#39;screeg\&#39; something is to swing a sword at it. An example of a sentence that uses the word screeg is:&#39;, }, ] print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False)) &lt;|user|&gt;A &#39;Gigamuru&#39; is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:&lt;|end|&gt;&lt;|assistant|&gt;I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.&lt;|end|&gt;&lt;|user|&gt;To &#39;screeg&#39; something is to swing a sword at it. An example of a sentence that uses the word screeg is:&lt;|end|&gt;&lt;|endoftext|&gt; # generate the output outputs = pipe(one_shot_prompt) print(outputs[0][&quot;generated_text&quot;]) In the medieval fantasy novel, the knight would screeg his enemies with his gleaming sword. 6.4.3. Chain Prompting: Breaking up the Problem Prompt chaining is a technique that addresses complex tasks by breaking them down across multiple prompts, where the output of one prompt serves as the input for the subsequent prompt, creating a sequence of interactions that collectively solve the problem. Figure 69. Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch. # create name and slogan for a product product_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a name and slogan for a chatbot that leverages LLMs.&quot;, } ] outputs = pipe(product_prompt) product_description = outputs[0][&quot;generated_text&quot;] print(product_description) # based on a name and slogan for a product, generate a sales pitch sales_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Generate a very short sales pitch for the following product: &#39;{product_description}&#39;&quot;, } ] outputs = pipe(sales_prompt) sales_pitch = outputs[0][&quot;generated_text&quot;] print(sales_pitch) Name: LexiBot Slogan: &quot;Unlock the Power of Language with LexiBot – Your AI Conversation Partner!&quot; Discover the future of communication with LexiBot – your AI conversation partner. Say goodbye to language barriers and hello to seamless, intelligent interactions. LexiBot is here to unlock the power of language, making every conversation more engaging and productive. Embrace the power of AI with LexiBot today! 6.5. Reasoning with Generative Models Reasoning is a core component of human intelligence and is often compared to the emergent behavior of LLMs that often resembles reasoning (through memorization of training data and pattern matching, rather than true reasoning). Human reasoning can be broadly categorized into two systems. System 1 thinking represents an automatic, intuitive, and near-instantaneous process, which shares similarities with generative models that automatically generate tokens without any self-reflective behavior. System 2 thinking, in contrast, is a conscious, slow, and logical process, akin to brainstorming and self-reflection. The system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking, would be emulated by giving a generative model the ability to mimic a form of self-reflection. 6.5.1. Chain-of-Thought: Think Before Answering Chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps (&quot;thoughts&quot;) before giving a final answer. Although chain-of-thought is a great method for enhancing the output of a generative model, it does require one or more examples of reasoning in the prompt, which the user might not have access to. Figure 70. Chain-of-thought prompting uses reasoning examples to persuade the generative model to use reasoning in its answer. # answering with chain-of-thought cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?&quot;, }, { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.&quot;, }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?&quot;, }, ] # generate the output outputs = pipe(cot_prompt) print(outputs[0][&quot;generated_text&quot;]) The cafeteria started with 23 apples. They used 20, so they had 23 - 20 = 3 apples left. Then they bought 6 more, so they now have 3 + 6 = 9 apples. The answer is 9. Instead of providing examples, zero-shot chain-of-thought allows a generative model to provide reasoning without explicit examples by directly prompting it for its thought process. Although the prompt “Let’s think step by step” can improve the output, you are not constrained by this exact formulation. Alterna‐ tives exist like “Take a deep breath and think step-by-step” and “Let’s work through this problem step-by-step.” Figure 71. Chain-of-thought prompting without using examples. Instead, it uses the phrase “Let’s think step-by-step” to prime reasoning in its answer. # zero-shot chain-of-thought prompt zeroshot_cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let&#39;s think step-by-step.&quot;, } ] # generate the output outputs = pipe(zeroshot_cot_prompt) print(outputs[0][&quot;generated_text&quot;]) Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 new apples to the 3 apples they have left: 3 apples + 6 apples = 9 apples. So, the cafeteria now has 9 apples. 6.5.2. Self-Consistency: Sampling Outputs Self-consistency is a technique that reduces randomness in generative models by prompting them multiple times with the same input, using varied sampling parameters like temperature and top_p to enhance diversity, and selecting the majority result as the final answer for robustness. Figure 72. By sampling from multiple reasoning paths, we can use majority voting to extract the most likely answer. # zero-shot chain-of-thought prompt zeroshot_cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let&#39;s think step-by-step.&quot;, } ] # self-consistency settings num_samples = 3 temperature = [0.3, 0.5, 0.7] top_p = [0.8, 0.85, 0.9] # extract final numerical answers def extract_answer(text): numbers = re.findall(r&quot;\d+&quot;, text) # find all numbers in the output return ( numbers[-1] if numbers else None ) # take the last number as the final answer # generate multiple answers answers = [] for i in range(num_samples): outputs = pipe( zeroshot_cot_prompt, do_sample=True, temperature=temperature[i % len(temperature)], top_p=top_p[i % len(top_p)], ) response = outputs[0][&quot;generated_text&quot;].strip() print(f&#39;\n{response}&#39; final_answer = extract_answer(response) if final_answer: answers.append(final_answer) # perform majority voting on numerical answers most_common_answer, count = Counter(answers).most_common(1)[0] print(&quot;\ngenerated answers:&quot;) for i, ans in enumerate(answers, 1): print(f&quot;{i}. {ans}&quot;) print(f&quot;\nfinal answer (majority vote): {most_common_answer}&quot;) Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 apples to the 3 apples they have left gives: 3 apples + 6 apples = 9 apples. So, the cafeteria Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 new apples to the 3 apples they have left, they now have: 3 apples + 6 apples = 9 apples. Sure, let&#39;s break it down step by step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. - 23 apples - 20 apples = 3 apples remaining. 3. They then buy 6 more apples. - 3 apples + 6 apples = 9 apples. So, after these transactions, the cafeteria has 9 apples. generated answers: 1. 9 2. 9 3. 9 final answer (majority vote): 9 6.5.3. Tree-of-Thought: Exploring Intermediate Steps Tree-of-Thought (ToT) is a problem-solving technique structuring reasoning as a decision tree that explores multiple potential solutions at each step, evaluates them, and branches forward with the most promising, similar to brainstorming, to enhance the final outcome. Figure 73. By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned. Tree-of-Thought excels at tasks requiring exploration of multiple paths, such as creative writing, but its reliance on numerous generative model calls can be slow. A more efficient approach involves prompting the model to simulate a multi-expert discussion to reach a consensus, mimicking the ToT framework with a single call. # zero-shot tree-of-thought prompt zeroshot_tot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &quot;Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they&#39;re wrong at any point then they leave. The question is &#39;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?&#39; Make sure to discuss the results.&quot;, } ] # generate the output outputs = pipe(zeroshot_tot_prompt) print(outputs[0][&#39;generated_text&#39;]) **Expert 1:** Step 1: Start with the initial number of apples, which is 23. **Expert 2:** Step 1: Subtract the apples used for lunch, which is 20, from the initial 23 apples. This leaves 3 apples. **Expert 3:** Step 1: Add the 6 apples that were bought to the remaining 3 apples. This results in 9 apples. **Discussion:** All three experts agree on the final result. The cafeteria started with 23 apples, used 20 for lunch, leaving them with 3 apples. Then, they bought 6 more apples, bringing the total to 9 apples. Therefore, the cafeteria now has 9 apples. 6.6. Output Verification Systems and applications built with generative models might eventually end up in production. When that happens, it is important to verify and control the output of the model to prevent breaking the application and to create a robust generative AI application. By default, most generative models create free-form text without adhering to specific structures other than those defined by natural language. Some use cases require their output to be structured in certain formats, like JSON. Even allowing the model to generate structured output, it still has the capability to freely generate its content. For instance, when a model is asked to output either one of two choices, it should not come up with a third. Some open source generative models have no guardrails and will generate outputs that do not consider safety or ethical considerations. For instance, use cases might require the output to be free of profanity, personally identifiable information (PII), bias, cultural stereotypes, etc. Many use cases require the output to adhere to certain standards or performance. The aim is to double-check whether the generated information is factually accurate, coherent, or free from hallucination. Generally, there are three ways of controlling the output of a generative model: Examples: Provide a number of examples of the expected output. Grammar: Control the token selection process. Fine-tuning: Tune a model on data that contains the expected output. 6.6.1. Providing Examples A simple and straightforward method to fix the output is to provide the generative model with examples of what the output should look like. The few-shot learning is a helpful technique that guides the output of the generative model, which can be generalized to guide the structure of the output as well. An important note here is that it is still up to the model whether it will adhere to your suggested format or not. Some models are better than others at following instructions. # zero-shot learning: providing no in-context examples zeroshot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Create a character profile for an RPG game in JSON format.&#39;, } ] # generate the output outputs = pipe(zeroshot_prompt) print(outputs[0][&#39;generated_text&#39;]) # one-shot learning: providing a single in-context example of the desired output structure one_shot_template = &#39;&#39;&#39;Create a short character profile for an RPG game. Make sure to only use this format: { &quot;description&quot;: &quot;A SHORT DESCRIPTION&quot;, &quot;name&quot;: &quot;THE CHARACTER&#39;S NAME&quot;, &quot;armor&quot;: &quot;ONE PIECE OF ARMOR&quot;, &quot;weapon&quot;: &quot;ONE OR MORE WEAPONS&quot; } &#39;&#39;&#39; one_shot_prompt = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: one_shot_template}] # generate the output outputs = pipe(one_shot_prompt) print(outputs[0][&#39;generated_text&#39;]) { &quot;name&quot;: &quot;Eldrin Shadowbane&quot;, &quot;class&quot;: &quot;Rogue&quot;, &quot;level&quot;: 10, &quot;race&quot;: &quot;Elf&quot;, &quot;background&quot;: &quot;Eldrin was born into a noble family in the elven city of Luminara. He was trained in the arts of stealth and combat from a young age. However, Eldrin always felt a deep connection to the shadows and the mysteries of the night. He left his family to become a rogue { &quot;description&quot;: &quot;A skilled archer with a mysterious past, known for their agility and precision.&quot;, &quot;name&quot;: &quot;Lyra Swiftarrow&quot;, &quot;armor&quot;: &quot;Leather bracers and a lightweight leather tunic&quot;, &quot;weapon&quot;: &quot;Longbow, throwing knives&quot; } 6.6.2. Grammar: Constrained Sampling Few-shot learning has a significant disadvantage: explicitly preventing certain output is not possible. Although the model is guided and given instructions, it might still not follow them completely. Grammar-constrained sampling is a technique used during the token generation process of a Large Language Model (LLM) that enforces adherence to predefined grammars or rules when selecting the next token. Instead, packages have been rapidly developed to constrain and validate the output of generative models, like Guidance, Guardrails, and LMQL, which leverage generative models to validate their own output. Figure 74. The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails. Figure 75. Use an LLM to generate only the pieces of information we do not know beforehand. Figure 76. Constrain the token selection to only three possible tokens: “positive,” “neutral,” and “negative.” Like transformers, llama-cpp-python is a library, generally used to efficiently load and use compressed models (quantization) in the GGUF format but can also be used to apply a JSON grammar. from llama_cpp.llama import Llama # load the Phi-3 language model using the llama-cpp-python library llm = Llama.from_pretrained( repo_id=&quot;microsoft/Phi-3-mini-4k-instruct-gguf&quot;, filename=&quot;*fp16.gguf&quot;, n_gpu_layers=-1, n_ctx=2048, verbose=False, ) # generate output using the loaded language model for a chat completion task output = llm.create_chat_completion( messages=[ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a warrior for an RPG in JSON for mat.&quot;, }, ], response_format={&quot;type&quot;: &quot;json_object&quot;}, # specify the response_format as a JSON temperature=0, )[&#39;choices&#39;][0][&#39;message&#39;][&quot;content&quot;] import json # check whether the output actually is JSON json_output = json.dumps(json.loads(output), indent=4) print(json_output) { &quot;warrior&quot;: { &quot;name&quot;: &quot;Aldarion the Brave&quot;, &quot;class&quot;: &quot;Warrior&quot;, &quot;level&quot;: 10, &quot;attributes&quot;: { &quot;strength&quot;: 18, &quot;dexterity&quot;: 10, &quot;constitution&quot;: 16, &quot;intelligence&quot;: 8, &quot;wisdom&quot;: 10, &quot;charisma&quot;: 12 }, 7. Advanced Text Generation Techniques and Tools LangChain is a framework for developing applications powered by large language models (LLMs), which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. Figure 77. LangChain is a complete framework for using LLMs. It has modular compo‐ nents that can be chained together to allow for complex LLM systems. Hugging Face models can be run locally through the HuggingFacePipeline class. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer model_id = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_id) # create a pipeline pipe = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=True, ) from langchain_huggingface.llms import HuggingFacePipeline llm = HuggingFacePipeline(pipeline=pipe) 7.1. Model I/O: Loading Quantized Models with LangChain A GGUF model represents a compressed version of its original counterpart through a method called quantization, which reduces the number of bits needed to represent the parameters of an LLM. Figure 78. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the lowered accuracy when we halve the number of bits. Bits, a series of 0s and 1s, represent values through binary encoding; more bits allow for a wider range of values but demand greater memory for storage. Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information. Quantization comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original. Like rounding the time to the nearest minute (&quot;14:16&quot;) instead of including seconds (&quot;14:16 and 12 seconds&quot;), quantization reduces the precision of a value without losing essential information. As a rule of thumb, look for at least 4-bit quantized models. These models have a good balance between compression and accuracy. Although it is possible to use 3-bit or even 2-bit quantized mod‐ els, the performance degradation becomes noticeable and it would instead be preferable to choose a smaller model with a higher precision. To download a specific bit-variant file (e.g., fp16) of the microsoft/Phi-3-mini-4k-instruct-gguf model, which includes multiple files with different bit-variants (see the &#39;Files and versions&#39; tab). # download from the primary Hugging Face URL: wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf # alternatively, download from the HF mirror: wget https://hf-mirror.com/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf Use Llama.cpp together with LangChain to load the GGUF file, and generate output. # !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf # !pip install llama-cpp-python langchain_communit from langchain_community.llms import LlamaCpp # initialize the LlamaCpp language model integration from Langchain llm = LlamaCpp( # path to the downloaded GGUF model file (ensure this file exists!) model_path=&quot;Phi-3-mini-4k-instruct-fp16.gguf&quot;, n_gpu_layers=-1, max_tokens=500, n_ctx=2048, seed=42, verbose=False, ) # invoke the language model with a prompt. output = llm.invoke(&quot;Hi! My name is Maarten. What is 1 + 1?&quot;) # no/meanless output! Phi-3 requires a specific prompt template. print(output) 7.2. Chains: Extending the Capabilities of LLMs In Langchain, a &quot;chain&quot; is a core concept that goes beyond running LLMs in isolation, which involves connecting an LLM with other components like prompts, tools, or even other chains, to enhance its capabilities and create more complex systems. Figure 79. A single chain connects some modular component, like a prompt template or external memory, to the LLM. 7.2.1. A Single Link in the Chain: Prompt Template Figure 80. By chaining a prompt template with an LLM, we only need to define the input prompts. The template will be constructed for you. By chaining a prompt template with an LLM to get the output, only the user and system prompts need to be defined for each interaction, eliminating the need to repeatedly define the full prompt template. Figure 81. An example of a single chain using Phi-3’s template. The template for Phi-3 is comprised of four main components: &lt;s&gt; to indicate when the prompt starts &lt;|user|&gt; to indicate the start of the user’s prompt &lt;|assistant|&gt; to indicate the start of the model’s output &lt;|end|&gt; to indicate the end of either the prompt or the model’s output Figure 82. The prompt template Phi-3 expects. from langchain_core.prompts import PromptTemplate # create a prompt template with a placeholder for the user&#39;s input template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt; {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate( template=template, input_variables=[&quot;input_prompt&quot;], ) # create a simple chain with the prompt template and the language model basic_chain = prompt | llm # invoke the chain with the input for the prompt template output = basic_chain.invoke( { &quot;input_prompt&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;, } ) # the &#39;output&#39; variable now contains the generated text print(output) Hello Maarten! The answer to 1 + 1 is 2. 7.2.2. A Chain with Multiple Prompts Figure 83. With sequential chains, the output of a prompt is used as the input for the next prompt. A multiple prompt chain, or sequential chain, processes a complex task by dividing it into a series of smaller, sequential subtasks, where each subtask utilizes a distinct prompt and LLM call, with the output from one step feeding directly into the input of the subsequent step. Figure 84. An example to generate a story that has three components: a title, a description of the main character, a summary of the story. The output of the title prompt is used as the input of the character prompt. To generate the story, the output of all previous prompts is used. import json from langchain_core.prompts import PromptTemplate from langchain_core.runnables import RunnablePassthrough, RunnableLambda from langchain.schema import StrOutputParser from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&#39;qwen2.5:0.5b-instruct&#39;, temperature=0.7, max_tokens=100, timeout=30, max_retries=2, base_url=&#39;http://localhost:11434/v1&#39;, # Ollama API api_key=&#39;API-KEY&#39;, verbose=True, ) title_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Create a title for a story about {summary}.&quot; &quot;Only return the title.&quot; &quot;&lt;|end|&gt; &lt;|assistant|&gt;&quot; ) character_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Describe the main character of a story about {summary} with the title {title}. &quot; &quot;Use only two sentences.&quot; &quot;&lt;|end|&gt;&lt;|assistant|&gt;&quot; ) story_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Create a story about {summary} with the title {title}.&quot; &quot;The main character is: {character}. &quot; &quot;Only return the story and it cannot be longer than one paragraph.&quot; &quot;&lt;|end|&gt;&lt;|assistant|&gt;&quot; ) # LCEL-style chain using Runnables title_chain = ( {&quot;summary&quot;: RunnablePassthrough()} | title_prompt | llm | StrOutputParser() ) character_chain = ( {&quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain} | character_prompt | llm | StrOutputParser() ) story_chain = ( { &quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain, &quot;character&quot;: character_chain, } | story_prompt | llm | StrOutputParser() ) aggregate_chain = RunnableLambda( lambda inputs: { &quot;summary&quot;: inputs[&quot;summary&quot;], &quot;title&quot;: inputs[&quot;title&quot;], &quot;character&quot;: inputs[&quot;character&quot;], &quot;story&quot;: inputs[&quot;story&quot;], } ) final_chain = { &quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain, &quot;character&quot;: character_chain, &quot;story&quot;: story_chain, } | aggregate_chain output = final_chain.invoke({&quot;summary&quot;: &quot;a girl that lost her mother&quot;}) print(json.dumps(output, indent=2)) { &quot;summary&quot;: { &quot;summary&quot;: &quot;a girl that lost her mother&quot; }, &quot;title&quot;: &quot;\&quot;Lost Mother Girl\&quot;&quot;, &quot;character&quot;: &quot;In the story, the main character named Lily, who was born to an ordinary family, unexpectedly finds herself the daughter of a rich individual after losing her mother. She navigates this new reality with courage and strength, learning valuable lessons about empathy, perseverance, and the power of resilience.&quot;, &quot;story&quot;: &quot;In the quiet village where Linxue lived, her mother had been gone for many years. As an only child, she often felt distant from the other children in the village. One day, 7.3. Memory: Helping LLMs to Remember Conversations Memory can be added to the LLM chain using methods like conversation buffers and conversation summaries to make chat models stateful to remember previous conversations. 7.3.1. Conversation Buffer In Langchain, ConversationBufferMemory provides an intuitive way to give LLMs memory by updating the prompt to include the full chat history. Figure 85. We can remind an LLM of what previously happened by simply appending the entire conversation history to the input prompt. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke({&quot;input&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;}) {&#39;input&#39;: &#39;Hi! My name is Maarten. What is 1 + 1?&#39;, &#39;chat_history&#39;: &#39;&#39;, &#39;text&#39;: &#39;Nice to meet you, Maarten!\n\nThe answer to 1 + 1 is... 2!&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) {&#39;input&#39;: &#39;What is my name?&#39;, &#39;chat_history&#39;: &#39;Human: Hi! My name is Maarten. What is 1 + 1?\nAI: Nice to meet you, Maarten!\n\nThe answer to 1 + 1 is... 2!&#39;, &#39;text&#39;: &#39;Nice to meet you too, Maarten! Your name is indeed Maarten. Would you like to ask another question or have a conversation?&#39;} 7.3.2. Windowed Conversation Buffer In LangChain, ConversationBufferWindowMemory decides how many the last k conversations are passed to the input prompt. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=2, memory_key=&quot;chat_history&quot;) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke( input=&quot;Hi! My name is Maarten and I am 33 years old. What is 1 + 1?&quot; ) llm_chain.invoke(input=&quot;What is 3 + 3?&quot;) llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) llm_chain.invoke({&quot;input&quot;: &quot;What is my age?&quot;}) 7.3.3. Conversation Summary In LangChain, ConversationSummaryMemory summarizes the entire conversation history (typically using an external LLM) before providing it to the input prompt. Figure 86. Instead of passing the conversation history directly to the prompt, we use another LLM to summarize it first. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationSummaryMemory # prepare a summarization template as the summarization prompt summary_prompt_template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Summarize the conversations and update with the new lines. Current summary: {summary} new lines of conversation: {new_lines} New summary:&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; summary_prompt = PromptTemplate.from_template(template=summary_prompt_template) memory = ConversationSummaryMemory( llm=llm, memory_key=&quot;chat_history&quot;, prompt=summary_prompt ) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke({&quot;input&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;}) {&#39;input&#39;: &#39;Hi! My name is Maarten. What is 1 + 1?&#39;, &#39;chat_history&#39;: &#39;&#39;, &#39;text&#39;: &#39;Hi Maarten!\n\nThe answer to 1 + 1 is 2.&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) {&#39;input&#39;: &#39;What is my name?&#39;, &#39;chat_history&#39;: &quot;Here is the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n\nNew lines of conversation:\nHuman: That&#39;s correct, what&#39;s 2 * 2?\nAI: Let me calculate... The answer to 2 * 2 is 4.&quot;, &#39;text&#39;: &#39;Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What was the first question I asked?&quot;}) {&#39;input&#39;: &#39;What was the first question I asked?&#39;, &#39;chat_history&#39;: &#39;Here\&#39;s the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n* Human: That\&#39;s correct, what\&#39;s 2 * 2?\n* AI: Let me calculate... The answer to 2 * 2 is 4.\n* Human: What is my name?\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?&#39;, &#39;text&#39;: &#39;The first question you asked was: &quot;what\&#39;s 1 + 1?&quot;&#39;} # check what the summary is thus far memory.load_memory_variables({}) {&#39;chat_history&#39;: &#39;Here is the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n* Human: That\&#39;s correct, what\&#39;s 2 * 2?\n* AI: Let me calculate... The answer to 2 * 2 is 4.\n* Human: What is my name?\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?\n* Human: What was the first question I asked?\n* AI: The first question you asked was: &quot;what\&#39;s 1 + 1?&quot;&#39;} 7.4. Agents: Creating a System of LLMs Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions. ReAct (Reasoning and Acting) is a cognitive framework for language models that interleaves reasoning (&quot;Thoughts&quot;) and acting (&quot;Actions&quot;) with observations, allowing the model to dynamically plan, execute, and learn from its interactions with external tools or environments to solve complex tasks. Figure 87. An example of a ReAct prompt template. Figure 88. An example of two cycles in a ReAct pipeline. from langchain_openai import ChatOpenAI # an LLM that is powerful enough to properly follow complex instructions llm = ChatOpenAI( model=&quot;mistral:7b-instruct&quot;, # &quot;llama3.1:8b&quot;, # &quot;llama3.2:1b&quot;, temperature=0.7, max_tokens=100, base_url=&quot;http://localhost:11434/v1&quot;, api_key=&quot;API-KEY&quot;, verbose=True, ) from langchain_core.prompts import PromptTemplate # create the ReAct template react_template = &quot;&quot;&quot;Answer the following questions as best you can. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Agents: Creating a System of LLMs Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: {input} Thought:{agent_scratchpad}&quot;&quot;&quot; prompt = PromptTemplate( template=react_template, input_variables=[&quot;tools&quot;, &quot;tool_names&quot;, &quot;input&quot;, &quot;agent_scratchpad&quot;], ) from langchain.agents import load_tools, Tool from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults search = DuckDuckGoSearchResults() search_tool = Tool( name=&quot;duckduck&quot;, description=&quot;A web search engine. Use this to as a search engine for general queries.&quot;, func=search.run, ) tools = load_tools([&quot;llm-math&quot;], llm=llm) tools.append(search_tool) from langchain.agents import AgentExecutor, create_react_agent agent = create_react_agent(llm, tools, prompt) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=5, ) agent_executor.invoke( { &quot;input&quot;: &quot;What is 123 + 456?&quot; } ) &gt; Entering new AgentExecutor chain... To solve this, I will use the Calculator tool. The input for the calculator will be the equation &quot;123 + 456&quot;. Action: Calculator Action Input: &quot;123 + 456&quot;Answer: 579 I now know the final answer. Final Answer: The result of the calculation (123 + 456) is 579. &gt; Finished chain. {&#39;input&#39;: &#39;What is 123 + 456?&#39;, &#39;output&#39;: &#39;The result of the calculation (123 + 456) is 579.&#39;} agent_executor.invoke( { &quot;input&quot;: &quot;What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.&quot; } ) &gt; Entering new AgentExecutor chain... I need to find the current price of a MacBook Pro and then convert that price from USD to EUR using the given exchange rate. Agents: Calculator, duckduck Action: duckduck Action Input: What is the current price of a MacBook Pro in USD?snippet: Apple resellers are hosting a variety of MacBook Pro sales that discount current M4, M4 Pro and M4 Max 14-inch and 16-inch models, in addition to blowout bargains on M3 models. Apple offers two ..., title: Best MacBook Pro Deals for March 2025 | Save up to $1,200 - AppleInsider, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: The newly launched M4 Pro and M4 Max 14-inch MacBook Pros have shown notable performance improvements over their M1, M2, and M3 counterparts, especially in single-core scores. In recent benchmarks, the M4 Pro 14-inch MacBook Pro achieved a single-core score of approximately 3,850, surpassing the M3 Pro&#39;s single-core score by about 15-20%., title: Apple 14″ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/14-macbook-pro/, snippet: Apple MacBook Pro 14&quot; (M4/512GB): was $1,599 now $1,399 at Amazon. The M4-based MacBook Pro M4 is pretty close to being the perfect laptop. You get fantastic performance from the M4 chip, useful ..., title: Epic Apple MacBook sale is live — shop the best deals from $629 right ..., link: https://www.tomsguide.com/sales-events/epic-apple-macbook-sale-is-live-shop-the-best-deals-from-usd629-right-now, snippet: The M4 Max MacBook Pro is Apple&#39;s most powerful option, and both the silver and space black options are on sale. ... List price Best price (current) Best price (all-time) M2 MacBook Air (13-inch ..., title: Best MacBook Deals: Save on Apple&#39;s Latest Laptops and Previous-Gen ..., link: https://www.cnet.com/deals/best-macbook-deals/ The current price of a MacBook Pro in USD can be found from the search results. Let me filter the results a bit more specifically to find the price. Agents: duckduck Action: duckduck Action Input: What is the price of a new 14-inch MacBook Pro (M4/512GB) in USD?snippet: - 14″ M4 MacBook Pro (16GB/1TB/Gray): $1599, $200 off MSRP - 14″ M4 MacBook Pro (24GB/1TB/Gray): $1799, $200 off MSRP. These are currently the lowest prices available for new M4-powered 14″ MacBook Pros among the Apple retailers we track. For the latest sales and prices, keep an eye on our 14-inch MacBook Pro Price Tracker, updated daily., title: 14-inch M4 MacBook Pros on sale today for $150-$200 off MSRP, link: https://www.macprices.net/2025/01/14/14-inch-m4-macbook-pros-on-sale-today-for-150-200-off-msrp/, snippet: Every M4 Pro and M4 Max model is also on sale at up to $300 off in our Mac Price Guide. Prices start at $1,699. Here are a few top picks from the MacBook Pro sale: 14-inch M4, 16GB, 512GB, Space ..., title: Apple M4 MacBook Pro Drops to $1,399, Free Next Day Shipping - AppleInsider, link: https://appleinsider.com/articles/24/12/25/snag-an-m4-macbook-pro-14-inch-for-1399-with-free-next-day-delivery, snippet: The M4 Pro MacBook Pro 14-inch has hit a new record low price of $1,699, with units in stock with free store pickup as early as today. But don&#39;t delay, as the deal ends on Christmas Eve., title: Apple MacBook Pro 14-inch M4 Pro Drops to Best $1,699 Price - AppleInsider, link: https://appleinsider.com/articles/24/12/24/apples-14-inch-macbook-pro-with-m4-pro-chip-plunges-to-record-low-1699-today-only, snippet: Right now the 14-inch MacBook Pro is available with a discount that slashes its price to the lowest yet, and you won&#39;t want to miss out. Amazon is now selling the M4 MacBook Pro for just $1,398 ..., title: Apple&#39;s Latest M4 14-inch MacBook Pro Is Now Yours for Its Best-Ever Price, link: https://www.cnet.com/deals/apples-latest-m4-14-inch-macbook-pro-is-now-yours-for-its-best-ever-price/ The current price of a new 14-inch MacBook Pro (M4/512GB) in USD is $1399. To find the cost in EUR, we can use the given exchange rate of 0.85 EUR for 1 USD. So, the cost of the MacBook Pro in EUR would be 1399 * 0.85 = €1176.21. Final Answer: The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR. &gt; Finished chain. {&#39;input&#39;: &#39;What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.&#39;, &#39;output&#39;: &#39;The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR.&#39;} Appendix A: LangChain LangChain is a framework that consists of a number of packages, which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. langchain-core is a lightweight package containing base abstractions and interfaces for core Langchain components like chat models, vector stores, and tools, without including any third-party integrations and with minimal dependencies. langchain is the main package containing generic chains and retrieval strategies that form an application&#8217;s cognitive architecture, independent of specific third-party integrations. Integrations are a list of lightweight packages (e.g., langchain-openai, langchain-anthropic) that contain specific integrations and are co-maintained for proper versioning. langchain-community is a package containing third-party integrations for various components (chat models, vector stores, tools, etc.), maintained by the Langchain community, with all dependencies being optional to ensure a lightweight package. langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. langserve is a package to deploy LangChain chains as REST APIs that makes it easy to get a production ready API up and running. LangSmith is a developer platform for debugging, testing, evaluating, and monitoring LLM applications. 7.A.1. Chat Models and Messages Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario. LangChain provides a consistent interface for working with chat models from different providers that takes a list of messages as input and returns a message as output while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs. LangChain supports two message formats to interact with chat models: LangChain Message Format: LangChain&#8217;s own message format, which is used by default and is used internally by LangChain. OpenAI&#8217;s Message Format: OpenAI&#8217;s message format. Messages are the unit of communication in chat models, which are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation. Each message has a role (e.g., &quot;user&quot;, &quot;assistant&quot;) and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider. LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider. LangChain messages are Python objects that subclass from a BaseMessage. SystemMessage: corresponds to system role HumanMessage: corresponds to user role AIMessage: corresponds to assistant role AIMessageChunk: corresponds to assistant role, used for streaming responses ToolMessage: corresponds to tool role When invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. model.invoke(&quot;Hello, how are you?&quot;) from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&quot;gpt-4o&quot;, temperature=0, max_tokens=100, timeout=30, max_retries=2, ) llm.invoke(&#39;What is LangChain?&#39;) 7.A.2. Prompt Templates Prompt Templates are responsible for formatting user input into a format that can be passed to a language model, take as input a dictionary, where each key represents a variable in the prompt template to fill in, and output a PromptValue. from langchain_core.prompts import PromptTemplate prompt_template = PromptTemplate.from_template(&quot;Tell me a joke about {topic}&quot;) prompt = prompt_template.format(**{&quot;topic&quot;: &quot;cats&quot;}) print(prompt) # Tell me a joke about cats from langchain_core.prompts import ChatPromptTemplate prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), (&quot;user&quot;, &quot;Tell me a joke about {topic}&quot;) ]) prompt = prompt_template.format(**{&quot;topic&quot;: &quot;cats&quot;}) print(prompt) # System: You are a helpful assistant # Human: Tell me a joke about cats from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.messages import HumanMessage prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), MessagesPlaceholder(&quot;msgs&quot;) ]) prompt = prompt_template.format(**{&quot;msgs&quot;: [HumanMessage(content=&quot;hi!&quot;)]}) print(prompt) # System: You are a helpful assistant # Human: hi! # alternatively prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), (&quot;placeholder&quot;, &quot;{msgs}&quot;) # &lt;-- This is the changed part ]) prompt = prompt_template.format(**{&quot;msgs&quot;: [HumanMessage(content=&quot;hi!&quot;)]}) print(prompt) # System: You are a helpful assistant # Human: hi! 7.A.3. Structured Outputs Structured outputs are a concept where language models are instructed to respond in a structured format, rather than in direct natural language, which is useful in scenarios where the output needs to be machine-readable, such as storing output in a database and ensure that the output conforms to the database schema. LangChain provides a method, with_structured_output(), that automates the process of binding the schema to the model and parsing the output. from pydantic import BaseModel, Field class ResponseFormatter(BaseModel): &quot;&quot;&quot;Always use this tool to structure your response to the user.&quot;&quot;&quot; answer: str = Field(description=&quot;The answer to the user&#39;s question&quot;) followup_question: str = Field(description=&quot;A followup question the user could ask&quot;) llm_with_structure = llm.with_structured_output(ResponseFormatter) structured_output = llm_with_structure.invoke( &quot;What is the powerhouse of the cell?&quot;, verbose=True ) structured_output ResponseFormatter(answer=&#39;The powerhouse of the cell is the mitochondria.&#39;, followup_question=&#39;What is the organelle that powers the cell?&#39;) While one approach is to include defined schema in the prompt and ask nicely for the model to use it, it is not recommended. from langchain.output_parsers.structured import ResponseSchema, StructuredOutputParser response_schemas = [ ResponseSchema( name=&quot;answer&quot;, description=&quot;The answer to the user&#39;s question&quot;, type=&quot;string&quot;, ), ResponseSchema( name=&quot;followup_question&quot;, description=&quot;A followup question the user could ask&quot;, type=&quot;string&quot;, ), ] parser = StructuredOutputParser.from_response_schemas(response_schemas) format_instructions = parser.get_format_instructions() from langchain.prompts import PromptTemplate prompt = PromptTemplate( template=&quot;{query}\n{format_instructions}\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: format_instructions}, ) print(prompt.format(**{&quot;query&quot;: &quot;What is the powerhouse of the cell?&quot;})) What is the powerhouse of the cell? The output should be a markdown code snippet formatted in the following schema, including the leading and trailing &quot;```json&quot; and &quot;```&quot;: ```json { &quot;answer&quot;: string // The answer to the user&#39;s question &quot;followup_question&quot;: string // A followup question the user could ask } ``` chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;What is the powerhouse of the cell?&quot;}) output {&#39;answer&#39;: &#39;The powerhouse of the cell is the nucleus.&#39;, &#39;followup_question&#39;: &#39;What does the nucleus play a crucial role in?&#39;} 7.A.4. Output Parsers Output Parsers are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks, which are useful when using LLMs to generate structured data, or to normalize output from chat models and LLMs. # parse text from message objects from langchain_core.output_parsers import StrOutputParser chain = llm | StrOutputParser() output = chain.invoke(&#39;What is 2 + 2 ?&#39;) print(output) # 2 + 2 equals 4. # use output parsers to parse an LLM response into structured format from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import PromptTemplate from pydantic import BaseModel, Field, model_validator class Joke(BaseModel): setup: str = Field(description=&quot;question to set up a joke&quot;) punchline: str = Field(description=&quot;answer to resolve the joke&quot;) parser = PydanticOutputParser(pydantic_object=Joke) prompt = PromptTemplate( template=&quot;Answer the user query.\n{format_instructions}\n{query}\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}, ) chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;Tell me a joke.&quot;}) print(output.model_dump_json(indent=2)) # { # &quot;setup&quot;: &quot;Why did the tomato turn red?&quot;, # &quot;punchline&quot;: &quot;Because it saw the salad dressing!&quot; # } # parse JSON output from langchain_core.output_parsers import JsonOutputParser from langchain_core.prompts import PromptTemplate from pydantic import BaseModel, Field class Joke(BaseModel): setup: str = Field(description=&quot;question to set up a joke&quot;) punchline: str = Field(description=&quot;answer to resolve the joke&quot;) parser = JsonOutputParser(pydantic_object=Joke) instructions = parser.get_format_instructions() print(f&#39;\n{instructions}\n---------------&#39;) prompt = PromptTemplate( template=&quot;Answer the user query.\n{format_instructions}\n{query}\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}, ) chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;Tell me a joke.&quot;}) print(output) # The output should be formatted as a JSON instance that conforms to the JSON schema below. # # As an example, for the schema {&quot;properties&quot;: {&quot;foo&quot;: {&quot;title&quot;: &quot;Foo&quot;, &quot;description&quot;: &quot;a list of strings&quot;, &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}}}, &quot;required&quot;: [&quot;foo&quot;]} # the object {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]} is a well-formatted instance of the schema. The object {&quot;properties&quot;: {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]}} is not well-formatted. # # Here is the output schema: # ``` # {&quot;properties&quot;: {&quot;setup&quot;: {&quot;description&quot;: &quot;question to set up a joke&quot;, &quot;title&quot;: &quot;Setup&quot;, &quot;type&quot;: &quot;string&quot;}, &quot;punchline&quot;: {&quot;description&quot;: &quot;answer to resolve the joke&quot;, &quot;title&quot;: &quot;Punchline&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;setup&quot;, &quot;punchline&quot;]} # ``` # --------------- # {&#39;setup&#39;: &#39;Why did the tomato turn red?&#39;, &#39;punchline&#39;: &#39;Because it saw the salad dressing!&#39;} 7.A.5. Embedding, Vector Stores, and Retrievers Embedding models are machine learning models that transform human language or multimodal data (text, audio, images, video - not currently fully supported by Langchain) into numerical vector representations (embeddings), which are fixed-length arrays capturing the semantic meaning of the input, enabling machines to understand and compare data based on conceptual similarity, not just keywords. (1) Embed text as a vector: Embeddings transform text into a numerical vector representation. (2) Measure similarity: Embedding vectors can be compared using simple mathematical operations. LangChain provides a universal interface for working with embedding models, providing standard methods for common operations, and simplifies interaction with various embedding providers through two central methods: embed_documents: For embedding multiple texts (documents) embed_query: For embedding a single text (query) # for embedding multiple texts (documents) from langchain_openai import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings() embeddings = embeddings_model.embed_documents( [ &quot;Hi there!&quot;, &quot;Oh, hello!&quot;, &quot;What&#39;s your name?&quot;, &quot;My friends call me World&quot;, &quot;Hello World!&quot; ] ) len(embeddings), len(embeddings[0]) (5, 1536) # for embedding a single text (query) query_embedding = embeddings_model.embed_query(&quot;What is the meaning of life?&quot;) # measure similarity import numpy as np def cosine_similarity(vec1, vec2): dot_product = np.dot(vec1, vec2) norm_vec1 = np.linalg.norm(vec1) norm_vec2 = np.linalg.norm(vec2) return dot_product / (norm_vec1 * norm_vec2) similarity = cosine_similarity(query_result, document_result) print(&quot;Cosine Similarity:&quot;, similarity) # hugging face embeddings from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) query_embedding = embeddings.embed_query(&quot;Hello, world!&quot;) print(len(query_embedding)) # 384 Vector stores are databases that can efficiently store and retrieve embeddings, which are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches. LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations. The key methods are: add_documents: Add a list of texts to the vector store. delete: Delete a list of documents from the vector store. similarity_search: Search for similar documents to a given query. from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) from langchain_core.vectorstores import InMemoryVectorStore # initialize with an embedding model vector_store = InMemoryVectorStore(embedding=embeddings) # add documents from langchain_core.documents import Document document_1 = Document( page_content=&quot;I had chocalate chip pancakes and scrambled eggs for breakfast this morning.&quot;, metadata={&quot;source&quot;: &quot;tweet&quot;}, ) document_2 = Document( page_content=&quot;The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.&quot;, metadata={&quot;source&quot;: &quot;news&quot;}, ) documents = [document_1, document_2] vector_store.add_documents(documents=documents) # [&#39;df0f6926-c824-4114-a2c5-2b19d9d8740c&#39;, &#39;fa105761-9dd6-4c1c-860a-28e3e4ba181a&#39;] # provide IDs for the documents to the vector store vector_store.add_documents(documents=documents, ids=[&quot;doc1&quot;, &quot;doc2&quot;]) # [&#39;doc1&#39;, &#39;doc2&#39;] # delete documents vector_store.delete(ids=[&quot;doc1&quot;]) # similarity search query = &quot;my query&quot; docs = vectorstore.similarity_search(query) print(docs[0].page_content) Retrievers in Langchain are components that provide a unified way to interact with various retrieval systems, including vector stores, graph databases, and relational databases, and take a natural language query as input to return a list of relevant documents. LangChain provides a uniform interface for interacting with different types of retrieval systems that accepts a query and return documents. A Langchain retriever is a runnable, which is a standard interface for Langchain components, and it has a few common methods, including invoke, that are used to interact with it. docs = retriever.invoke(query) Lost in the Middle is the phenomenon where Large Language Models (LLMs) have difficulty effectively using information located in the middle of a long input context, often performing better when relevant details are at the beginning or end. Documents retrieved from vector stores are typically returned in descending order of relevance, often measured by cosine similarity of embeddings. To mitigate the &quot;lost in the middle&quot; effect, re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. The LongContextReorder document transformer implements the re-ordering procedure. from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) from langchain_core.vectorstores import InMemoryVectorStore texts = [ &quot;Basquetball is a great sport.&quot;, &quot;Fly me to the moon is one of my favourite songs.&quot;, &quot;The Celtics are my favourite team.&quot;, &quot;This is a document about the Boston Celtics&quot;, &quot;I simply love going to the movies&quot;, &quot;The Boston Celtics won the game by 20 points&quot;, &quot;This is just a random text.&quot;, &quot;Elden Ring is one of the best games in the last 15 years.&quot;, &quot;L. Kornet is one of the best Celtics players.&quot;, &quot;Larry Bird was an iconic NBA player.&quot;, ] vector_store = InMemoryVectorStore.from_texts(texts, embedding=embeddings) from langchain_core.runnables import chain from langchain_core.documents import Document # create a retriever @chain def retriever(query: str) -&gt; list[Document]: docs, scores = zip(*vector_store.similarity_search_with_score(query, k=10)) for doc, score in zip(docs, scores): doc.metadata[&quot;score&quot;] = score return docs docs = retriever.invoke(query) max_score_length = max(len(f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;) for doc in docs) for doc in docs: score_str = f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;.rjust(max_score_length) print(f&quot;- {score_str}: {doc.page_content}&quot;) - 0.675469: This is a document about the Boston Celtics - 0.638917: The Celtics are my favourite team. - 0.552694: L. Kornet is one of the best Celtics players. - 0.460651: The Boston Celtics won the game by 20 points - 0.320224: Larry Bird was an iconic NBA player. - 0.244521: Elden Ring is one of the best games in the last 15 years. - 0.231564: Basquetball is a great sport. - 0.106447: I simply love going to the movies - 0.059917: Fly me to the moon is one of my favourite songs. - 0.034081: This is just a random text. from langchain_community.document_transformers import LongContextReorder # Reorder the documents: # Less relevant document will be at the middle of the list and more # relevant elements at beginning / end. reordering = LongContextReorder() reordered_docs = reordering.transform_documents(docs) # Confirm that the 4 relevant documents are at beginning and end. for doc in reordered_docs: score_str = f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;.rjust(max_score_length) print(f&quot;- {score_str}: {doc.page_content}&quot;) - 0.638917: The Celtics are my favourite team. - 0.460651: The Boston Celtics won the game by 20 points - 0.244521: Elden Ring is one of the best games in the last 15 years. - 0.106447: I simply love going to the movies - 0.034081: This is just a random text. - 0.059917: Fly me to the moon is one of my favourite songs. - 0.231564: Basquetball is a great sport. - 0.320224: Larry Bird was an iconic NBA player. - 0.552694: L. Kornet is one of the best Celtics players. - 0.675469: This is a document about the Boston Celtics 7.A.6. Document Loaders Document Loaders are responsible for loading documents from a variety of sources. # simple and fast text extraction from langchain_community.document_loaders import PyPDFLoader file_path = &quot;./books/llm-book.pdf&quot; loader = PyPDFLoader(file_path) pages = [] for page in loader.lazy_load(): pages.append(page) print(f&quot;{pages[0].metadata}\n&quot;) print(pages[0].page_content) {&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} Hands-On Large Language Models Language Understanding and Generation Jay Alammar &amp; Maarten Grootendorst # vector search over PDFs from langchain_core.vectorstores import InMemoryVectorStore from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) vector_store = InMemoryVectorStore.from_documents(pages, embeddings) docs = vector_store.similarity_search(&quot;What is Prompt Engineering?&quot;, k=2) for doc in docs: print(f&#39;Page {doc.metadata[&quot;page&quot;]}: {doc.page_content[:300]}\n&#39;) Page 194: Intro to Prompt Engineering An essential part of working with text-generative LLMs is prompt engineering. By carefully designing our prompts we can guide the LLM to generate desired responses. Whether the prompts are questions, statements, or instructions, the main goal of prompt engineering is to e Page 219: Summary In this chapter, we explored the basics of using generative models through prompt engineering and output verification. We focused on the creativity and potential com‐ plexity that comes with prompt engineering. These components of a prompt are key in generating and optimizing output appropri 7.A.7. Text Splitters Text splitters split documents into smaller, manageable chunks for use in downstream applications, particularly retrieval systems, to handle non-uniform document lengths, overcome model limitations, improve representation quality, enhance retrieval precision, and optimize computational resources. Text splitting approaches include length-based methods (token or character), text-structure based methods (like recursive splitting that respects paragraphs and sentences), document-structure based methods (leveraging formats like Markdown or HTML), and semantic meaning based methods (analyzing content for significant meaning shifts). from langchain_text_splitters import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=100, chunk_overlap=20, length_function=len, is_separator_regex=False, ) with open(&quot;state_of_the_union.txt&quot;) as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) print(texts[0]) print(texts[1]) Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. from langchain_community.document_loaders.text import TextLoader loader = TextLoader(&quot;state_of_the_union.txt&quot;) documents = loader.load() split_documents = text_splitter.split_documents(documents) print(split_documents[0]) print(split_documents[1]) page_content=&#39;Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and&#39; metadata={&#39;source&#39;: &#39;state_of_the_union.txt&#39;} page_content=&#39;of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.&#39; metadata={&#39;source&#39;: &#39;state_of_the_union.txt&#39;} from langchain_community.document_loaders import PyPDFLoader loader = PyPDFLoader(&quot;./books/llm-book.pdf&quot;) documents = loader.load() split_documents = text_splitter.split_documents(documents) print(split_documents[0]) print(split_documents[1]) page_content=&#39;Hands-On Large Language Models Language Understanding and Generation Jay Alammar &amp;&#39; metadata={&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} page_content=&#39;Jay Alammar &amp; Maarten Grootendorst&#39; metadata={&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} 7.A.8. Tools LangChain&#8217;s tool abstraction links a Python function to a schema defining its name, description, and expected arguments, which chat models that support tool calling (or function calling) can use to request the execution of a specific function with specific inputs A key principle of tool calling is that the model decides when to use a tool based on the input&#8217;s relevance. # tool creation @tool def multiply(a: int, b: int) -&gt; int: &quot;&quot;&quot;Multiply a and b.&quot;&quot;&quot; return a * b tools = [multiply] # tool binding llm_with_tools = llm.bind_tools(tools) # tool calling output = llm_with_tools.invoke(&quot;What is 2 multiplied by 3?&quot;) output.content, output.tool_calls (&#39;&#39;, [{&#39;name&#39;: &#39;multiply&#39;, &#39;args&#39;: {&#39;a&#39;: 2, &#39;b&#39;: 3}, &#39;id&#39;: &#39;call_zerallda&#39;, &#39;type&#39;: &#39;tool_call&#39;}]) # model doesn&#39;t always need to call a tool output = llm_with_tools.invoke(&quot;Hello world!&quot;) output.content, output.tool_calls (&#39;Hello! How can I assist you today?&#39;, []) 7.A.9. Chat History Chat history is sequence of messages, each of which is associated with a specific role, such as user, assistant, system, or tool, a record of the conversation between the user and the chat model, which is used to maintain context and state throughout the conversation. A full conversation often starts with a system message that sets the context for the conversation, and follows a combination of two alternating message patterns: user and assistant, representing a back-and-forth conversation, or assistant and tool, representing an &quot;agentic&quot; workflow where the assistant invokes tools for specific tasks. All models have finite context windows, and trim_messages can be used to reduce the size of a chat history to a specified token count or specified message count. from langchain_core.messages import ( AIMessage, HumanMessage, SystemMessage, trim_messages, ) messages = [ SystemMessage(&quot;you&#39;re a good assistant, you always respond with a joke.&quot;), HumanMessage(&quot;i wonder why it&#39;s called langchain&quot;), AIMessage( &#39;Well, I guess they thought &quot;WordRope&quot; and &quot;SentenceString&quot; just didn\&#39;t have the same ring to it!&#39; ), HumanMessage(&quot;and who is harrison chasing anyways&quot;), AIMessage( &quot;Hmmm let me think.\n\nWhy, he&#39;s probably chasing after the last cup of coffee in the office!&quot; ), HumanMessage(&quot;what do you call a speechless parrot&quot;), ] # trimming based on token count from langchain_core.messages.utils import count_tokens_approximately trim_messages( messages, strategy=&quot;last&quot;, token_counter=count_tokens_approximately, max_tokens=45, start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, allow_partial=False, ) SystemMessage(content=&quot;you&#39;re a good assistant, you always respond with a joke.&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;what do you call a speechless parrot&#39;, additional_kwargs={}, response_metadata={})] # trimming based on message count trim_messages( messages, strategy=&quot;last&quot;, token_counter=len, max_tokens=5, # message count start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, ) [SystemMessage(content=&quot;you&#39;re a good assistant, you always respond with a joke.&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;and who is harrison chasing anyways&#39;, additional_kwargs={}, response_metadata={}), AIMessage(content=&quot;Hmmm let me think.\n\nWhy, he&#39;s probably chasing after the last cup of coffee in the office!&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;what do you call a speechless parrot&#39;, additional_kwargs={}, response_metadata={})] # using a chat model as a token counter from langchain_openai import ChatOpenAI trim_messages( messages, max_tokens=45, strategy=&quot;first&quot;, token_counter=ChatOpenAI(model=&quot;gpt-4o&quot;), ) # chaining from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=&quot;gpt-4o&quot;) trimmer = trim_messages( token_counter=llm, strategy=&quot;last&quot;, max_tokens=45, start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, ) chain = trimmer | llm chain.invoke(messages) from langchain_core.chat_history import InMemoryChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory chat_history = InMemoryChatMessageHistory(messages=messages[:-1]) def dummy_get_session_history(session_id): if session_id != &quot;1&quot;: return InMemoryChatMessageHistory() return chat_history trimmer = trim_messages( max_tokens=45, strategy=&quot;last&quot;, token_counter=llm, include_system=True, start_on=&quot;human&quot;, ) chain = trimmer | llm chain_with_history = RunnableWithMessageHistory( chain, dummy_get_session_history ) chain_with_history.invoke( [HumanMessage(&quot;what do you call a speechless parrot&quot;)], config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;1&quot;}}, ) 7.A.10. Memory Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. Short-term memory, or thread-scoped memory, can be recalled at any time from within a single conversational thread with a user. Long-term memory is shared across conversational threads, and can be recalled at any time and in any thread. 7.A.11. LangChain Expression Language (LCEL) The LangChain Expression Language (LCEL) uses a declarative approach, similar to a Unix pipe, to build new Runnable components from existing ones, where a Runnable created with LCEL is often referred to as a &quot;chain&quot; and fully implements the Runnable interface. from langchain_core.vectorstores import InMemoryVectorStore from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) vectorstore = InMemoryVectorStore.from_texts( [&quot;harrison worked at kensho&quot;], embedding=embeddings, ) retriever = vectorstore.as_retriever() from langchain_core.prompts import ChatPromptTemplate template = &quot;&quot;&quot;Answer the question based only on the following context: {context} Question: {question} &quot;&quot;&quot; prompt = ChatPromptTemplate.from_template(template) from langchain_core.runnables import RunnablePassthrough prompt_chain = { &quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough(), } | prompt prompt_text = prompt_chain.invoke(&quot;where did harrison work?&quot;).to_string() print(prompt_text) Human: Answer the question based only on the following context: [Document(id=&#39;d03a67c7-a031-43aa-a27c-6411f9dd0dba&#39;, metadata={}, page_content=&#39;harrison worked at kensho&#39;)] Question: where did harrison work? from langchain_core.output_parsers import StrOutputParser from langchain_openai import ChatOpenAI llm = ChatOpenAI() retrieval_chain = ( {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()} | prompt | llm | StrOutputParser() ) output = retrieval_chain.invoke(&quot;where did harrison work?&quot;) print(output) Harrison worked at Kensho. In LCEL chains, the two main composition primitives are RunnableSequence and RunnableParallel. RunnableSequence is a composition primitive to chain multiple runnables sequentially, with the output of one runnable serving as the input to the next. from langchain_core.runnables import RunnableSequence chain = RunnableSequence([runnable1, runnable2]) final_output = chain.invoke(some_input) corresponds to the following: output1 = runnable1.invoke(some_input) final_output = runnable2.invoke(output1) RunnableParallel is a composition primitive to run multiple runnables concurrently, with the same input provided to each. from langchain_core.runnables import RunnableParallel chain = RunnableParallel({ &quot;key1&quot;: runnable1, &quot;key2&quot;: runnable2, }) final_output = chain.invoke(some_input) { &quot;key1&quot;: runnable1.invoke(some_input), &quot;key2&quot;: runnable2.invoke(some_input), } The | (pipe) operator have been overloaded to create a RunnableSequence from two Runnables. chain = runnable1 | runnable2 is Equivalent to: chain = RunnableSequence([runnable1, runnable2]) is Equivalent to: chain = runnable1.pipe(runnable2) LCEL applies automatic type coercion to make it easier to compose chains. Inside an LCEL expression, a dictionary is automatically converted to a RunnableParallel. mapping = { &quot;key1&quot;: runnable1, &quot;key2&quot;: runnable2, } chain = mapping | runnable3 is automatically converted to the following: chain = RunnableSequence([RunnableParallel(mapping), runnable3]) Inside an LCEL expression, a function is automatically converted to a RunnableLambda. def some_func(x): return x chain = some_func | runnable1 is automatically converted to the following: chain = RunnableSequence([RunnableLambda(some_func), runnable1]) A dict object defines data routing in LCEL by mapping keys to Runnables, functions, or static values, while RunnablePassthrough duplicates data across the pipeline as a data conduit to orchestrate chain flow. chain = ( {&quot;input&quot;: RunnablePassthrough()} # capture initial input | { &quot;output&quot;: llm_chain, # generate LLM output &quot;input&quot;: RunnablePassthrough() # maintain original input } ) # output: {&quot;output&quot;: &quot;LLM&#39;s answer&quot;, &quot;input&quot;: &quot;user&#39;s question&quot;} 8. Semantic Search and Retrieval-Augmented Generation Dense retrieval, reranking, and Retrieval-Augmented Generation (RAG) represent three significant strategies for enhancing search using language models. Dense retrieval systems rely on the concept of embeddings, and turn the search problem into retrieving the nearest neighbors of the search query (after both the query and the documents are converted into embeddings). Figure 89. Dense retrieval is one of the key types of semantic search, relying on the similarity of text embeddings to retrieve relevant results. A reranking language model is one of multiple steps in search system pipelines and is tasked with scoring the relevance of a subset of results against the query; the order of results is then changed based on these scores. Figure 90. Rerankers, the second key type of semantic search, take a search query and a collection of results, and reorder them by relevance, often resulting in vastly improved results. An RAG (Retrieval-Augmented Generation) system is a text generation system that incorporates search capabilities to reduce hallucinations, increase factuality, and/or ground the generation model on a specific dataset. Figure 91. A RAG system formulates an answer to a question and (preferably) cites its information sources. 8.1. Semantic Search with Language Models An embedding is a numeric representation of text, where each text is intuitively represented as a point (or a vector), and texts with similar meaning are close to each other in the high multi-dimensional embedding space. 8.1.1. Dense Retrieval Figure 92. Dense retrieval relies on the property that search queries will be close to their relevant results. # dense retrieval with FAISS from sentence_transformers import SentenceTransformer import faiss text = &quot;&quot;&quot; Artificial intelligence was founded as an academic discipline in 1956. Alan Turing was the first person to conduct substantial research in AI. Born in Maida Vale, London, Turing was raised in southern England. &quot;&quot;&quot; sentences = text.split(&quot;.&quot;) sentences = [s.strip() for s in sentences if s.strip()] model = SentenceTransformer(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) # embedding the text chunks. xb = model.encode(sentences) # building the search index. d = xb.shape[1] index = faiss.IndexFlatL2(d) index.add(xb) # search the index q = &quot;Who is Alan Turing?&quot; xq = model.encode([q]) distances, indices = index.search(xq, 3) print(f&quot;Q: {q}&quot;) for i in range(len(indices[0])): sentence = sentences[indices[0][i]] distance = distances[0][i] print(f&quot; Sentence: {sentence}&quot;) print(f&quot; Distance: {distance:.4f}&quot;) Q: Who is Alan Turing? Sentence: Alan Turing was the first person to conduct substantial research in AI Distance: 0.4903 Sentence: Born in Maida Vale, London, Turing was raised in southern England Distance: 1.0674 Sentence: Artificial intelligence was founded as an academic discipline in 1956 Distance: 1.4276 # keyword search with BM25 import string import numpy as np from rank_bm25 import BM25Okapi from sklearn.feature_extraction import _stop_words from tqdm import tqdm def bm25_tokenizer(text: str): tokenized_doc = [] for token in text.lower().split(): token = token.strip(string.punctuation) if len(token) &gt; 0 and token not in _stop_words.ENGLISH_STOP_WORDS: tokenized_doc.append(token) return tokenized_doc tokenized_corpus = [] text = &quot;&quot;&quot; Artificial intelligence was founded as an academic discipline in 1956. Alan Turing was the first person to conduct substantial research in AI. Born in Maida Vale, London, Turing was raised in southern England. &quot;&quot;&quot; texts = text.split(&#39;.&#39;) for passage in tqdm(texts): tokenized_corpus.append(bm25_tokenizer(passage)) bm25 = BM25Okapi(tokenized_corpus) def keyword_search(q: str, k=3, n=3): print(&quot;Input question:&quot;, q) bm25_scores = bm25.get_scores(bm25_tokenizer(q)) top_n = np.argpartition(bm25_scores, -n)[-n:] bm25_hits = [ {&#39;corpus_id&#39;: idx, &#39;score&#39;: bm25_scores[idx]} for idx in top_n ] bm25_hits = sorted(bm25_hits, key=lambda x: x[&#39;score&#39;], reverse=True) print(&quot;Top-3 lexical search (BM25) hits&quot;) for hit in bm25_hits[0:k]: print( &quot;\t{:.3f}\t{}&quot;.format( hit[&#39;score&#39;], texts[hit[&#39;corpus_id&#39;]].replace(&quot;\n&quot;, &quot; &quot;) ) ) q = &quot;Who is Alan Turing?&quot; keyword_search(q=q, k=3, n=len(texts)) Input question: Who is Alan Turing? Top-3 lexical search (BM25) hits 0.737 Alan Turing was the first person to conduct substantial research in AI 0.000 Artificial intelligence was founded as an academic discipline in 1956 0.000 Born in Maida Vale, London, Turing was raised in southern England It’s useful to be aware of some of the drawbacks of dense retrieval and how to address them. Lack of Answer in Retrieved Texts Dense retrieval always returns results based on semantic similarity, even if none of the texts actually contain the answer to the query. A potential solution is to implement a distance threshold to filter out results that are not sufficiently relevant. User feedback (click-through rates and satisfaction) can also help improve the system over time. Difficulty with Exact Phrase Matches Dense retrieval, relying on semantic similarity, may not perform well when a user is looking for an exact match of a specific phrase. In such cases, traditional keyword matching is more effective, suggesting the use of hybrid search systems that combine both approaches. Domain Specificity Dense retrieval models trained on data from one domain (e.g., internet and Wikipedia) may not generalize well to other, unseen domains (e.g., legal texts) without sufficient training data from that new domain. Handling Multi-Sentence Answers Dense retrieval systems face the challenge of how to best chunk long texts into embeddings. A key design parameter is deciding the optimal way to divide documents, as answers to some questions may span multiple sentences, and models have context size limitations. Chunking strategies include embedding per document (which can lose information) or embedding multiple chunks per document (which offers better coverage). Various chunking methods exist, such as by sentence, paragraph, or overlapping segments to retain context, with the best approach depending on the text and query types. Scalability and Efficiency While simple nearest neighbor search with tools like NumPy works for smaller datasets, for millions of vectors, optimized approximate nearest neighbor (ANN) search libraries like FAISS or Annoy are necessary for efficient retrieval. Vector databases like Weaviate or Pinecone offer additional functionalities like adding/deleting vectors without rebuilding the index and advanced filtering options. Need for Fine-Tuning To optimize dense retrieval for specific tasks, fine-tuning the embedding models with relevant query-result pairs (including negative examples) is crucial. This process aims to bring embeddings of relevant queries and results closer together in the vector space while pushing irrelevant ones further apart. 8.1.2. Reranking A reranker takes in the search query and a number of search results, and returns the optimal ordering of these documents so the most relevant ones to the query are higher in ranking. Figure 93. LLM rerankers operate as part of a search pipeline with the goal of reordering a number of shortlisted search results by relevance. Figure 94. A reranker assigns a relevance score to each document by looking at the document and the query at the same time. For the retrieval, either lexical search, e.g. with a vector engine like Elasticsearch, or dense retrieval with a SentenceTransformer (a.k.a. bi-encoder) can be used. However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, a re-ranker based on a CrossEncoder that scores the relevancy of all shortlisted candidates for the given search query can be used to output a ranked list. from sentence_transformers import SentenceTransformer bi_encoder = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;) corpus = [ &quot;A man is eating food.&quot;, &quot;A man is eating a piece of bread.&quot;, &quot;The girl is carrying a baby.&quot;, &quot;A man is riding a horse.&quot;, &quot;A woman is playing violin.&quot;, &quot;Two men pushed carts through the woods.&quot;, &quot;A man is riding a white horse on an enclosed ground.&quot;, &quot;A monkey is playing drums.&quot;, &quot;A cheetah is running behind its prey.&quot;, ] corpus_embeddings = bi_encoder.encode(corpus, convert_to_tensor=True) query = &quot;A man is eating pasta.&quot; query_embedding = bi_encoder.encode(query, convert_to_tensor=True) top_N = min(10, len(corpus)) similarity_scores = bi_encoder.similarity(query_embedding, corpus_embeddings)[0] import torch scores, indices = torch.topk(similarity_scores, k=top_N) documents = [] for score, index in zip(scores, indices): document = corpus[index] print(f&quot;({score:.4f})&quot;, document) documents.append(document) (0.7035) A man is eating food. (0.5272) A man is eating a piece of bread. (0.1889) A man is riding a horse. (0.1047) A man is riding a white horse on an enclosed ground. (0.0980) A cheetah is running behind its prey. (0.0819) A monkey is playing drums. (0.0336) A woman is playing violin. (-0.0594) Two men pushed carts through the woods. (-0.0898) The girl is carrying a baby. from sentence_transformers import CrossEncoder cross_encoder = CrossEncoder(&quot;cross-encoder/ms-marco-MiniLM-L-6-v2&quot;) top_K = min(5, top_N) ranking = cross_encoder.rank( query, documents, top_k=top_K, return_documents=True, ) for r in ranking: print(f&quot;({r[&#39;score&#39;]:.4f})&quot;, r[&quot;text&quot;]) (1.9005) A man is eating food. (1.4804) A man is eating a piece of bread. (-7.0890) A man is riding a horse. (-8.9042) A man is riding a white horse on an enclosed ground. (-10.7628) A monkey is playing drums. 8.2. Retrieval-Augmented Generation (RAG) RAG systems incorporate search capabilities in addition to generation capabilities to enhance factuality and reduce hallucinations. Figure 95. A basic RAG pipeline is made up of a search step followed by a grounded generation step where the LLM is prompted with the question and the information retrieved from the search step. Figure 96. Generative search formulates answers and summaries at the end of a search pipeline while citing its sources (returned by the previous steps in the search system). Figure 97. Find the most relevant information to an input prompt by comparing the similarities between embeddings. The most relevant information is added to the prompt before giving it to the LLM. from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&quot;mistral:7b-instruct&quot;, api_key=&#39;APK-KEY&#39;, base_url=&quot;http://localhost:11434/v1&quot;, # Ollama ) from langchain_text_splitters import HTMLHeaderTextSplitter headers_to_split_on = [ (&quot;h1&quot;, &quot;Header 1&quot;), (&quot;h2&quot;, &quot;Header 2&quot;), (&quot;h3&quot;, &quot;Header 3&quot;), (&quot;h4&quot;, &quot;Header 4&quot;), ] html_splitter = HTMLHeaderTextSplitter(headers_to_split_on) url = &quot;https://plato.stanford.edu/entries/goedel/&quot; documents = html_splitter.split_text_from_url(url) from langchain_community.vectorstores import FAISS from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) db = FAISS.from_documents(documents, embeddings) from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot; Relevant information: {context} Provide a concise answer the following question using the relevant information provided above: {question} &quot;&quot;&quot; prompt = PromptTemplate.from_template(template=template) from langchain.chains.retrieval_qa.base import RetrievalQA rag = RetrievalQA.from_chain_type( llm=llm, chain_type=&quot;stuff&quot;, retriever=db.as_retriever(), chain_type_kwargs={&quot;prompt&quot;: prompt}, verbose=True, ) rag.invoke(&quot;Who is Kurt Gödel?&quot;) {&#39;query&#39;: &#39;Who is Kurt Gödel?&#39;, &#39;result&#39;: &quot; Kurt Gödel was an Austrian mathematician and logician. He is best known for his work on the incompleteness theorems, which were established in 1930 and prove that any sufficiently rich formal axiomatic system contains either statements that cannot be proven or disproven within the system itself. Some of Gödel&#39;s other notable contributions include his proof of the consistency of the continuum hypothesis using large cardinals, and his work on undecidable propositions in number theory, which led to the concept of Gödel numbers for representing mathematical statements in a formal system. Throughout his life, Gödel also explored philosophical questions related to logic, mathematics, and metaphysics, including questions about realism, the foundations of mathematics, set theory, and the nature of time and truth.&quot;} 9. Multimodal Large Language Models A multimodal model is a type of artificial intelligence model capable of processing and reasoning across different modalities, where a modality refers to a distinct type of data such as text, images, audio, video, or sensor data. Figure 98. Models that are able to deal with different types (or modalities) of data, such as images, audio, video, or sensors, are said to be multimodal. It’s possible for a model to accept a modality as input yet not be able to generate in that modality. 9.1. Vision Transformer (ViT) Vision Transformer (ViT) is a method that adapts the Transformer architecture to the field of computer vision, particularly for image recognition tasks, by treating an image as a sequence of flattened image patches which are then linearly embedded and processed by the Transformer encoder in a manner similar to textual tokens, allowing it to capture global relationships in the image more directly than the local receptive fields of convolutional neural networks (CNNs). Figure 99. The main algorithm behind ViT. After patching the images and linearly projecting them, the patch embeddings are passed to the encoder and treated as if they were textual tokens. 9.2. Multimodal Embedding Models A multimodal embedding model is a type of model that can create numerical representations (embeddings) for multiple modalities, such as text and imagery, within the same vector space, allowing for direct comparison of representations from different modalities based on their semantic content. Figure 100. Despite having coming from different modalities, embeddings with similar meaning will be close to each other in vector space. Contrastive Language-Image Pre-training (CLIP) is an embedding model to compute embeddings of both images and texts. Figure 101. In the first step of training CLIP, both images and text are embedded using an image and text encoder, respectively. Figure 102. In the second step of training CLIP, the similarity between the sentence and image embedding is calculated using cosine similarity. Figure 103. In the third step of training CLIP, the text and image encoders are updated to match what the intended similarity should be (called contrastive learning). This updates the embeddings such that they are closer in vector space if the inputs are similar. from urllib.request import urlopen from PIL import Image # load an AI-generated image of a puppy playing in the snow from a URL puppy_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/puppy.png&quot; ) # open the image from the URL and convert it to RGB format image = Image.open(urlopen(puppy_path)).convert(&quot;RGB&quot;) # define a text caption for the image caption = &quot;a puppy playing in the snow&quot; Figure 104. An AI-generated image of a puppy playing in the snow. from transformers import CLIPTokenizer, CLIPProcessor, CLIPModel model_id = &quot;openai/clip-vit-base-patch32&quot; # load the tokenizer associated with the CLIP model to preprocess text clip_tokenizer = CLIPTokenizer.from_pretrained(model_id, use_fast=True) # load the processor associated with the CLIP model to preprocess images and text clip_processor = CLIPProcessor.from_pretrained(model_id, use_fast=True) # load the main CLIP model for generating text and image embeddings model = CLIPModel.from_pretrained(model_id) # tokenize the input caption into numerical representations inputs = clip_tokenizer(caption, return_tensors=&quot;pt&quot;) inputs {&#39;input_ids&#39;: tensor([[49406, 320, 6829, 1629, 530, 518, 2583, 49407]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} # convert the token IDs back to the corresponding text tokens clip_tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0]) [&#39;&lt;|startoftext|&gt;&#39;, &#39;a&lt;/w&gt;&#39;, &#39;puppy&lt;/w&gt;&#39;, &#39;playing&lt;/w&gt;&#39;, &#39;in&lt;/w&gt;&#39;, &#39;the&lt;/w&gt;&#39;, &#39;snow&lt;/w&gt;&#39;, &#39;&lt;|endoftext|&gt;&#39;] # create a text embedding vector representing the semantic meaning of the caption text_embedding = model.get_text_features(**inputs) text_embedding.shape # (batch_size, embedding_dimension) torch.Size([1, 512]) # preprocess the image to match the input requirements of the CLIP model image_inputs = clip_processor(text=None, images=image, return_tensors=&quot;pt&quot;) image_pixel_values = image_inputs[&quot;pixel_values&quot;] image_pixel_values.shape # (batch_size, num_channels, height, width) torch.Size([1, 3, 224, 224]) import torch import numpy as np import matplotlib.pyplot as plt # prepare the preprocessed image tensor for visualization img = image_pixel_values.squeeze(0) # remove the batch dimension img = img.permute(*torch.arange(img.ndim - 1, -1, -1)) # transpose dimensions for correct visualization order (C, H, W -&gt; H, W, C) img = np.einsum(&quot;ijk-&gt;jik&quot;, img) # visualize the preprocessed image plt.imshow(img) # turn off axis labels and ticks plt.axis(&quot;off&quot;) Figure 105. The preprocessed input image by CLIP. # create the image embedding vector representing the visual content of the image image_embedding = model.get_image_features(image_pixel_values) image_embedding.shape # (batch_size, embedding_dimension): same as that of the text embedding torch.Size([1, 512]) # normalize the text and image embeddings text_embedding /= text_embedding.norm(dim=-1, keepdim=True) image_embedding /= image_embedding.norm(dim=-1, keepdim=True) # calculate the cosine similarity score text_embedding = text_embedding.detach().cpu().numpy() # move the text embedding to CPU and convert to NumPy array image_embedding = image_embedding.detach().cpu().numpy() # move the image embedding to CPU and convert to NumPy array score = np.dot(text_embedding, image_embedding.T) score array([[0.33146894]], dtype=float32) sentence-transformers implements a few CLIP-based models that make it much easier to create embeddings. It only takes a few lines of code: from urllib.request import urlopen from PIL import Image puppy_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/puppy.png&quot; ) image = Image.open(urlopen(puppy_path)).convert(&quot;RGB&quot;) caption = &quot;a puppy playing in the snow&quot; from sentence_transformers import SentenceTransformer, util model = SentenceTransformer(&quot;sentence-transformers/clip-ViT-B-32&quot;) image_embeddings = model.encode([image]) text_embeddings = model.encode([caption]) sim_matrix = util.cos_sim(image_embeddings, text_embeddings) sim_matrix # tensor([[0.3315]]) 9.3. Multimodal Text Generation Models BLIP-2 (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 2) is a multimodal text generation model designed to introduce vision capabilities to existing, pre-trained language models (LLMs) without requiring end-to-end training from scratch. Figure 106. The Querying Transformer is the bridge between vision (ViT) and text (LLM) that is the only trainable component of the pipeline. 9.3.1. BLIP-2: Bridging the Modality Gap BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), connecting a frozen (non-trainable) pre-trained image encoder like a Vision Transformer and a frozen pre-trained LLM. The Q-Former is trained in two stages, one for each modality to make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM to give information about the image in a similar manner to the context providing an LLM when prompting. Figure 107. In step 1, representation learning is applied to learn representations for vision and language simultaneously. In step 2, these representations are converted to soft visual prompts to feed the LLM. In step 1, image-document pairs are used to train the Q-Former to represent both images and text, which are generally captions of images similar tranning CLIP. Figure 108. In step 1, the output of the frozen ViT is used together with its caption and trained on three contrastive-like tasks to learn visual-text representations. The images are fed to the frozen ViT to extract vision embeddings, which are used as the input of Q-Former’s ViT, and the captions are used as the input of Q-Former’s Text Transformer. The Q-Former is then trained on three tasks: image-text contrastive learning that attempts to align pairs of image and text embeddings such that they maximize their mutual information, image-text matching that predicts whether an image and text pair is positive (matched) or negative (unmatched), and image-grounded text generation that generates text based on information extracted from the input image. In step 2, the learnable embeddings containing aligned visual and textual information in the same dimensional space from the Q-Former are projected to match the LLM&#8217;s input format and then serve as soft visual prompts, conditioning the LLM on the visual representations. Figure 109. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer. The projected embeddings serve as a soft visual prompt. 9.3.2. Preprocessing Multimodal Inputs from urllib.request import urlopen from PIL import Image # load image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) Figure 110. An orange supercar driving on the road at sunset. import torch from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=torch.float16, device_map=dev, ) model.vision_model # vision transformer in the loaded BLIP-2 model. Blip2VisionModel( (embeddings): Blip2VisionEmbeddings( (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14)) ) (encoder): Blip2Encoder( (layers): ModuleList( (0-38): 39 x Blip2EncoderLayer( (self_attn): Blip2Attention( (dropout): Dropout(p=0.0, inplace=False) (qkv): Linear(in_features=1408, out_features=4224, bias=True) (projection): Linear(in_features=1408, out_features=1408, bias=True) ) (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) (mlp): Blip2MLP( (activation_fn): GELUActivation() (fc1): Linear(in_features=1408, out_features=6144, bias=True) (fc2): Linear(in_features=6144, out_features=1408, bias=True) ) (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) ) ) ) (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) ) model.language_model # text generative model in the loaded BLIP-2 model. OPTForCausalLM( (model): OPTModel( (decoder): OPTDecoder( (embed_tokens): Embedding(50304, 2560, padding_idx=1) (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560) (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) (layers): ModuleList( (0-31): 32 x OPTDecoderLayer( (self_attn): OPTSdpaAttention( (k_proj): Linear(in_features=2560, out_features=2560, bias=True) (v_proj): Linear(in_features=2560, out_features=2560, bias=True) (q_proj): Linear(in_features=2560, out_features=2560, bias=True) (out_proj): Linear(in_features=2560, out_features=2560, bias=True) ) (activation_fn): ReLU() (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=2560, out_features=10240, bias=True) (fc2): Linear(in_features=10240, out_features=2560, bias=True) (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) ) ) ) ) (lm_head): Linear(in_features=2560, out_features=50304, bias=False) ) # preprocess the image image_inputs = blip_processor(image, return_tensors=&quot;pt&quot;).to(dev, torch.float16) image_pixel_values = image_inputs[&quot;pixel_values&quot;] image_pixel_values.shape # a 224 × 224-sized image torch.Size([1, 3, 224, 224]) # tokenizer used to tokenize the input text blip_processor.tokenizer GPT2TokenizerFast(name_or_path=&#39;Salesforce/blip2-opt-2.7b&#39;, vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;bos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;}, clean_up_tokenization_spaces=False, added_tokens_decoder={ 1: AddedToken(&quot;&lt;pad&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 2: AddedToken(&quot;&lt;/s&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 50265: AddedToken(&quot;&lt;image&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), } ) # preprocess the text text = &quot;Her vocalization was remarkably melodic&quot; token_ids = blip_processor(image, text=text, return_tensors=&quot;pt&quot;) token_ids = token_ids.to(dev, torch.float16)[&quot;input_ids&quot;][0] # convert input ids back to tokens tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids) tokens [&#39;&lt;/s&gt;&#39;, &#39;Her&#39;, &#39;Ġvocal&#39;, &#39;ization&#39;, &#39;Ġwas&#39;, &#39;Ġremarkably&#39;, &#39;Ġmel&#39;, &#39;odic&#39;] # replace the space token with an underscore tokens = [token.replace(&quot;Ġ&quot;, &quot;_&quot;) for token in tokens] tokens [&#39;&lt;/s&gt;&#39;, &#39;Her&#39;, &#39;_vocal&#39;, &#39;ization&#39;, &#39;_was&#39;, &#39;_remarkably&#39;, &#39;_mel&#39;, &#39;odic&#39;] 9.3.3. Use Case 1: Image Captioning from urllib.request import urlopen import torch from PIL import Image from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; dtype = torch.float16 if torch.cuda.is_available() else torch.float32 model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=dtype, device_map=dev, ) # load an AI-generated image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) # convert an image into inputs and preprocess it inputs = blip_processor(image, return_tensors=&quot;pt&quot;).to(dev, dtype) # {&#39;pixel_values&#39;: tensor([[[[-1.0039, -1.0039, -0.9893, ..., -0.0842, -0.0988, -0.0842], # generate image ids to be passed to the decoder (LLM) generated_ids = model.generate(**inputs, max_new_tokens=20) # generate text from the image ids generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text an orange supercar driving on the road at sunset 9.3.4. Use Case 2: Multimodal Chat-Based Prompting from urllib.request import urlopen import torch from PIL import Image from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; dtype = torch.float16 if torch.cuda.is_available() else torch.float32 model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=dtype, device_map=dev, ) # load an AI-generated image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) # visual question answering prompt = &quot;Question: Write down what you see in this picture. Answer:&quot; # process both the image and the prompt inputs = blip_processor(image, text=prompt, return_tensors=&quot;pt&quot;).to(dev, dtype) # generate text generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset # chat-like prompting: a follow-up question prompt = ( &quot;Question: Write down what you see in this picture. Answer: A sports &quot; &quot;car driving on the road at sunset. Question: What would it cost me to &quot; &quot;drive that car? Answer:&quot; ) # Generate output inputs = blip_processor(image, text=prompt, return_tensors=&quot;pt&quot;).to(dev, dtype) generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer: $1,000,000 10. Creating and Fine-Tuning Text Embedding Models Embedding models are Large Language Models (LLMs) used to convert unstructured textual data (like documents, sentences, or phrases) into dense numerical representations called embeddings. The primary goal of these models is to accurately capture the semantic meaning of the text, such that texts with similar meanings have embeddings that are close to each other in a high-dimensional vector space, while texts with different meanings have dissimilar embeddings. Figure 111. The idea of semantic similarity is that we expect textual data with similar meanings to be closer to each other in n-dimensional space (two dimensions are illustra‐ ted here). Embedding models can also be trained or fine-tuned for other purposes, such as capturing sentiment similarity, by guiding the model with appropriate training examples. Figure 112. In addition to semantic similarity, an embedding model can be trained to focus on sentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to positive reviews (green). 10.1. Contrastive Learning Contrastive learning is a self-supervised or supervised machine learning technique that aims to learn representations of data by contrasting similar (&quot;positive&quot;) and dissimilar (&quot;negative&quot;) examples (Why P and not Q?) to create an embedding space where similar data points are located close to each other, while dissimilar data points are far apart, which is effective in various domains, including computer vision and natural language processing, for tasks like representation learning, similarity search, and few-shot learning. Reporter: “Why did you rob a bank?” Robber: “Because that is where the money is.” Reporter (alternatively): “Why did you rob a bank (P) instead of obeying the law (Q)?” 10.2. Sentence Transformers (SBERT) A cross-encoder is a Transformer-based model that processes two sentences together to directly predict their similarity score via a classification head, but it&#8217;s computationally expensive for large-scale pairwise comparisons and doesn&#8217;t typically generate individual sentence embeddings. Figure 113. The architecture of a cross-encoder. Both sentences are concatenated, separated with a &lt;SEP&gt; token, and fed to the model simultaneously. The authors of sentence-transformers addressed the limitations of cross-encoders (slow speed, no embeddings) by developing a fast alternative that generates semantically comparable, fixed-size embeddings by using a Siamese architecture, also known as a bi-encoder or SBERT, with two identical BERT models (sharing weights) that process sentences independently and then apply mean pooling to the final layer. Figure 114. The architecture of the original sentence-transformers model, which leverages a Siamese network, also called a bi-encoder. 10.3. Creating an Embedding Model Natural Language Inference (NLI) datasets, used in pretraining embedding models, classify premise-hypothesis pairs as entailment (similar meaning), contradiction (opposite meaning), or neutral. Figure 115. We can leverage the structure of NLI datasets to generate negative examples (contradiction) and positive examples (entailments) for contrastive learning. Entailments serve as positive examples for contrastive learning (similar pairs), while contradictions serve as negative examples (dissimilar pairs). The Multi-Genre Natural Language Inference (MNLI) corpus from the General Language Understanding Evaluation (GLUE) benchmark contains annotated sentence pairs with these relationships, and is a common source for generating such contrastive training data. A subset of MNLI is often used for faster experimentation, though larger, quality datasets are generally preferred for stable training. from datasets import load_dataset # Load MNLI dataset from GLUE # 0 = entailment, 1 = neutral, 2 = contradiction train_dataset = load_dataset( &quot;glue&quot;, # load a dataset from the GLUE benchmark &quot;mnli&quot;, # load the MNLI dataset split=&quot;train&quot;, # load the training split ).select(range(50_000)) train_dataset = train_dataset.remove_columns(&quot;idx&quot;) train_dataset[2] {&#39;premise&#39;: &#39;One of our number will carry out your instructions minutely.&#39;, &#39;hypothesis&#39;: &#39;A member of my team will execute your orders with immense precision.&#39;, &#39;label&#39;: 0} # train model from sentence_transformers import SentenceTransformer # use a base model model = SentenceTransformer(&quot;google-bert/bert-base-uncased&quot;) from sentence_transformers import losses # define the softmax loss function. train_loss = losses.SoftmaxLoss( model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=3, ) from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # create an embedding similarity evaluator for STSB val_sts = load_dataset(&quot;glue&quot;, &quot;stsb&quot;, split=&quot;validation&quot;) evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[&quot;sentence1&quot;], sentences2=val_sts[&quot;sentence2&quot;], scores=[score / 5 for score in val_sts[&quot;label&quot;]], main_similarity=&quot;cosine&quot;, ) from sentence_transformers.training_args import ( SentenceTransformerTrainingArguments, ) args = SentenceTransformerTrainingArguments( output_dir=&quot;base_embedding_model&quot;, num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32, warmup_steps=100, fp16=True, eval_steps=100, logging_steps=100, ) from sentence_transformers.trainer import SentenceTransformerTrainer # train embedding model trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator, ) trainer.train() # evaluate the trained model evaluator(model) References [1] Jay Alammar, Maarten Grootendorst Hands-On Large Language Models: Language Understanding and Generation. O&#8217;Reilly Media; 1st edition (October 15, 2024)" />
<link rel="canonical" href="https://blog.codefarm.me/2025/03/25/hands-on-large-language-models/" />
<meta property="og:url" content="https://blog.codefarm.me/2025/03/25/hands-on-large-language-models/" />
<meta property="og:site_name" content="CODE FARM" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-25T08:06:54+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Hands-On Large Language Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-25T08:06:54+08:00","datePublished":"2025-03-25T08:06:54+08:00","description":"1. Language AI 2. Tokens and Embeddings 2.1. LLM Tokenization 2.2. Token Embeddings 2.3. Text Embeddings 3. Large Language Models 3.1. Inputs and Outputs 3.2. Components 3.3. Probability Distribution (Sampling/Decoding) 3.4. Parallel Token Processing and Context Size 3.5. Keys and Values Caching 3.6. Transformer Block 4. Text Classification 4.1. Representation Models 4.1.1. Task-Specific Model 4.1.2. Embedding model 4.2. Generative Models 4.2.1. Text-to-Text Transfer Transformer 4.2.2. ChatGPT for Classification 5. Text Clustering and Topic Modeling 5.1. ArXiv’s Articles: Computation and Language 5.2. A Common Pipeline for Text Clustering 5.2.1. Embedding Documents 5.2.2. Reducing the Dimensionality of Embeddings 5.2.3. Cluster the Reduced Embeddings 5.2.4. Inspecting the Clusters 5.3. From Text Clustering to Topic Modeling 5.3.1. BERTopic: A Modular Topic Modeling Framework 6. Prompt Engineering 6.1. Using Text Generation Models 6.1.1. Prompt Template 6.1.2. Controlling Model Output 6.2. Prompt Engineering 6.3. Instruction-Based Prompting 6.4. Advanced Prompt Engineering 6.4.1. Prompt Components 6.4.2. In-Context Learning: Providing Examples 6.4.3. Chain Prompting: Breaking up the Problem 6.5. Reasoning with Generative Models 6.5.1. Chain-of-Thought: Think Before Answering 6.5.2. Self-Consistency: Sampling Outputs 6.5.3. Tree-of-Thought: Exploring Intermediate Steps 6.6. Output Verification 6.6.1. Providing Examples 6.6.2. Grammar: Constrained Sampling 7. Advanced Text Generation Techniques and Tools 7.1. Model I/O: Loading Quantized Models with LangChain 7.2. Chains: Extending the Capabilities of LLMs 7.2.1. A Single Link in the Chain: Prompt Template 7.2.2. A Chain with Multiple Prompts 7.3. Memory: Helping LLMs to Remember Conversations 7.3.1. Conversation Buffer 7.3.2. Windowed Conversation Buffer 7.3.3. Conversation Summary 7.4. Agents: Creating a System of LLMs Appendix A: LangChain 7.A.1. Chat Models and Messages 7.A.2. Prompt Templates 7.A.3. Structured Outputs 7.A.4. Output Parsers 7.A.5. Embedding, Vector Stores, and Retrievers 7.A.6. Document Loaders 7.A.7. Text Splitters 7.A.8. Tools 7.A.9. Chat History 7.A.10. Memory 7.A.11. LangChain Expression Language (LCEL) 8. Semantic Search and Retrieval-Augmented Generation 8.1. Semantic Search with Language Models 8.1.1. Dense Retrieval 8.1.2. Reranking 8.2. Retrieval-Augmented Generation (RAG) 9. Multimodal Large Language Models 9.1. Vision Transformer (ViT) 9.2. Multimodal Embedding Models 9.3. Multimodal Text Generation Models 9.3.1. BLIP-2: Bridging the Modality Gap 9.3.2. Preprocessing Multimodal Inputs 9.3.3. Use Case 1: Image Captioning 9.3.4. Use Case 2: Multimodal Chat-Based Prompting 10. Creating and Fine-Tuning Text Embedding Models 10.1. Contrastive Learning 10.2. Sentence Transformers (SBERT) 10.3. Creating an Embedding Model References 1. Language AI Google Colab offers free, cloud-based GPU and TPU access for accelerated computation, subject to usage limits, and requires changing the runtime type to GPU to enable it. Artificial Intelligence (AI) is the science and engineering of creating intelligent machines, particularly intelligent computer programs, that can perform tasks similar to human intelligence. Language AI is a subfield of AI focused on developing technologies that can understand, process, and generate human language, which is often used interchangeably with Natural Language Processing (NLP). Figure 1. A peek into the history of Language AI. Figure 2. Language AI is capable of many tasks by processing textual input. The Bag-of-Words, a representation model, converts text to numerical vectors by tokenizing it—splitting sentences into individual words or subwords (tokens)—creating a vocabulary, and counting token occurrences to form a vector representation (the &#39;bag of words&#39;). Figure 3. A bag-of-words is created by counting individual words. These values are referred to as vector representations. Word2vec introduced dense vector embeddings, a significant improvement over Bag-of-Words, by using neural networks to capture the semantic meaning of words based on their context within large datasets, allowing for the measurement of semantic similarity. Figure 4. Embeddings of words that are similar will be close to each other in dimensional space. Figure 5. Embeddings can be created for different types of input. Attention-based Transformer models, replacing RNNs which struggled with long sentences, enabled parallel processing and context-aware language representation by using stacked encoders and decoders to focus on relevant input, revolutionizing language AI. Figure 6. Using word2vec embeddings, a context embedding is generated that represents the entire sequence. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder. Figure 7. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder. Figure 8. The encoder block revolves around self-attention to generate intermediate representations. Figure 9. The decoder has an additional attention layer that attends to the output of the encoder. Encoder-only models (a.k.a., representation models) like Bidirectional Encoder Representations from Transformers(BERT) excel at language representation through masked language modeling, while decoder-only models (a.k.a., generative models) like Generative Pre-trained Transformer (GPT) focus on text generation and are the foundation for large language models. Figure 10. The architecture of a BERT base model with 12 encoders. Figure 11. The architecture of a GPT-1. It uses a decoder-only architecture and removes the encoder-attention block. Generative LLMs function as sequence-to-sequence machines, initially designed for text completion, but their capability to be fine-tuned into chatbots or instruct models that can follow user prompts revealed their true potential. Figure 12. Generative LLMs take in some input and try to complete it. With instruct models, this is more than just autocomplete and attempts to answer the question. The context length, or window, represents the maximum number of tokens the model can process, enabling the generative LLM to handle larger documents, and the current length expands as the model generates new tokens due to its autoregressive nature. Figure 13. The context length is the maximum context an LLM can handle. LLMs differ from traditional machine learning by using a two-step training process: pretraining, for general language learning, and fine-tuning (or post-training), to adapt the pretrained (foundation/base) model for specific tasks. Figure 14. Compared to traditional machine learning, LLM training takes a multistep approach. Closed-source LLMs, like GPT-4 and Claude, are models that do not have their weights and architecture shared with the public, which are accessed via APIs, and offer high performance with managed hosting, but are costly and limit user control; open LLMs, such as Llama, share their architecture, enabling local use, fine-tuning, and privacy, but require powerful hardware and expertise. The main source for finding and downloading LLMs is the Hugging Face Hub. Hugging Face is the organization behind the well-known Transformers package, which for years has driven the development of language models in general. # If a connection to the Hugging Face URL (https://huggingface.co/) fails, try to set the HF_ENDPOINT environment variable to the mirror URL. import os os.environ[&quot;HF_ENDPOINT&quot;] = &quot;https://hf-mirror.com&quot; Hugging Face, the organization behind the Transformers package, is the primary source for finding and downloading LLMs, built upon the Transformer framework. import os from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # HF_ENDPOINT controls the base URL used by the transformers library # to download models and other resources from the Hugging Face Hub. os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # create a pipeline pipe = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=True, ) # the prompt (user input / query) messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a funny joke about chickens.&quot;}] # generate output output = pipe(messages) print(output[0][&quot;generated_text&quot;]) Why did the chicken join the band? Because he heard they had the &quot;cluck-loudest&quot; performers around! # clear memory and empty the VRAM import gc import torch # attempt to delete the model, tokenizer, and pipeline objects from memory del model, tokenizer, pipe # flush memory gc.collect() if torch.cuda.is_available(): # if a GPU is available, empty the CUDA cache to free up GPU memory torch.cuda.empty_cache() 2. Tokens and Embeddings Tokens and embeddings are two of the central concepts of using large language models (LLMs). Figure 15. Language models deal with text in small chunks called tokens. For the lan‐ guage model to compute language, it needs to turn tokens into numeric representations called embeddings. 2.1. LLM Tokenization import os import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # HF_ENDPOINT controls the base URL used by the transformers library # to download models and other resources from the Hugging Face Hub. os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39; # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) prompt = &#39;&lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;&#39; # tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=&#39;pt&#39;).input_ids.to(dev) print(f&#39;input_ids: {input_ids}&#39;) # generate the text output_ids = model.generate(input_ids=input_ids, max_new_tokens=20) print(f&#39;output_ids: {output_ids}&#39;) # print the output print(tokenizer.decode(output_ids[0])) input_ids: tensor([[101950, 29, 16465, 448, 3719, 39950, 6396, 316, 32145, 395, 290, 62374, 66241, 80785, 403, 13, 115474, 1495, 480, 12570, 13, 200019]]) output_ids: tensor([[101950, 29, 16465, 448, 3719, 39950, 6396, 316, 32145, 395, 290, 62374, 66241, 80785, 403, 13, 115474, 1495, 480, 12570, 13, 200019, 18174, 25, 336, 2768, 512, 6537, 10384, 395, 290, 193145, 147276, 403, 279, 36210, 32145, 4464, 40, 5498, 495, 3719]]) &lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;Subject: Sincere Apologies for the Gardening Mishap Dear Sarah, I hope this email Tokens, the units into which text prompts are broken for model input, also form the model&#8217;s output. Figure 16. A tokenizer encodes input prompts into token ID lists for the language model and decodes the model&#8217;s output token IDs back into words or tokens. Each ID corresponds to a specific token (character, word, or subword) in the tokenizer&#8217;s vocabulary. The tokenizer&#8217;s vocabulary acts as a lookup table, allowing the model to convert between text and these integer representations. for id in [101950, 29, 16465, 448, 3719, 39950]: print(tokenizer.decode(id)) # &lt;s # &gt; # Write # an # email # apolog for id in [18174, 25, 336, 2768, 512]: print(tokenizer.decode(id) # Subject # : # S # inc # ere Tokenization is determined by three major design decisions: the tokenizer algorithm (e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size, special tokens, capitalization, treatment of capitalization and different languages), and the dataset the tokenizer is trained on (a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset). Tokenization methods vary in granularity, from word-level to byte-level, with subword tokenization offering a balance of vocabulary expressiveness and efficiency, making it the most common approach in modern language models. 2.2. Token Embeddings Text --&gt; Tokens --&gt; Token IDs --&gt; Embeddings (Vectors) A tokenizer, once trained, becomes intrinsically linked to its language model during the model&#8217;s training; consequently, a pretrained language model cannot function with a different tokenizer without retraining, as their vocabularies and tokenization schemes are aligned. An embedding is a dense, numerical vector representation of a token (like a word or subword) that captures its semantic meaning within a high-dimensional space, enabling language models to understand and process relationships between words. A language model stores static embedding vectors for each token in its vocabulary, but also generates contextualized word embeddings, dynamically representing a token based on its context instead of a single, fixed vector. A language model holds an embedding vector associated with each token in its tokenizer. Figure 17. A language model holds an embedding vector associated with each token in its tokenizer. A language model operates on raw, static embeddings as its input and produces contextual text embeddings. Figure 18. A language model operates on raw, static embeddings as its input and produces contextual text embeddings. from transformers import AutoModel, AutoTokenizer # load a tokenizer tokenizer = AutoTokenizer.from_pretrained(&#39;microsoft/deberta-base&#39;) # load a language model model = AutoModel.from_pretrained(&#39;microsoft/deberta-v3-xsmall&#39;) # tokenize the sentence: convert text to token IDs tokens = tokenizer(&#39;Hello world&#39;, return_tensors=&#39;pt&#39;) # print the decoded tokens to show tokenization for token_id in tokens[&#39;input_ids&#39;][0]: print(tokenizer.decode(token_id)) print(&#39;\\n&#39;) # process the token IDs through the model to get contextualized embeddings output = model(**tokens)[0] # show the shape of the embedding result print(f&#39;{output.shape}\\n&#39;) # output contains the contextualized embedding vectors print(output) [CLS] Hello world [SEP] torch.Size([1, 4, 384]) tensor([[[-3.4816, 0.0861, -0.1819, ..., -0.0612, -0.3911, 0.3017], [ 0.1898, 0.3208, -0.2315, ..., 0.3714, 0.2478, 0.8048], [ 0.2071, 0.5036, -0.0485, ..., 1.2175, -0.2292, 0.8582], [-3.4278, 0.0645, -0.1427, ..., 0.0658, -0.4367, 0.3834]]], grad_fn=&lt;NativeLayerNormBackward0&gt;) 2.3. Text Embeddings Text embeddings are single, dense vectors that represent the semantic meaning of entire sentences, paragraphs, or documents, in contrast to token embeddings, which represent individual words or subwords. from sentence_transformers import SentenceTransformer # load model model = SentenceTransformer(&#39;sentence-transformers/all-MiniLM-L6-v2&#39;) # convert text to text embeddings embeddings = model.encode(&quot;Best movie ever!&quot;) print(embeddings.shape) # (384,) Input Sequence Length: https://www.sbert.net/ For transformer models like BERT, RoBERTa, DistilBERT etc., the runtime and memory requirement grows quadratic with the input length. This limits transformers to inputs of certain lengths. A common value for BERT-based models are 512 tokens, which corresponds to about 300-400 words (for English). Each model has a maximum sequence length under model.max_seq_length, which is the maximal number of tokens that can be processed. Longer texts will be truncated to the first model.max_seq_length tokens: from sentence_transformers import SentenceTransformer model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;) print(&quot;Max Sequence Length:&quot;, model.max_seq_length) # =&gt; Max Sequence Length: 256 # Change the length to 200 model.max_seq_length = 200 print(&quot;Max Sequence Length:&quot;, model.max_seq_length) # =&gt; Max Sequence Length: 200 3. Large Language Models import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # create a pipeline generator = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=50, do_sample=False, ) 3.1. Inputs and Outputs The most common picture of understanding the behavior of a Transformer LLM is to think of it as a software system that takes in text and generates text in response. Once a large enough text-in-text-out model is trained on a large enough high-quality dataset, it becomes able to generate impressive and useful outputs. Figure 19. At a high level of abstraction, Transformer LLMs take a text prompt and output generated text. The model does not generate the text all in one operation; it actually generates one token at a time. Figure 20. Transformer LLMs generate one token at a time, not the entire text at once. Each token generation step is one forward pass through the model (that’s machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph). After each token generation, the input prompt for the next generation step is tweaked by appending the output token to the end of the input prompt. Figure 21. An output token is appended to the prompt, then this new text is presented to the model again for another forward pass to generate the next token. Text generation LLMs are called autoregressive models because they generate text sequentially, using prior outputs as input, unlike text representation models like BERT, which process the entire input at once. 3.2. Components A language model consists of a tokenizer, a stack of Transformer blocks for processing, and an LM head that converts the processed information into probability scores for the next token. Figure 22. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head. The model has a vector representation associated with each of these tokens in the vocabulary (token embeddings). Figure 23. The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings. For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probability distribution for the next token. Figure 24. At the end of the forward pass, the model predicts a probability score for each token in the vocabulary. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer MODEL_NAME = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) print(model) Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(200064, 3072, padding_idx=199999) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear(in_features=3072, out_features=3072, bias=False) (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False) ) (mlp): Phi3MLP( (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False) (down_proj): Linear(in_features=8192, out_features=3072, bias=False) (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (resid_attn_dropout): Dropout(p=0.0, inplace=False) (resid_mlp_dropout): Dropout(p=0.0, inplace=False) ) ) (norm): Phi3RMSNorm((3072,), eps=1e-05) (rotary_emb): Phi3RotaryEmbedding() ) (lm_head): Linear(in_features=3072, out_features=200064, bias=False) ) 3.3. Probability Distribution (Sampling/Decoding) Language models use a probability distribution to determine the next token, which is called the decoding strategy. The easiest strategy would be to always pick the token with the highest probability score, which is called greedy decoding (equivalent to setting the temperature to zero in an LLM). In practice, this doesn’t tend to lead to the best outputs for most use cases. A better approach is to introduce randomness by sampling from the probability distribution, sometimes choosing the second or third highest probability token. 3.4. Parallel Token Processing and Context Size Transformers excel at parallel processing, unlike earlier architectures, which is evident in how they handle token generation. Each input token is processed simultaneously through its own computation path or stream. Figure 25. Each token is processed through its own stream of computation (with some interaction between them in attention steps). A model with 4K context length or context size can only process 4K tokens and would only have 4K of these streams. Each of the token streams starts with an input vector (the embedding vector and some positional information). Figure 26. Each processing stream takes a vector as input and produces a final resulting vector of the same size (often referred to as the model dimension). At the end of the stream, another vector emerges as the result of the model’s processing. For text generation, only the output result of the last stream is used to predict the next token. That output vector is the only input into the LM head as it calculates the probabilities of the next token. 3.5. Keys and Values Caching Transformer models use a key/value (KV) cache to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), speeding up text generation by avoiding redundant calculations. Figure 27. When generating text, it’s important to cache the computation results of previous tokens instead of repeating the same calculation over and over again. In Hugging Face Transformers, cache is enabled by default, and can be disabled it by setting use_cache to False. prompt = &#39;Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&#39; input_ids = tokenizer(prompt, return_tensors=&#39;pt&#39;).input_ids.to(dev) generation_output = model.generate( input_ids=input_ids, max_new_tokens=100, use_cache=False, ) 3.6. Transformer Block Transformer LLMs are composed of a series Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs) and each block processes its inputs, then passes the results of its processing to the next block. Figure 28. The bulk of the Transformer LLM processing happens inside a series of Transformer blocks, each handing the result of its processing as input to the subsequent block. A Transformer block is made up of two successive components: Figure 29. A Transformer block is made up of a self-attention layer and a feedforward neural network. The attention layer is mainly concerned with incorporating relevant information from other input tokens and positions The feedforward layer houses the majority of the model’s processing capacity The feedforward network in a Transformer model stores learned information, such as &#39;The Shawshank&#39; and &#39;Redemption,&#39; and enables interpolation and generalization for generating text on unseen inputs. Figure 30. The feedforward neural network component of a Transformer block likely does the majority of the model’s memorization and interpolation. The attention layer in a Transformer model enables context awareness, crucial for language understanding beyond simple memorization. Figure 31. The self-attention layer incorporates relevant information from previous positions that help process the current token. 4. Text Classification A common task in natural language processing is classification, where the goal is to train a model to assign a label or class to input text, a technique widely used in applications like sentiment analysis and intent detection, significantly impacted by both representative and generative language models. Figure 32. Although both representation and generative models can be used for classification, their approaches differ. The Hugging Face Hub is a collaborative platform for machine learning resources (models, datasets, applications), and the datasets package can be used to load datasets. The dataset is split into train (for training), test (for final evaluation), and validation (for intermediate generalization checks, especially during hyperparameter tuning). from datasets import load_dataset # load data data = load_dataset(&quot;rotten_tomatoes&quot;) # the well-known &#39;rotten_tomatoes&#39; dataset data DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 8530 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 1066 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 1066 }) }) 4.1. Representation Models Classification with pretrained representation models generally comes in two flavors, either using a task-specific model or an embedding model. Figure 33. A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings. A task-specific model is a representation model, such as BERT, trained for a specific task, like sentiment analysis. An embedding model generates general-purpose embeddings that can be used for a variety of tasks not limited to classification, like semantic search. Figure 34. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings. 4.1.1. Task-Specific Model from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&quot;rotten_tomatoes&quot;) # determine the device to use for computation (GPU if available, otherwise CPU) import torch dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; from transformers import pipeline # specify the path to the pre-trained Twitter-RoBERTa-base for Sentiment Analysis model model_path = &quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot; # load the pre-trained sentiment analysis model into a pipeline for easy inference pipe = pipeline( model=model_path, tokenizer=model_path, return_all_scores=True, # return the scores for all sentiment labels device=dev, # specify the device to run the pipeline on ) import numpy as np from tqdm import tqdm # for progress bar during inference from transformers.pipelines.pt_utils import KeyDataset # utility to feed data to the pipeline # run inference on the test dataset y_pred = [] # list to store the predicted sentiment labels for output in tqdm( # iterate through the &#39;text&#39; column of the test dataset pipe(KeyDataset(data[&quot;test&quot;], &quot;text&quot;)), total=len(data[&quot;test&quot;]) ): # extract the negative sentiment score negative_score = output[0][&quot;score&quot;] # extract the positive sentiment score (assuming labels are ordered: negative, neutral, positive) positive_score = output[2][&quot;score&quot;] # predict the sentiment based on the highest score (0 for negative, 1 for positive) assignment = np.argmax([negative_score, positive_score]) # add the predicted label to the list y_pred.append(assignment) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&quot;Negative Review&quot;, &quot;Positive Review&quot;] ) print(performance) # evaluate the performance of the sentiment analysis model on the test set evaluate_performance(data[&quot;test&quot;][&quot;label&quot;], y_pred) # compare the true labels with the predicted labels precision recall f1-score support Negative Review 0.76 0.88 0.81 533 Positive Review 0.86 0.72 0.78 533 accuracy 0.80 1066 macro avg 0.81 0.80 0.80 1066 weighted avg 0.81 0.80 0.80 1066 The above generated classification report shows four such methods: precision, recall, accuracy, and the F1 score. Precision measures how many of the items found are relevant, which indicates the accuracy of the relevant results. Recall refers to how many relevant classes were found, which indicates its ability to find all relevant results. Accuracy refers to how many correct predictions the model makes out of all predictions, which indicates the overall correctness of the model. The F1 score balances both precision and recall to create a model’s overall performance. A confusion matrix visualizes the performance of a classification model by showing the counts of four prediction outcomes: True Positives, True Negatives, False Positives, and False Negatives, which serves as the basis for calculating various metrics to evaluate the model&#8217;s quality. Figure 35. The confusion matrix describes four types of predictions. Figure 36. The classification report describes several metrics for evaluating a model’s performance. 4.1.2. Embedding model Without fine-tuning a representation model, a general-purpose embedding model can generate features that are then fed into a separate, trainable classifier (like logistic regression, which can be trained efficiently on a CPU), creating a two-step classification approach. A major benefit of this separation is avoiding the costly fine-tuning of the embedding model, instead, a classifier, such as logistic regression, can be trained efficiently on the CPU. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&quot;rotten_tomatoes&quot;) # load the SentenceTransformer model for generating text embeddings from sentence_transformers import SentenceTransformer model = SentenceTransformer(&quot;sentence-transformers/all-mpnet-base-v2&quot;) # convert the text data from the train and test splits into embeddings train_embeddings = model.encode(data[&quot;train&quot;][&quot;text&quot;], show_progress_bar=True) test_embeddings = model.encode(data[&quot;test&quot;][&quot;text&quot;], show_progress_bar=True) from sklearn.linear_model import LogisticRegression # train a logistic regression classifier on the generated training embeddings # initialize the logistic regression model with a random state for reproducibility clf = LogisticRegression(random_state=42) # train the classifier using the training embeddings and their corresponding labels clf.fit(train_embeddings, data[&quot;train&quot;][&quot;label&quot;]) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&quot;Negative Review&quot;, &quot;Positive Review&quot;] ) print(performance) # predict the sentiment labels for the test embeddings using the trained classifier y_pred = clf.predict(test_embeddings) # evaluate the performance of the classifier on the test set evaluate_performance(data[&quot;test&quot;][&quot;label&quot;], y_pred) precision recall f1-score support Negative Review 0.85 0.86 0.85 533 Positive Review 0.86 0.85 0.85 533 accuracy 0.85 1066 macro avg 0.85 0.85 0.85 1066 weighted avg 0.85 0.85 0.85 1066 Zero-shot classification can be used on unlabeled data by leveraging the model&#8217;s pre-existing knowledge to predict labels based solely on their definitions. In zero-shot classification, without any labeled examples, the model determines the relationship between input text and predefined candidate labels. Figure 37. In zero-shot classification, we have no labeled data, only the labels them‐ selves. The zero-shot model decides how the input is related to the candidate labels. Zero-shot classification generates target labels without labeled data by describing and embedding labels (e.g., &quot;negative movie review&quot;) and documents. Figure 38. To embed the labels, we first need to give them a description, such as “a negative movie review.” This can then be embedded through sentence-transformers. To assign labels to documents in zero-shot classification, cosine similarity, representing the cosine of the angle between the embedding vectors, can be applied to document-label embedding pairs. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) from sentence_transformers import SentenceTransformer # load model model = SentenceTransformer(&#39;sentence-transformers/all-mpnet-base-v2&#39;) # convert text to embeddings train_embeddings = model.encode(data[&#39;train&#39;][&#39;text&#39;], show_progress_bar=True) test_embeddings = model.encode(data[&#39;test&#39;][&#39;text&#39;], show_progress_bar=True) # create embeddings for our labels label_embeddings = model.encode([&#39;A negative review&#39;, &#39;A positive review&#39;]) import numpy as np from sklearn.metrics.pairwise import cosine_similarity # find the best matching label for each document using cosine similarity sim_matrix = cosine_similarity(test_embeddings, label_embeddings) # get the index of the label with the highest similarity score for each test embedding y_pred = np.argmax(sim_matrix, axis=1) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) precision recall f1-score support Negative Review 0.78 0.77 0.78 533 Positive Review 0.77 0.79 0.78 533 accuracy 0.78 1066 macro avg 0.78 0.78 0.78 1066 weighted avg 0.78 0.78 0.78 1066 From Wikipedia, the free encyclopedia In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval [−1, 1]. import numpy as np # import the NumPy library for numerical operations A = np.array([1, 2, 3]) # create a NumPy array named A B = np.array([4, 5, 6]) # create a NumPy array named B # calculate the cosine similarity using the formula: (A dot B) / (||A|| * ||B||) dot_product = np.dot(A, B) # calculate the dot product of A and B norm_A = np.linalg.norm(A) # calculate the Euclidean norm (magnitude) of A norm_B = np.linalg.norm(B) # calculate the Euclidean norm (magnitude) of B cosine_similarity = dot_product / (norm_A * norm_B) # calculate the cosine similarity print(cosine_similarity) # 0.9746318461970762 4.2. Generative Models Text classification with generative language models (like GPT) involves feeding input text to the model and having it generate text as output, in contrast to task-specific models that directly output a class label. Figure 39. A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens. Generative models are generally trained on a wide variety of tasks and usually don&#8217;t inherently know how to handle specific tasks like classifying a movie review without explicit instructions. Prompt engineering is the skill of crafting effective instructions, or prompts, to guide generative AI models towards producing desired and high-quality outputs for specific tasks, like text classification, which often involves iterative refinement of these prompts based on the model&#8217;s responses. Figure 40. Prompt engineering allows prompts to be updated to improve the output generated by the model. 4.2.1. Text-to-Text Transfer Transformer Text-to-Text Transfer Transformer or T5, like the original Transformer, is a generative encoder-decoder sequence-to-sequence model, contrasting with encoder-only BERT and decoder-only GPT. Figure 41. The T5 architecture is similar to the original Transformer model, a decoder- encoder architecture. In the first step of training, namely pretraining, encoder-decoder models like T5 are initially trained with a masked language modeling objective that masks sets of tokens (or token spans), differing from BERT&#8217;s individual token masking approach. Figure 42. In the first step of training, namely pretraining, the T5 model needs to predict masks that could contain multiple tokens. In the second step of training, namely fine-tuning the base model, instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously. Figure 43. By converting specific tasks to textual instructions, the T5 model can be trained on a variety of tasks during fine-tuning. from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) import torch # determine the device to use for computation (GPU if available, otherwise CPU) dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; from transformers import pipeline # specify the path to the pre-trained FLAN-T5-small model for text-to-text generation model_path = &#39;google/flan-t5-small&#39; # load the pre-trained text-to-text generation model into a pipeline for easy inference pipe = pipeline( &#39;text2text-generation&#39;, model=model_path, device=dev, ) # prepare our data by creating a prompt and combining it with the text prompt = &#39;Is the following sentence positive or negative? &#39; # apply the prompt to each example in the dataset&#39;s &#39;text&#39; column to create a new &#39;t5&#39; column data = data.map(lambda example: {&#39;t5&#39;: prompt + example[&#39;text&#39;]}) # data # uncomment to inspect the modified dataset from tqdm import tqdm # for progress bar during inference from transformers.pipelines.pt_utils import ( KeyDataset, ) # utility to feed data to the pipeline # Run inference y_pred = [] # iterate through the test dataset using the pipeline for text generation for output in tqdm( pipe(KeyDataset(data[&#39;test&#39;], &#39;t5&#39;)), total=len(data[&#39;test&#39;]) ): # extract the generated text from the pipeline&#39;s output text = output[0][&#39;generated_text&#39;] # classify the generated text as 0 (negative) if it equals &#39;negative&#39;, otherwise 1 (positive) y_pred.append(0 if text == &#39;negative&#39; else 1) from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) # evaluate the performance of the model by comparing the true labels with the predicted labels evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) precision recall f1-score support Negative Review 0.83 0.85 0.84 533 Positive Review 0.85 0.83 0.84 533 accuracy 0.84 1066 macro avg 0.84 0.84 0.84 1066 weighted avg 0.84 0.84 0.84 1066 4.2.2. ChatGPT for Classification OpenAI shared an overview of the training procedure that involved an important component, namely preference tuning. OpenAI first manually created the desired output to an input prompt (instruction data) and used that data to create a first variant of its model. Figure 44. Manually labeled data consisting of an instruction (prompt) and output was used to perform fine-tuning (instruction-tuning). OpenAI used the resulting model to generate multiple outputs that were manually ranked from best to worst. Figure 45. Manually ranked preference data was used to generate the final model, ChatGPT. import openai # create client for interacting with OpenAI API client = openai.OpenAI(api_key=&#39;YOUR_KEY_HERE&#39;) def chatgpt_generation(prompt, document, model=&#39;gpt-3.5-turbo-0125&#39;): &#39;&#39;&#39;Generate an output based on a prompt and an input document using ChatGPT.&#39;&#39;&#39; # define the message structure for the OpenAI API messages = [ {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;}, {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: prompt.replace(&#39;[DOCUMENT]&#39;, document)}, ] # call the OpenAI Chat Completions API to get a response chat_completion = client.chat.completions.create( messages=messages, model=model, temperature=0 # temperature=0 for deterministic output ) # return the content of the first choice&#39;s message return chat_completion.choices[0].message.content # define a prompt template as a base for sentiment classification prompt = &#39;&#39;&#39;Predict whether the following document is a positive or negative movie review: [DOCUMENT] If it is positive return 1 and if it is negative return 0. Do not give any other answers. &#39;&#39;&#39; # predict the target for a single document using GPT document = &#39;unpretentious , charming , quirky , original&#39; chatgpt_generation(prompt, document) from datasets import load_dataset # load the well-known &#39;rotten_tomatoes&#39; dataset for sentiment analysis data = load_dataset(&#39;rotten_tomatoes&#39;) from tqdm import tqdm # generate predictions for all documents in the test set predictions = [ chatgpt_generation(prompt, doc) for doc in tqdm(data[&#39;test&#39;][&#39;text&#39;]) ] # convert the string predictions (&#39;0&#39; or &#39;1&#39;) to integers y_pred = [int(pred) for pred in predictions] from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): &#39;&#39;&#39;Create and print the classification report comparing true and predicted labels&#39;&#39;&#39; performance = classification_report( y_true, y_pred, target_names=[&#39;Negative Review&#39;, &#39;Positive Review&#39;] ) print(performance) # evaluate the performance of ChatGPT on the test set evaluate_performance(data[&#39;test&#39;][&#39;label&#39;], y_pred) 5. Text Clustering and Topic Modeling Although supervised techniques, such as classification, have reigned supreme over the last few years in the industry, the potential of unsupervised techniques such as text clustering cannot be understated. Text clustering aims to group similar texts based on their semantic content, meaning, and relationships. Figure 46. Clustering unstructured textual data. Text clustering is also applied in topic modeling to uncover abstract topics within large textual datasets. Figure 47. Topic modeling is a way to give meaning to clusters of textual documents. 5.1. ArXiv’s Articles: Computation and Language ArXiv is an open-access platform for scholarly articles, mostly in the fields of computer science, mathematics, and physics. from datasets import load_dataset # load the &#39;arxiv_nlp&#39; dataset from Hugging Face Datasets library dataset = load_dataset(&quot;maartengr/arxiv_nlp&quot;)[&quot;train&quot;] # extract metadata abstracts = dataset[&quot;Abstracts&quot;] titles = dataset[&quot;Titles&quot;] 5.2. A Common Pipeline for Text Clustering Text clustering enables the discovery of both known and unknown data patterns, providing an intuitive understanding of tasks like classification and their complexity, making it valuable beyond just exploratory data analysis. Although there are many methods for text clustering, from graph-based neural networks to centroid-based clustering techniques, a common pipeline that has gained popularity involves three steps and algorithms: Convert the input documents to embeddings with an embedding model. Figure 48. Step 1: We convert documents to embeddings using an embedding model. Reduce the dimensionality of embeddings with a dimensionality reduction model. Figure 49. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction. Find groups of semantically similar documents with a cluster model. Figure 50. Step 3: We cluster the documents using the embeddings with reduced dimensionality. 5.2.1. Embedding Documents from sentence_transformers import SentenceTransformer # create an embedding model using a pre-trained Sentence Transformer model embedding_model = SentenceTransformer(&#39;thenlper/gte-small&#39;) (1) # generate embeddings for each abstract in the &#39;abstracts&#39; list embeddings = embedding_model.encode(abstracts, show_progress_bar=True) # check the dimensions (shape) of the resulting embeddings embeddings.shape # (44949, 384) (2) 1 The thenlper/gte-small model is a more recent model that outperforms the previous model on clustering tasks and due to its small size is even faster for inference. 2 The embeddings.shape of (44949, 384) shows that there are 44,949 abstract embeddings, each with a dimensionality of 384. 5.2.2. Reducing the Dimensionality of Embeddings Reducing the dimensionality of embeddings is essential before clustering high-dimensional data to simplify the representation and enhance clustering effectiveness. Dimensionality reduction is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions. Figure 51. Dimensionality reduction allows data in high-dimensional space to be compressed to a lower-dimensional representation. Well-known methods for dimensionality reduction are Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP). from umap import UMAP # reduce the input embeddings from 384 dimensions to 5 dimensions using UMAP umap_model = UMAP( # generally, values between 5 and 10 work well to capture high-dimensional global structures. n_components=5, # the number of dimensions to reduce to min_dist=0.0, # the effective minimum distance between embedded points metric=&#39;cosine&#39;, # the metric to use to compute distances in high dimensional space random_state=42, # for reproducibility of the embedding ) # fit and then transform the embeddings to the lower-dimensional space reduced_embeddings = umap_model.fit_transform(embeddings) 5.2.3. Cluster the Reduced Embeddings While k-means, a centroid-based algorithm needing a predefined number of clusters, is common, density-based algorithms are preferable when the number of clusters is unknown as they automatically determine the clusters and don&#8217;t require all data points to belong to one. Figure 52. The clustering algorithm not only impacts how clusters are generated but also how they are viewed. A common density-based model is Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). from hdbscan import HDBSCAN # initialize and fit the HDBSCAN clustering model hdbscan_model = HDBSCAN( # the minimum number of samples in a group for it to be considered a cluster min_cluster_size=50, # the metric to use when calculating pairwise distances between data points metric=&#39;euclidean&#39;, # the method used to select clusters from the hierarchy (&#39;eom&#39; stands for Excess of Mass) cluster_selection_method=&#39;eom&#39; ).fit(reduced_embeddings) # fit the HDBSCAN model to the reduced dimensionality embeddings # extract the cluster labels assigned to each data point (-1 indicates noise) clusters = hdbscan_model.labels_ # How many clusters did we generate? (excluding the noise cluster labeled -1) num_clusters = len(set(clusters)) - (1 if -1 in clusters else 0) 5.2.4. Inspecting the Clusters To inspect each cluster manually and explore the assigned documents to get an understanding of its content. import numpy as np # print first three documents in cluster 0 cluster = 0 for index in np.where(clusters == cluster)[0][:3]: print(abstracts[index][:300] + &quot;... \\n&quot;) To visualize clustering approximation results without manual review, further reduce document embeddings to two dimensions for plotting on an 2D plane. import pandas as pd from umap import UMAP import matplotlib.pyplot as plt # reduce 384-dimensional embeddings to two dimensions for easier visualization reduced_embeddings = UMAP( n_components=2, min_dist=0.0, metric=&quot;cosine&quot;, random_state=42, ).fit_transform(embeddings) # create dataframe df = pd.DataFrame(reduced_embeddings, columns=[&quot;x&quot;, &quot;y&quot;]) df[&quot;title&quot;] = titles df[&quot;cluster&quot;] = [str(c) for c in clusters] # select outliers (cluster -1) and non-outliers (clusters) to_plot = df.loc[df.cluster != &quot;-1&quot;, :] outliers = df.loc[df.cluster == &quot;-1&quot;, :] # plot outliers and non-outliers separately plt.scatter(outliers.x, outliers.y, alpha=0.05, s=2, c=&quot;grey&quot;, label=&quot;Outliers&quot;) plt.scatter( to_plot.x, to_plot.y, c=to_plot.cluster.astype(int), alpha=0.6, s=2, cmap=&quot;tab20b&quot;, label=&quot;Clusters&quot;, ) plt.axis(&quot;off&quot;) plt.legend() # Add a legend to distinguish outliers and clusters plt.title(&quot;Visualization of Clustered Abstracts&quot;) # Add a title for context plt.show() Figure 53. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization. 5.3. From Text Clustering to Topic Modeling Text clustering is a powerful tool for finding structure among large collections of documents, whereas topic modeling is the process of discovering underlying themes or latent topics within a collection of textual data, which typically involves finding a set of keywords or phrases that best represent and capture the meaning of the topic. Figure 54. Traditionally, topics are represented by a number of keywords but can take other forms. Instead of labeling a topic as “sign language,” these techniques use keywords such as “sign,” “language,” and “translation” to describe the topic. As such, this does not give a single label to a topic and instead requires the user to understand the meaning of the topic through those keywords. 5.3.1. BERTopic: A Modular Topic Modeling Framework BERTopic is a topic modeling technique that leverages clusters of semantically similar texts to extract various types of topic representations. Figure 55. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation. First, similar to text clustering, it embeds documents, reduces their dimensionality, and then clusters these embeddings to group semantically similar texts. .The first part of BERTopic’s pipeline is to create clusters of semantically similar documents. Second, it models word distributions using a bag-of-words approach, counting word frequencies within documents to help extract the most frequent terms. The bag-of-words approach does exactly what its name implies: it counts the number of times each word appears in a document, which can then be used to extract the most frequent words within that document. Figure 56. A bag-of-words counts the number of times each word appears inside a document. Figure 57. Generating c-TF by counting the frequency of words per cluster instead of per document. 6. Prompt Engineering Prompt engineering is the art and science of crafting effective prompts to guide large language models (LLMs) and other generative AI systems to produce desired and high-quality outputs. It involves understanding how these models interpret and respond to different phrasings, instructions, and contexts within a prompt to achieve specific goals, such as generating creative text, answering questions accurately, or performing tasks effectively. 6.1. Using Text Generation Models import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer model_path = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( model_path, device_map=dev, torch_dtype=&#39;auto&#39;, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_path) # create a pipeline pipe = pipeline( &#39;text-generation&#39;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False, ) # prompt messages = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Create a funny joke about chickens.&#39;}] # generate the output output = pipe(messages) print(output[0][&#39;generated_text&#39;]) 6.1.1. Prompt Template Under the hood, transformers.pipeline first converts the messages into a specific prompt template which was used during the training of the model. # apply prompt template prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False) print(prompt) &lt;s&gt;&lt;|user|&gt; Create a funny joke about chickens.&lt;|end|&gt; &lt;|assistant|&gt; Figure 58. The template Phi-3 expects when interacting with the model. 6.1.2. Controlling Model Output Each time an LLM needs to generate a token, it assigns a likelihood number to each possible token to generate different responses for the exact same prompt. Figure 59. The model chooses the next token to generate based on their likelihood scores. The temperature controls the randomness or creativity of the text generated; a higher temperature increases creativity by making less probable tokens more likely, while a temperature of 0 results in deterministic output by always selecting the most probable token. # using a high temperature output = pipe(messages, do_sample=True, temperature=1) print(output[0][&quot;generated_text&quot;]) Figure 60. A higher temperature increases the likelihood that less probable tokens are generated and vice versa. The top-p, or nucleus sampling, is a technique that controls the subset of tokens (the nucleus) an LLM considers for generation by including tokens until their cumulative probability reaches a specified threshold. For instance, if top_p is set to 0.1, the model will consider tokens until their cumulative probability reaches 10%, and if top_p is set to 1, all tokens will be considered. # using a high top_p output = pipe(messages, do_sample=True, top_p=1) print(output[0][&quot;generated_text&quot;]) Figure 61. A higher top_p increases the number of tokens that can be selected to generate and vice versa. The top_k parameter directly limits the number of most probable tokens an LLM considers; setting it to 100 restricts the selection to only the top 100 tokens. Table 1. Use case examples when selecting values for temperature and top_p. Example use case temperature top_p Description Brainstorming session High High High randomness with large pool of potential tokens. The results will be highly diverse, often leading to very creative and unexpected results. Email generation Low Low Deterministic output with high probable predicted tokens. This results in predictable, focused, and conservative outputs. Creative writing High Low High randomness with a small pool of potential tokens. This combination produces creative outputs but still remains coherent. Translation Low High Deterministic output with high probable predicted tokens. Produces coherent output with a wider range of vocabulary, leading to outputs with linguistic variety. 6.2. Prompt Engineering Prompt engineering is the iterative process of designing effective prompts, including questions, statements, or instructions, to elicit useful and relevant outputs from LLMs through experimentation and optimization. A prompt is the input provided to a large language model to elicit a desired response, which generally consists of multiple components such as instructions, data, and output indicators, and can be as complex as needed. Figure 62. A basic example of a prompt. No instruction is given so the LLM will simply try to complete the sentence. Figure 63. Two components of a basic instruction prompt: the instruction itself and the data it refers to. Figure 64. Extending the prompt with an output indicator that allows for a specific output. 6.3. Instruction-Based Prompting Instruction-based prompting is a method of prompting where the primary goal is to have the LLM answer a specific question or resolve a certain task by providing it with specific instructions. Figure 65. Prompt examples of common use cases. Notice how within a use case, the structure and location of the instruction can be changed. Each of these tasks requires different prompting formats and more specifically, asking different questions of the LLM. A non-exhaustive list of the prompting techniques includes: Specificity Accurately describe the desired output, for example, instead of &quot;Write a product description,&quot; ask &quot;Write a product description in under two sentences using a formal tone.&quot; Specificity is arguably the most important aspect; by restricting and specifying what the model should generate, there is a smaller chance of it generating something unrelated to a use case. Hallucination LLMs may generate incorrect information confidently, which is referred to as hallucination. To reduce its impact, ask the LLM to only generate an answer if it knows the answer, and to respond with &quot;I don’t know&quot; if it does not know the answer. Order Either begin or end the prompt with the instruction. Especially with long prompts, information in the middle is often forgotten. LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect). 6.4. Advanced Prompt Engineering While creating a good prompt might initially seem straightforward—just ask a specific question, be accurate, and add examples—prompting can quickly become complex and is often an underestimated aspect of effectively using LLMs. 6.4.1. Prompt Components A prompt generally consists of multiple components, such as instruction, data, and output indicators, and other advanced components that can quickly make a prompt quite complex. Figure 66. An example of a complex prompt with many components. Figure 67. Iterating over modular components is a vital part of prompt engineering. # prompt components persona = &#39;You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.\\n&#39; instruction = &#39;Summarize the key findings of the paper provided.\\n&#39; context = &#39;Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.\\n&#39; data_format = &#39;Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.\\n&#39; audience = &#39;The summary is designed for busy researchers that quickly need to grasp the newest trends in Large Language Models.\\n&#39; tone = &#39;The tone should be professional and clear.\\n&#39; text = &#39;MY TEXT TO SUMMARIZE&#39; data = f&#39;Text to summarize: {text}&#39; # the full prompt - remove and add pieces to view its impact on the generated output query = persona + instruction + context + data_format + audience + tone + data 6.4.2. In-Context Learning: Providing Examples In-context learning (ICL) is a prompting technique that demonstrates the desired task to an LLM through direct examples, rather than solely describing it to provide the model with context to learn from within the prompt. Zero-shot prompting does not leverage examples, one-shot prompts use a single example, and few-shot prompts use two or more examples. Figure 68. An example of a complex prompt with many components. # use a single example of using the made-up word in a sentence one_shot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;A \\&#39;Gigamuru\\&#39; is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:&#39;, }, { &#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.&#39;, }, { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;To \\&#39;screeg\\&#39; something is to swing a sword at it. An example of a sentence that uses the word screeg is:&#39;, }, ] print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False)) &lt;|user|&gt;A &#39;Gigamuru&#39; is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:&lt;|end|&gt;&lt;|assistant|&gt;I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.&lt;|end|&gt;&lt;|user|&gt;To &#39;screeg&#39; something is to swing a sword at it. An example of a sentence that uses the word screeg is:&lt;|end|&gt;&lt;|endoftext|&gt; # generate the output outputs = pipe(one_shot_prompt) print(outputs[0][&quot;generated_text&quot;]) In the medieval fantasy novel, the knight would screeg his enemies with his gleaming sword. 6.4.3. Chain Prompting: Breaking up the Problem Prompt chaining is a technique that addresses complex tasks by breaking them down across multiple prompts, where the output of one prompt serves as the input for the subsequent prompt, creating a sequence of interactions that collectively solve the problem. Figure 69. Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch. # create name and slogan for a product product_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a name and slogan for a chatbot that leverages LLMs.&quot;, } ] outputs = pipe(product_prompt) product_description = outputs[0][&quot;generated_text&quot;] print(product_description) # based on a name and slogan for a product, generate a sales pitch sales_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Generate a very short sales pitch for the following product: &#39;{product_description}&#39;&quot;, } ] outputs = pipe(sales_prompt) sales_pitch = outputs[0][&quot;generated_text&quot;] print(sales_pitch) Name: LexiBot Slogan: &quot;Unlock the Power of Language with LexiBot – Your AI Conversation Partner!&quot; Discover the future of communication with LexiBot – your AI conversation partner. Say goodbye to language barriers and hello to seamless, intelligent interactions. LexiBot is here to unlock the power of language, making every conversation more engaging and productive. Embrace the power of AI with LexiBot today! 6.5. Reasoning with Generative Models Reasoning is a core component of human intelligence and is often compared to the emergent behavior of LLMs that often resembles reasoning (through memorization of training data and pattern matching, rather than true reasoning). Human reasoning can be broadly categorized into two systems. System 1 thinking represents an automatic, intuitive, and near-instantaneous process, which shares similarities with generative models that automatically generate tokens without any self-reflective behavior. System 2 thinking, in contrast, is a conscious, slow, and logical process, akin to brainstorming and self-reflection. The system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking, would be emulated by giving a generative model the ability to mimic a form of self-reflection. 6.5.1. Chain-of-Thought: Think Before Answering Chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps (&quot;thoughts&quot;) before giving a final answer. Although chain-of-thought is a great method for enhancing the output of a generative model, it does require one or more examples of reasoning in the prompt, which the user might not have access to. Figure 70. Chain-of-thought prompting uses reasoning examples to persuade the generative model to use reasoning in its answer. # answering with chain-of-thought cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?&quot;, }, { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.&quot;, }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?&quot;, }, ] # generate the output outputs = pipe(cot_prompt) print(outputs[0][&quot;generated_text&quot;]) The cafeteria started with 23 apples. They used 20, so they had 23 - 20 = 3 apples left. Then they bought 6 more, so they now have 3 + 6 = 9 apples. The answer is 9. Instead of providing examples, zero-shot chain-of-thought allows a generative model to provide reasoning without explicit examples by directly prompting it for its thought process. Although the prompt “Let’s think step by step” can improve the output, you are not constrained by this exact formulation. Alterna‐ tives exist like “Take a deep breath and think step-by-step” and “Let’s work through this problem step-by-step.” Figure 71. Chain-of-thought prompting without using examples. Instead, it uses the phrase “Let’s think step-by-step” to prime reasoning in its answer. # zero-shot chain-of-thought prompt zeroshot_cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let&#39;s think step-by-step.&quot;, } ] # generate the output outputs = pipe(zeroshot_cot_prompt) print(outputs[0][&quot;generated_text&quot;]) Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 new apples to the 3 apples they have left: 3 apples + 6 apples = 9 apples. So, the cafeteria now has 9 apples. 6.5.2. Self-Consistency: Sampling Outputs Self-consistency is a technique that reduces randomness in generative models by prompting them multiple times with the same input, using varied sampling parameters like temperature and top_p to enhance diversity, and selecting the majority result as the final answer for robustness. Figure 72. By sampling from multiple reasoning paths, we can use majority voting to extract the most likely answer. # zero-shot chain-of-thought prompt zeroshot_cot_prompt = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let&#39;s think step-by-step.&quot;, } ] # self-consistency settings num_samples = 3 temperature = [0.3, 0.5, 0.7] top_p = [0.8, 0.85, 0.9] # extract final numerical answers def extract_answer(text): numbers = re.findall(r&quot;\\d+&quot;, text) # find all numbers in the output return ( numbers[-1] if numbers else None ) # take the last number as the final answer # generate multiple answers answers = [] for i in range(num_samples): outputs = pipe( zeroshot_cot_prompt, do_sample=True, temperature=temperature[i % len(temperature)], top_p=top_p[i % len(top_p)], ) response = outputs[0][&quot;generated_text&quot;].strip() print(f&#39;\\n{response}&#39; final_answer = extract_answer(response) if final_answer: answers.append(final_answer) # perform majority voting on numerical answers most_common_answer, count = Counter(answers).most_common(1)[0] print(&quot;\\ngenerated answers:&quot;) for i, ans in enumerate(answers, 1): print(f&quot;{i}. {ans}&quot;) print(f&quot;\\nfinal answer (majority vote): {most_common_answer}&quot;) Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 apples to the 3 apples they have left gives: 3 apples + 6 apples = 9 apples. So, the cafeteria Sure, let&#39;s break it down step-by-step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. 3. After using 20 apples, they have: 23 apples - 20 apples = 3 apples left. 4. They then buy 6 more apples. 5. Adding the 6 new apples to the 3 apples they have left, they now have: 3 apples + 6 apples = 9 apples. Sure, let&#39;s break it down step by step: 1. The cafeteria starts with 23 apples. 2. They use 20 apples to make lunch. - 23 apples - 20 apples = 3 apples remaining. 3. They then buy 6 more apples. - 3 apples + 6 apples = 9 apples. So, after these transactions, the cafeteria has 9 apples. generated answers: 1. 9 2. 9 3. 9 final answer (majority vote): 9 6.5.3. Tree-of-Thought: Exploring Intermediate Steps Tree-of-Thought (ToT) is a problem-solving technique structuring reasoning as a decision tree that explores multiple potential solutions at each step, evaluates them, and branches forward with the most promising, similar to brainstorming, to enhance the final outcome. Figure 73. By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned. Tree-of-Thought excels at tasks requiring exploration of multiple paths, such as creative writing, but its reliance on numerous generative model calls can be slow. A more efficient approach involves prompting the model to simulate a multi-expert discussion to reach a consensus, mimicking the ToT framework with a single call. # zero-shot tree-of-thought prompt zeroshot_tot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &quot;Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they&#39;re wrong at any point then they leave. The question is &#39;The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?&#39; Make sure to discuss the results.&quot;, } ] # generate the output outputs = pipe(zeroshot_tot_prompt) print(outputs[0][&#39;generated_text&#39;]) **Expert 1:** Step 1: Start with the initial number of apples, which is 23. **Expert 2:** Step 1: Subtract the apples used for lunch, which is 20, from the initial 23 apples. This leaves 3 apples. **Expert 3:** Step 1: Add the 6 apples that were bought to the remaining 3 apples. This results in 9 apples. **Discussion:** All three experts agree on the final result. The cafeteria started with 23 apples, used 20 for lunch, leaving them with 3 apples. Then, they bought 6 more apples, bringing the total to 9 apples. Therefore, the cafeteria now has 9 apples. 6.6. Output Verification Systems and applications built with generative models might eventually end up in production. When that happens, it is important to verify and control the output of the model to prevent breaking the application and to create a robust generative AI application. By default, most generative models create free-form text without adhering to specific structures other than those defined by natural language. Some use cases require their output to be structured in certain formats, like JSON. Even allowing the model to generate structured output, it still has the capability to freely generate its content. For instance, when a model is asked to output either one of two choices, it should not come up with a third. Some open source generative models have no guardrails and will generate outputs that do not consider safety or ethical considerations. For instance, use cases might require the output to be free of profanity, personally identifiable information (PII), bias, cultural stereotypes, etc. Many use cases require the output to adhere to certain standards or performance. The aim is to double-check whether the generated information is factually accurate, coherent, or free from hallucination. Generally, there are three ways of controlling the output of a generative model: Examples: Provide a number of examples of the expected output. Grammar: Control the token selection process. Fine-tuning: Tune a model on data that contains the expected output. 6.6.1. Providing Examples A simple and straightforward method to fix the output is to provide the generative model with examples of what the output should look like. The few-shot learning is a helpful technique that guides the output of the generative model, which can be generalized to guide the structure of the output as well. An important note here is that it is still up to the model whether it will adhere to your suggested format or not. Some models are better than others at following instructions. # zero-shot learning: providing no in-context examples zeroshot_prompt = [ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Create a character profile for an RPG game in JSON format.&#39;, } ] # generate the output outputs = pipe(zeroshot_prompt) print(outputs[0][&#39;generated_text&#39;]) # one-shot learning: providing a single in-context example of the desired output structure one_shot_template = &#39;&#39;&#39;Create a short character profile for an RPG game. Make sure to only use this format: { &quot;description&quot;: &quot;A SHORT DESCRIPTION&quot;, &quot;name&quot;: &quot;THE CHARACTER&#39;S NAME&quot;, &quot;armor&quot;: &quot;ONE PIECE OF ARMOR&quot;, &quot;weapon&quot;: &quot;ONE OR MORE WEAPONS&quot; } &#39;&#39;&#39; one_shot_prompt = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: one_shot_template}] # generate the output outputs = pipe(one_shot_prompt) print(outputs[0][&#39;generated_text&#39;]) { &quot;name&quot;: &quot;Eldrin Shadowbane&quot;, &quot;class&quot;: &quot;Rogue&quot;, &quot;level&quot;: 10, &quot;race&quot;: &quot;Elf&quot;, &quot;background&quot;: &quot;Eldrin was born into a noble family in the elven city of Luminara. He was trained in the arts of stealth and combat from a young age. However, Eldrin always felt a deep connection to the shadows and the mysteries of the night. He left his family to become a rogue { &quot;description&quot;: &quot;A skilled archer with a mysterious past, known for their agility and precision.&quot;, &quot;name&quot;: &quot;Lyra Swiftarrow&quot;, &quot;armor&quot;: &quot;Leather bracers and a lightweight leather tunic&quot;, &quot;weapon&quot;: &quot;Longbow, throwing knives&quot; } 6.6.2. Grammar: Constrained Sampling Few-shot learning has a significant disadvantage: explicitly preventing certain output is not possible. Although the model is guided and given instructions, it might still not follow them completely. Grammar-constrained sampling is a technique used during the token generation process of a Large Language Model (LLM) that enforces adherence to predefined grammars or rules when selecting the next token. Instead, packages have been rapidly developed to constrain and validate the output of generative models, like Guidance, Guardrails, and LMQL, which leverage generative models to validate their own output. Figure 74. The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails. Figure 75. Use an LLM to generate only the pieces of information we do not know beforehand. Figure 76. Constrain the token selection to only three possible tokens: “positive,” “neutral,” and “negative.” Like transformers, llama-cpp-python is a library, generally used to efficiently load and use compressed models (quantization) in the GGUF format but can also be used to apply a JSON grammar. from llama_cpp.llama import Llama # load the Phi-3 language model using the llama-cpp-python library llm = Llama.from_pretrained( repo_id=&quot;microsoft/Phi-3-mini-4k-instruct-gguf&quot;, filename=&quot;*fp16.gguf&quot;, n_gpu_layers=-1, n_ctx=2048, verbose=False, ) # generate output using the loaded language model for a chat completion task output = llm.create_chat_completion( messages=[ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Create a warrior for an RPG in JSON for mat.&quot;, }, ], response_format={&quot;type&quot;: &quot;json_object&quot;}, # specify the response_format as a JSON temperature=0, )[&#39;choices&#39;][0][&#39;message&#39;][&quot;content&quot;] import json # check whether the output actually is JSON json_output = json.dumps(json.loads(output), indent=4) print(json_output) { &quot;warrior&quot;: { &quot;name&quot;: &quot;Aldarion the Brave&quot;, &quot;class&quot;: &quot;Warrior&quot;, &quot;level&quot;: 10, &quot;attributes&quot;: { &quot;strength&quot;: 18, &quot;dexterity&quot;: 10, &quot;constitution&quot;: 16, &quot;intelligence&quot;: 8, &quot;wisdom&quot;: 10, &quot;charisma&quot;: 12 }, 7. Advanced Text Generation Techniques and Tools LangChain is a framework for developing applications powered by large language models (LLMs), which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. Figure 77. LangChain is a complete framework for using LLMs. It has modular compo‐ nents that can be chained together to allow for complex LLM systems. Hugging Face models can be run locally through the HuggingFacePipeline class. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # determine the device dev = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; # load model and tokenizer model_id = &#39;microsoft/Phi-4-mini-instruct&#39; model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=&#39;auto&#39;, device_map=dev, trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_id) # create a pipeline pipe = pipeline( &quot;text-generation&quot;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=True, ) from langchain_huggingface.llms import HuggingFacePipeline llm = HuggingFacePipeline(pipeline=pipe) 7.1. Model I/O: Loading Quantized Models with LangChain A GGUF model represents a compressed version of its original counterpart through a method called quantization, which reduces the number of bits needed to represent the parameters of an LLM. Figure 78. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the lowered accuracy when we halve the number of bits. Bits, a series of 0s and 1s, represent values through binary encoding; more bits allow for a wider range of values but demand greater memory for storage. Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information. Quantization comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original. Like rounding the time to the nearest minute (&quot;14:16&quot;) instead of including seconds (&quot;14:16 and 12 seconds&quot;), quantization reduces the precision of a value without losing essential information. As a rule of thumb, look for at least 4-bit quantized models. These models have a good balance between compression and accuracy. Although it is possible to use 3-bit or even 2-bit quantized mod‐ els, the performance degradation becomes noticeable and it would instead be preferable to choose a smaller model with a higher precision. To download a specific bit-variant file (e.g., fp16) of the microsoft/Phi-3-mini-4k-instruct-gguf model, which includes multiple files with different bit-variants (see the &#39;Files and versions&#39; tab). # download from the primary Hugging Face URL: wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf # alternatively, download from the HF mirror: wget https://hf-mirror.com/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf Use Llama.cpp together with LangChain to load the GGUF file, and generate output. # !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf # !pip install llama-cpp-python langchain_communit from langchain_community.llms import LlamaCpp # initialize the LlamaCpp language model integration from Langchain llm = LlamaCpp( # path to the downloaded GGUF model file (ensure this file exists!) model_path=&quot;Phi-3-mini-4k-instruct-fp16.gguf&quot;, n_gpu_layers=-1, max_tokens=500, n_ctx=2048, seed=42, verbose=False, ) # invoke the language model with a prompt. output = llm.invoke(&quot;Hi! My name is Maarten. What is 1 + 1?&quot;) # no/meanless output! Phi-3 requires a specific prompt template. print(output) 7.2. Chains: Extending the Capabilities of LLMs In Langchain, a &quot;chain&quot; is a core concept that goes beyond running LLMs in isolation, which involves connecting an LLM with other components like prompts, tools, or even other chains, to enhance its capabilities and create more complex systems. Figure 79. A single chain connects some modular component, like a prompt template or external memory, to the LLM. 7.2.1. A Single Link in the Chain: Prompt Template Figure 80. By chaining a prompt template with an LLM, we only need to define the input prompts. The template will be constructed for you. By chaining a prompt template with an LLM to get the output, only the user and system prompts need to be defined for each interaction, eliminating the need to repeatedly define the full prompt template. Figure 81. An example of a single chain using Phi-3’s template. The template for Phi-3 is comprised of four main components: &lt;s&gt; to indicate when the prompt starts &lt;|user|&gt; to indicate the start of the user’s prompt &lt;|assistant|&gt; to indicate the start of the model’s output &lt;|end|&gt; to indicate the end of either the prompt or the model’s output Figure 82. The prompt template Phi-3 expects. from langchain_core.prompts import PromptTemplate # create a prompt template with a placeholder for the user&#39;s input template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt; {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate( template=template, input_variables=[&quot;input_prompt&quot;], ) # create a simple chain with the prompt template and the language model basic_chain = prompt | llm # invoke the chain with the input for the prompt template output = basic_chain.invoke( { &quot;input_prompt&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;, } ) # the &#39;output&#39; variable now contains the generated text print(output) Hello Maarten! The answer to 1 + 1 is 2. 7.2.2. A Chain with Multiple Prompts Figure 83. With sequential chains, the output of a prompt is used as the input for the next prompt. A multiple prompt chain, or sequential chain, processes a complex task by dividing it into a series of smaller, sequential subtasks, where each subtask utilizes a distinct prompt and LLM call, with the output from one step feeding directly into the input of the subsequent step. Figure 84. An example to generate a story that has three components: a title, a description of the main character, a summary of the story. The output of the title prompt is used as the input of the character prompt. To generate the story, the output of all previous prompts is used. import json from langchain_core.prompts import PromptTemplate from langchain_core.runnables import RunnablePassthrough, RunnableLambda from langchain.schema import StrOutputParser from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&#39;qwen2.5:0.5b-instruct&#39;, temperature=0.7, max_tokens=100, timeout=30, max_retries=2, base_url=&#39;http://localhost:11434/v1&#39;, # Ollama API api_key=&#39;API-KEY&#39;, verbose=True, ) title_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Create a title for a story about {summary}.&quot; &quot;Only return the title.&quot; &quot;&lt;|end|&gt; &lt;|assistant|&gt;&quot; ) character_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Describe the main character of a story about {summary} with the title {title}. &quot; &quot;Use only two sentences.&quot; &quot;&lt;|end|&gt;&lt;|assistant|&gt;&quot; ) story_prompt = PromptTemplate.from_template( &quot;&lt;s&gt;&lt;|user|&gt;&quot; &quot;Create a story about {summary} with the title {title}.&quot; &quot;The main character is: {character}. &quot; &quot;Only return the story and it cannot be longer than one paragraph.&quot; &quot;&lt;|end|&gt;&lt;|assistant|&gt;&quot; ) # LCEL-style chain using Runnables title_chain = ( {&quot;summary&quot;: RunnablePassthrough()} | title_prompt | llm | StrOutputParser() ) character_chain = ( {&quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain} | character_prompt | llm | StrOutputParser() ) story_chain = ( { &quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain, &quot;character&quot;: character_chain, } | story_prompt | llm | StrOutputParser() ) aggregate_chain = RunnableLambda( lambda inputs: { &quot;summary&quot;: inputs[&quot;summary&quot;], &quot;title&quot;: inputs[&quot;title&quot;], &quot;character&quot;: inputs[&quot;character&quot;], &quot;story&quot;: inputs[&quot;story&quot;], } ) final_chain = { &quot;summary&quot;: RunnablePassthrough(), &quot;title&quot;: title_chain, &quot;character&quot;: character_chain, &quot;story&quot;: story_chain, } | aggregate_chain output = final_chain.invoke({&quot;summary&quot;: &quot;a girl that lost her mother&quot;}) print(json.dumps(output, indent=2)) { &quot;summary&quot;: { &quot;summary&quot;: &quot;a girl that lost her mother&quot; }, &quot;title&quot;: &quot;\\&quot;Lost Mother Girl\\&quot;&quot;, &quot;character&quot;: &quot;In the story, the main character named Lily, who was born to an ordinary family, unexpectedly finds herself the daughter of a rich individual after losing her mother. She navigates this new reality with courage and strength, learning valuable lessons about empathy, perseverance, and the power of resilience.&quot;, &quot;story&quot;: &quot;In the quiet village where Linxue lived, her mother had been gone for many years. As an only child, she often felt distant from the other children in the village. One day, 7.3. Memory: Helping LLMs to Remember Conversations Memory can be added to the LLM chain using methods like conversation buffers and conversation summaries to make chat models stateful to remember previous conversations. 7.3.1. Conversation Buffer In Langchain, ConversationBufferMemory provides an intuitive way to give LLMs memory by updating the prompt to include the full chat history. Figure 85. We can remind an LLM of what previously happened by simply appending the entire conversation history to the input prompt. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke({&quot;input&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;}) {&#39;input&#39;: &#39;Hi! My name is Maarten. What is 1 + 1?&#39;, &#39;chat_history&#39;: &#39;&#39;, &#39;text&#39;: &#39;Nice to meet you, Maarten!\\n\\nThe answer to 1 + 1 is... 2!&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) {&#39;input&#39;: &#39;What is my name?&#39;, &#39;chat_history&#39;: &#39;Human: Hi! My name is Maarten. What is 1 + 1?\\nAI: Nice to meet you, Maarten!\\n\\nThe answer to 1 + 1 is... 2!&#39;, &#39;text&#39;: &#39;Nice to meet you too, Maarten! Your name is indeed Maarten. Would you like to ask another question or have a conversation?&#39;} 7.3.2. Windowed Conversation Buffer In LangChain, ConversationBufferWindowMemory decides how many the last k conversations are passed to the input prompt. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=2, memory_key=&quot;chat_history&quot;) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke( input=&quot;Hi! My name is Maarten and I am 33 years old. What is 1 + 1?&quot; ) llm_chain.invoke(input=&quot;What is 3 + 3?&quot;) llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) llm_chain.invoke({&quot;input&quot;: &quot;What is my age?&quot;}) 7.3.3. Conversation Summary In LangChain, ConversationSummaryMemory summarizes the entire conversation history (typically using an external LLM) before providing it to the input prompt. Figure 86. Instead of passing the conversation history directly to the prompt, we use another LLM to summarize it first. from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input}&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; prompt = PromptTemplate.from_template(template) from langchain.memory import ConversationSummaryMemory # prepare a summarization template as the summarization prompt summary_prompt_template = &quot;&quot;&quot;&lt;s&gt;&lt;|user|&gt;Summarize the conversations and update with the new lines. Current summary: {summary} new lines of conversation: {new_lines} New summary:&lt;|end|&gt; &lt;|assistant|&gt;&quot;&quot;&quot; summary_prompt = PromptTemplate.from_template(template=summary_prompt_template) memory = ConversationSummaryMemory( llm=llm, memory_key=&quot;chat_history&quot;, prompt=summary_prompt ) from langchain.chains.llm import LLMChain llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory) llm_chain.invoke({&quot;input&quot;: &quot;Hi! My name is Maarten. What is 1 + 1?&quot;}) {&#39;input&#39;: &#39;Hi! My name is Maarten. What is 1 + 1?&#39;, &#39;chat_history&#39;: &#39;&#39;, &#39;text&#39;: &#39;Hi Maarten!\\n\\nThe answer to 1 + 1 is 2.&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What is my name?&quot;}) {&#39;input&#39;: &#39;What is my name?&#39;, &#39;chat_history&#39;: &quot;Here is the updated summary:\\n\\nCurrent summary:\\n\\n* Human: Hi! My name is Maarten. What is 1 + 1?\\n* AI: Hi Maarten!\\n* Answer: The answer to 1 + 1 is 2.\\n\\nNew lines of conversation:\\nHuman: That&#39;s correct, what&#39;s 2 * 2?\\nAI: Let me calculate... The answer to 2 * 2 is 4.&quot;, &#39;text&#39;: &#39;Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?&#39;} llm_chain.invoke({&quot;input&quot;: &quot;What was the first question I asked?&quot;}) {&#39;input&#39;: &#39;What was the first question I asked?&#39;, &#39;chat_history&#39;: &#39;Here\\&#39;s the updated summary:\\n\\nCurrent summary:\\n\\n* Human: Hi! My name is Maarten. What is 1 + 1?\\n* AI: Hi Maarten!\\n* Answer: The answer to 1 + 1 is 2.\\n* Human: That\\&#39;s correct, what\\&#39;s 2 * 2?\\n* AI: Let me calculate... The answer to 2 * 2 is 4.\\n* Human: What is my name?\\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?&#39;, &#39;text&#39;: &#39;The first question you asked was: &quot;what\\&#39;s 1 + 1?&quot;&#39;} # check what the summary is thus far memory.load_memory_variables({}) {&#39;chat_history&#39;: &#39;Here is the updated summary:\\n\\nCurrent summary:\\n\\n* Human: Hi! My name is Maarten. What is 1 + 1?\\n* AI: Hi Maarten!\\n* Answer: The answer to 1 + 1 is 2.\\n* Human: That\\&#39;s correct, what\\&#39;s 2 * 2?\\n* AI: Let me calculate... The answer to 2 * 2 is 4.\\n* Human: What is my name?\\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said &quot;Hi! My name is Maarten.&quot; What can I help you with next?\\n* Human: What was the first question I asked?\\n* AI: The first question you asked was: &quot;what\\&#39;s 1 + 1?&quot;&#39;} 7.4. Agents: Creating a System of LLMs Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions. ReAct (Reasoning and Acting) is a cognitive framework for language models that interleaves reasoning (&quot;Thoughts&quot;) and acting (&quot;Actions&quot;) with observations, allowing the model to dynamically plan, execute, and learn from its interactions with external tools or environments to solve complex tasks. Figure 87. An example of a ReAct prompt template. Figure 88. An example of two cycles in a ReAct pipeline. from langchain_openai import ChatOpenAI # an LLM that is powerful enough to properly follow complex instructions llm = ChatOpenAI( model=&quot;mistral:7b-instruct&quot;, # &quot;llama3.1:8b&quot;, # &quot;llama3.2:1b&quot;, temperature=0.7, max_tokens=100, base_url=&quot;http://localhost:11434/v1&quot;, api_key=&quot;API-KEY&quot;, verbose=True, ) from langchain_core.prompts import PromptTemplate # create the ReAct template react_template = &quot;&quot;&quot;Answer the following questions as best you can. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Agents: Creating a System of LLMs Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: {input} Thought:{agent_scratchpad}&quot;&quot;&quot; prompt = PromptTemplate( template=react_template, input_variables=[&quot;tools&quot;, &quot;tool_names&quot;, &quot;input&quot;, &quot;agent_scratchpad&quot;], ) from langchain.agents import load_tools, Tool from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults search = DuckDuckGoSearchResults() search_tool = Tool( name=&quot;duckduck&quot;, description=&quot;A web search engine. Use this to as a search engine for general queries.&quot;, func=search.run, ) tools = load_tools([&quot;llm-math&quot;], llm=llm) tools.append(search_tool) from langchain.agents import AgentExecutor, create_react_agent agent = create_react_agent(llm, tools, prompt) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=5, ) agent_executor.invoke( { &quot;input&quot;: &quot;What is 123 + 456?&quot; } ) &gt; Entering new AgentExecutor chain... To solve this, I will use the Calculator tool. The input for the calculator will be the equation &quot;123 + 456&quot;. Action: Calculator Action Input: &quot;123 + 456&quot;Answer: 579 I now know the final answer. Final Answer: The result of the calculation (123 + 456) is 579. &gt; Finished chain. {&#39;input&#39;: &#39;What is 123 + 456?&#39;, &#39;output&#39;: &#39;The result of the calculation (123 + 456) is 579.&#39;} agent_executor.invoke( { &quot;input&quot;: &quot;What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.&quot; } ) &gt; Entering new AgentExecutor chain... I need to find the current price of a MacBook Pro and then convert that price from USD to EUR using the given exchange rate. Agents: Calculator, duckduck Action: duckduck Action Input: What is the current price of a MacBook Pro in USD?snippet: Apple resellers are hosting a variety of MacBook Pro sales that discount current M4, M4 Pro and M4 Max 14-inch and 16-inch models, in addition to blowout bargains on M3 models. Apple offers two ..., title: Best MacBook Pro Deals for March 2025 | Save up to $1,200 - AppleInsider, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: The newly launched M4 Pro and M4 Max 14-inch MacBook Pros have shown notable performance improvements over their M1, M2, and M3 counterparts, especially in single-core scores. In recent benchmarks, the M4 Pro 14-inch MacBook Pro achieved a single-core score of approximately 3,850, surpassing the M3 Pro&#39;s single-core score by about 15-20%., title: Apple 14″ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/14-macbook-pro/, snippet: Apple MacBook Pro 14&quot; (M4/512GB): was $1,599 now $1,399 at Amazon. The M4-based MacBook Pro M4 is pretty close to being the perfect laptop. You get fantastic performance from the M4 chip, useful ..., title: Epic Apple MacBook sale is live — shop the best deals from $629 right ..., link: https://www.tomsguide.com/sales-events/epic-apple-macbook-sale-is-live-shop-the-best-deals-from-usd629-right-now, snippet: The M4 Max MacBook Pro is Apple&#39;s most powerful option, and both the silver and space black options are on sale. ... List price Best price (current) Best price (all-time) M2 MacBook Air (13-inch ..., title: Best MacBook Deals: Save on Apple&#39;s Latest Laptops and Previous-Gen ..., link: https://www.cnet.com/deals/best-macbook-deals/ The current price of a MacBook Pro in USD can be found from the search results. Let me filter the results a bit more specifically to find the price. Agents: duckduck Action: duckduck Action Input: What is the price of a new 14-inch MacBook Pro (M4/512GB) in USD?snippet: - 14″ M4 MacBook Pro (16GB/1TB/Gray): $1599, $200 off MSRP - 14″ M4 MacBook Pro (24GB/1TB/Gray): $1799, $200 off MSRP. These are currently the lowest prices available for new M4-powered 14″ MacBook Pros among the Apple retailers we track. For the latest sales and prices, keep an eye on our 14-inch MacBook Pro Price Tracker, updated daily., title: 14-inch M4 MacBook Pros on sale today for $150-$200 off MSRP, link: https://www.macprices.net/2025/01/14/14-inch-m4-macbook-pros-on-sale-today-for-150-200-off-msrp/, snippet: Every M4 Pro and M4 Max model is also on sale at up to $300 off in our Mac Price Guide. Prices start at $1,699. Here are a few top picks from the MacBook Pro sale: 14-inch M4, 16GB, 512GB, Space ..., title: Apple M4 MacBook Pro Drops to $1,399, Free Next Day Shipping - AppleInsider, link: https://appleinsider.com/articles/24/12/25/snag-an-m4-macbook-pro-14-inch-for-1399-with-free-next-day-delivery, snippet: The M4 Pro MacBook Pro 14-inch has hit a new record low price of $1,699, with units in stock with free store pickup as early as today. But don&#39;t delay, as the deal ends on Christmas Eve., title: Apple MacBook Pro 14-inch M4 Pro Drops to Best $1,699 Price - AppleInsider, link: https://appleinsider.com/articles/24/12/24/apples-14-inch-macbook-pro-with-m4-pro-chip-plunges-to-record-low-1699-today-only, snippet: Right now the 14-inch MacBook Pro is available with a discount that slashes its price to the lowest yet, and you won&#39;t want to miss out. Amazon is now selling the M4 MacBook Pro for just $1,398 ..., title: Apple&#39;s Latest M4 14-inch MacBook Pro Is Now Yours for Its Best-Ever Price, link: https://www.cnet.com/deals/apples-latest-m4-14-inch-macbook-pro-is-now-yours-for-its-best-ever-price/ The current price of a new 14-inch MacBook Pro (M4/512GB) in USD is $1399. To find the cost in EUR, we can use the given exchange rate of 0.85 EUR for 1 USD. So, the cost of the MacBook Pro in EUR would be 1399 * 0.85 = €1176.21. Final Answer: The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR. &gt; Finished chain. {&#39;input&#39;: &#39;What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.&#39;, &#39;output&#39;: &#39;The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR.&#39;} Appendix A: LangChain LangChain is a framework that consists of a number of packages, which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. langchain-core is a lightweight package containing base abstractions and interfaces for core Langchain components like chat models, vector stores, and tools, without including any third-party integrations and with minimal dependencies. langchain is the main package containing generic chains and retrieval strategies that form an application&#8217;s cognitive architecture, independent of specific third-party integrations. Integrations are a list of lightweight packages (e.g., langchain-openai, langchain-anthropic) that contain specific integrations and are co-maintained for proper versioning. langchain-community is a package containing third-party integrations for various components (chat models, vector stores, tools, etc.), maintained by the Langchain community, with all dependencies being optional to ensure a lightweight package. langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. langserve is a package to deploy LangChain chains as REST APIs that makes it easy to get a production ready API up and running. LangSmith is a developer platform for debugging, testing, evaluating, and monitoring LLM applications. 7.A.1. Chat Models and Messages Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario. LangChain provides a consistent interface for working with chat models from different providers that takes a list of messages as input and returns a message as output while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs. LangChain supports two message formats to interact with chat models: LangChain Message Format: LangChain&#8217;s own message format, which is used by default and is used internally by LangChain. OpenAI&#8217;s Message Format: OpenAI&#8217;s message format. Messages are the unit of communication in chat models, which are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation. Each message has a role (e.g., &quot;user&quot;, &quot;assistant&quot;) and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider. LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider. LangChain messages are Python objects that subclass from a BaseMessage. SystemMessage: corresponds to system role HumanMessage: corresponds to user role AIMessage: corresponds to assistant role AIMessageChunk: corresponds to assistant role, used for streaming responses ToolMessage: corresponds to tool role When invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. model.invoke(&quot;Hello, how are you?&quot;) from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&quot;gpt-4o&quot;, temperature=0, max_tokens=100, timeout=30, max_retries=2, ) llm.invoke(&#39;What is LangChain?&#39;) 7.A.2. Prompt Templates Prompt Templates are responsible for formatting user input into a format that can be passed to a language model, take as input a dictionary, where each key represents a variable in the prompt template to fill in, and output a PromptValue. from langchain_core.prompts import PromptTemplate prompt_template = PromptTemplate.from_template(&quot;Tell me a joke about {topic}&quot;) prompt = prompt_template.format(**{&quot;topic&quot;: &quot;cats&quot;}) print(prompt) # Tell me a joke about cats from langchain_core.prompts import ChatPromptTemplate prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), (&quot;user&quot;, &quot;Tell me a joke about {topic}&quot;) ]) prompt = prompt_template.format(**{&quot;topic&quot;: &quot;cats&quot;}) print(prompt) # System: You are a helpful assistant # Human: Tell me a joke about cats from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.messages import HumanMessage prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), MessagesPlaceholder(&quot;msgs&quot;) ]) prompt = prompt_template.format(**{&quot;msgs&quot;: [HumanMessage(content=&quot;hi!&quot;)]}) print(prompt) # System: You are a helpful assistant # Human: hi! # alternatively prompt_template = ChatPromptTemplate([ (&quot;system&quot;, &quot;You are a helpful assistant&quot;), (&quot;placeholder&quot;, &quot;{msgs}&quot;) # &lt;-- This is the changed part ]) prompt = prompt_template.format(**{&quot;msgs&quot;: [HumanMessage(content=&quot;hi!&quot;)]}) print(prompt) # System: You are a helpful assistant # Human: hi! 7.A.3. Structured Outputs Structured outputs are a concept where language models are instructed to respond in a structured format, rather than in direct natural language, which is useful in scenarios where the output needs to be machine-readable, such as storing output in a database and ensure that the output conforms to the database schema. LangChain provides a method, with_structured_output(), that automates the process of binding the schema to the model and parsing the output. from pydantic import BaseModel, Field class ResponseFormatter(BaseModel): &quot;&quot;&quot;Always use this tool to structure your response to the user.&quot;&quot;&quot; answer: str = Field(description=&quot;The answer to the user&#39;s question&quot;) followup_question: str = Field(description=&quot;A followup question the user could ask&quot;) llm_with_structure = llm.with_structured_output(ResponseFormatter) structured_output = llm_with_structure.invoke( &quot;What is the powerhouse of the cell?&quot;, verbose=True ) structured_output ResponseFormatter(answer=&#39;The powerhouse of the cell is the mitochondria.&#39;, followup_question=&#39;What is the organelle that powers the cell?&#39;) While one approach is to include defined schema in the prompt and ask nicely for the model to use it, it is not recommended. from langchain.output_parsers.structured import ResponseSchema, StructuredOutputParser response_schemas = [ ResponseSchema( name=&quot;answer&quot;, description=&quot;The answer to the user&#39;s question&quot;, type=&quot;string&quot;, ), ResponseSchema( name=&quot;followup_question&quot;, description=&quot;A followup question the user could ask&quot;, type=&quot;string&quot;, ), ] parser = StructuredOutputParser.from_response_schemas(response_schemas) format_instructions = parser.get_format_instructions() from langchain.prompts import PromptTemplate prompt = PromptTemplate( template=&quot;{query}\\n{format_instructions}\\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: format_instructions}, ) print(prompt.format(**{&quot;query&quot;: &quot;What is the powerhouse of the cell?&quot;})) What is the powerhouse of the cell? The output should be a markdown code snippet formatted in the following schema, including the leading and trailing &quot;```json&quot; and &quot;```&quot;: ```json { &quot;answer&quot;: string // The answer to the user&#39;s question &quot;followup_question&quot;: string // A followup question the user could ask } ``` chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;What is the powerhouse of the cell?&quot;}) output {&#39;answer&#39;: &#39;The powerhouse of the cell is the nucleus.&#39;, &#39;followup_question&#39;: &#39;What does the nucleus play a crucial role in?&#39;} 7.A.4. Output Parsers Output Parsers are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks, which are useful when using LLMs to generate structured data, or to normalize output from chat models and LLMs. # parse text from message objects from langchain_core.output_parsers import StrOutputParser chain = llm | StrOutputParser() output = chain.invoke(&#39;What is 2 + 2 ?&#39;) print(output) # 2 + 2 equals 4. # use output parsers to parse an LLM response into structured format from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import PromptTemplate from pydantic import BaseModel, Field, model_validator class Joke(BaseModel): setup: str = Field(description=&quot;question to set up a joke&quot;) punchline: str = Field(description=&quot;answer to resolve the joke&quot;) parser = PydanticOutputParser(pydantic_object=Joke) prompt = PromptTemplate( template=&quot;Answer the user query.\\n{format_instructions}\\n{query}\\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}, ) chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;Tell me a joke.&quot;}) print(output.model_dump_json(indent=2)) # { # &quot;setup&quot;: &quot;Why did the tomato turn red?&quot;, # &quot;punchline&quot;: &quot;Because it saw the salad dressing!&quot; # } # parse JSON output from langchain_core.output_parsers import JsonOutputParser from langchain_core.prompts import PromptTemplate from pydantic import BaseModel, Field class Joke(BaseModel): setup: str = Field(description=&quot;question to set up a joke&quot;) punchline: str = Field(description=&quot;answer to resolve the joke&quot;) parser = JsonOutputParser(pydantic_object=Joke) instructions = parser.get_format_instructions() print(f&#39;\\n{instructions}\\n---------------&#39;) prompt = PromptTemplate( template=&quot;Answer the user query.\\n{format_instructions}\\n{query}\\n&quot;, input_variables=[&quot;query&quot;], partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}, ) chain = prompt | llm | parser output = chain.invoke({&quot;query&quot;: &quot;Tell me a joke.&quot;}) print(output) # The output should be formatted as a JSON instance that conforms to the JSON schema below. # # As an example, for the schema {&quot;properties&quot;: {&quot;foo&quot;: {&quot;title&quot;: &quot;Foo&quot;, &quot;description&quot;: &quot;a list of strings&quot;, &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}}}, &quot;required&quot;: [&quot;foo&quot;]} # the object {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]} is a well-formatted instance of the schema. The object {&quot;properties&quot;: {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]}} is not well-formatted. # # Here is the output schema: # ``` # {&quot;properties&quot;: {&quot;setup&quot;: {&quot;description&quot;: &quot;question to set up a joke&quot;, &quot;title&quot;: &quot;Setup&quot;, &quot;type&quot;: &quot;string&quot;}, &quot;punchline&quot;: {&quot;description&quot;: &quot;answer to resolve the joke&quot;, &quot;title&quot;: &quot;Punchline&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;setup&quot;, &quot;punchline&quot;]} # ``` # --------------- # {&#39;setup&#39;: &#39;Why did the tomato turn red?&#39;, &#39;punchline&#39;: &#39;Because it saw the salad dressing!&#39;} 7.A.5. Embedding, Vector Stores, and Retrievers Embedding models are machine learning models that transform human language or multimodal data (text, audio, images, video - not currently fully supported by Langchain) into numerical vector representations (embeddings), which are fixed-length arrays capturing the semantic meaning of the input, enabling machines to understand and compare data based on conceptual similarity, not just keywords. (1) Embed text as a vector: Embeddings transform text into a numerical vector representation. (2) Measure similarity: Embedding vectors can be compared using simple mathematical operations. LangChain provides a universal interface for working with embedding models, providing standard methods for common operations, and simplifies interaction with various embedding providers through two central methods: embed_documents: For embedding multiple texts (documents) embed_query: For embedding a single text (query) # for embedding multiple texts (documents) from langchain_openai import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings() embeddings = embeddings_model.embed_documents( [ &quot;Hi there!&quot;, &quot;Oh, hello!&quot;, &quot;What&#39;s your name?&quot;, &quot;My friends call me World&quot;, &quot;Hello World!&quot; ] ) len(embeddings), len(embeddings[0]) (5, 1536) # for embedding a single text (query) query_embedding = embeddings_model.embed_query(&quot;What is the meaning of life?&quot;) # measure similarity import numpy as np def cosine_similarity(vec1, vec2): dot_product = np.dot(vec1, vec2) norm_vec1 = np.linalg.norm(vec1) norm_vec2 = np.linalg.norm(vec2) return dot_product / (norm_vec1 * norm_vec2) similarity = cosine_similarity(query_result, document_result) print(&quot;Cosine Similarity:&quot;, similarity) # hugging face embeddings from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) query_embedding = embeddings.embed_query(&quot;Hello, world!&quot;) print(len(query_embedding)) # 384 Vector stores are databases that can efficiently store and retrieve embeddings, which are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches. LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations. The key methods are: add_documents: Add a list of texts to the vector store. delete: Delete a list of documents from the vector store. similarity_search: Search for similar documents to a given query. from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) from langchain_core.vectorstores import InMemoryVectorStore # initialize with an embedding model vector_store = InMemoryVectorStore(embedding=embeddings) # add documents from langchain_core.documents import Document document_1 = Document( page_content=&quot;I had chocalate chip pancakes and scrambled eggs for breakfast this morning.&quot;, metadata={&quot;source&quot;: &quot;tweet&quot;}, ) document_2 = Document( page_content=&quot;The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.&quot;, metadata={&quot;source&quot;: &quot;news&quot;}, ) documents = [document_1, document_2] vector_store.add_documents(documents=documents) # [&#39;df0f6926-c824-4114-a2c5-2b19d9d8740c&#39;, &#39;fa105761-9dd6-4c1c-860a-28e3e4ba181a&#39;] # provide IDs for the documents to the vector store vector_store.add_documents(documents=documents, ids=[&quot;doc1&quot;, &quot;doc2&quot;]) # [&#39;doc1&#39;, &#39;doc2&#39;] # delete documents vector_store.delete(ids=[&quot;doc1&quot;]) # similarity search query = &quot;my query&quot; docs = vectorstore.similarity_search(query) print(docs[0].page_content) Retrievers in Langchain are components that provide a unified way to interact with various retrieval systems, including vector stores, graph databases, and relational databases, and take a natural language query as input to return a list of relevant documents. LangChain provides a uniform interface for interacting with different types of retrieval systems that accepts a query and return documents. A Langchain retriever is a runnable, which is a standard interface for Langchain components, and it has a few common methods, including invoke, that are used to interact with it. docs = retriever.invoke(query) Lost in the Middle is the phenomenon where Large Language Models (LLMs) have difficulty effectively using information located in the middle of a long input context, often performing better when relevant details are at the beginning or end. Documents retrieved from vector stores are typically returned in descending order of relevance, often measured by cosine similarity of embeddings. To mitigate the &quot;lost in the middle&quot; effect, re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. The LongContextReorder document transformer implements the re-ordering procedure. from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) from langchain_core.vectorstores import InMemoryVectorStore texts = [ &quot;Basquetball is a great sport.&quot;, &quot;Fly me to the moon is one of my favourite songs.&quot;, &quot;The Celtics are my favourite team.&quot;, &quot;This is a document about the Boston Celtics&quot;, &quot;I simply love going to the movies&quot;, &quot;The Boston Celtics won the game by 20 points&quot;, &quot;This is just a random text.&quot;, &quot;Elden Ring is one of the best games in the last 15 years.&quot;, &quot;L. Kornet is one of the best Celtics players.&quot;, &quot;Larry Bird was an iconic NBA player.&quot;, ] vector_store = InMemoryVectorStore.from_texts(texts, embedding=embeddings) from langchain_core.runnables import chain from langchain_core.documents import Document # create a retriever @chain def retriever(query: str) -&gt; list[Document]: docs, scores = zip(*vector_store.similarity_search_with_score(query, k=10)) for doc, score in zip(docs, scores): doc.metadata[&quot;score&quot;] = score return docs docs = retriever.invoke(query) max_score_length = max(len(f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;) for doc in docs) for doc in docs: score_str = f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;.rjust(max_score_length) print(f&quot;- {score_str}: {doc.page_content}&quot;) - 0.675469: This is a document about the Boston Celtics - 0.638917: The Celtics are my favourite team. - 0.552694: L. Kornet is one of the best Celtics players. - 0.460651: The Boston Celtics won the game by 20 points - 0.320224: Larry Bird was an iconic NBA player. - 0.244521: Elden Ring is one of the best games in the last 15 years. - 0.231564: Basquetball is a great sport. - 0.106447: I simply love going to the movies - 0.059917: Fly me to the moon is one of my favourite songs. - 0.034081: This is just a random text. from langchain_community.document_transformers import LongContextReorder # Reorder the documents: # Less relevant document will be at the middle of the list and more # relevant elements at beginning / end. reordering = LongContextReorder() reordered_docs = reordering.transform_documents(docs) # Confirm that the 4 relevant documents are at beginning and end. for doc in reordered_docs: score_str = f&quot;{doc.metadata[&#39;score&#39;]:.6f}&quot;.rjust(max_score_length) print(f&quot;- {score_str}: {doc.page_content}&quot;) - 0.638917: The Celtics are my favourite team. - 0.460651: The Boston Celtics won the game by 20 points - 0.244521: Elden Ring is one of the best games in the last 15 years. - 0.106447: I simply love going to the movies - 0.034081: This is just a random text. - 0.059917: Fly me to the moon is one of my favourite songs. - 0.231564: Basquetball is a great sport. - 0.320224: Larry Bird was an iconic NBA player. - 0.552694: L. Kornet is one of the best Celtics players. - 0.675469: This is a document about the Boston Celtics 7.A.6. Document Loaders Document Loaders are responsible for loading documents from a variety of sources. # simple and fast text extraction from langchain_community.document_loaders import PyPDFLoader file_path = &quot;./books/llm-book.pdf&quot; loader = PyPDFLoader(file_path) pages = [] for page in loader.lazy_load(): pages.append(page) print(f&quot;{pages[0].metadata}\\n&quot;) print(pages[0].page_content) {&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} Hands-On Large Language Models Language Understanding and Generation Jay Alammar &amp; Maarten Grootendorst # vector search over PDFs from langchain_core.vectorstores import InMemoryVectorStore from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) vector_store = InMemoryVectorStore.from_documents(pages, embeddings) docs = vector_store.similarity_search(&quot;What is Prompt Engineering?&quot;, k=2) for doc in docs: print(f&#39;Page {doc.metadata[&quot;page&quot;]}: {doc.page_content[:300]}\\n&#39;) Page 194: Intro to Prompt Engineering An essential part of working with text-generative LLMs is prompt engineering. By carefully designing our prompts we can guide the LLM to generate desired responses. Whether the prompts are questions, statements, or instructions, the main goal of prompt engineering is to e Page 219: Summary In this chapter, we explored the basics of using generative models through prompt engineering and output verification. We focused on the creativity and potential com‐ plexity that comes with prompt engineering. These components of a prompt are key in generating and optimizing output appropri 7.A.7. Text Splitters Text splitters split documents into smaller, manageable chunks for use in downstream applications, particularly retrieval systems, to handle non-uniform document lengths, overcome model limitations, improve representation quality, enhance retrieval precision, and optimize computational resources. Text splitting approaches include length-based methods (token or character), text-structure based methods (like recursive splitting that respects paragraphs and sentences), document-structure based methods (leveraging formats like Markdown or HTML), and semantic meaning based methods (analyzing content for significant meaning shifts). from langchain_text_splitters import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=100, chunk_overlap=20, length_function=len, is_separator_regex=False, ) with open(&quot;state_of_the_union.txt&quot;) as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) print(texts[0]) print(texts[1]) Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. from langchain_community.document_loaders.text import TextLoader loader = TextLoader(&quot;state_of_the_union.txt&quot;) documents = loader.load() split_documents = text_splitter.split_documents(documents) print(split_documents[0]) print(split_documents[1]) page_content=&#39;Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and&#39; metadata={&#39;source&#39;: &#39;state_of_the_union.txt&#39;} page_content=&#39;of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.&#39; metadata={&#39;source&#39;: &#39;state_of_the_union.txt&#39;} from langchain_community.document_loaders import PyPDFLoader loader = PyPDFLoader(&quot;./books/llm-book.pdf&quot;) documents = loader.load() split_documents = text_splitter.split_documents(documents) print(split_documents[0]) print(split_documents[1]) page_content=&#39;Hands-On Large Language Models Language Understanding and Generation Jay Alammar &amp;&#39; metadata={&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} page_content=&#39;Jay Alammar &amp; Maarten Grootendorst&#39; metadata={&#39;source&#39;: &#39;./books/llm-book.pdf&#39;, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;Cover&#39;} 7.A.8. Tools LangChain&#8217;s tool abstraction links a Python function to a schema defining its name, description, and expected arguments, which chat models that support tool calling (or function calling) can use to request the execution of a specific function with specific inputs A key principle of tool calling is that the model decides when to use a tool based on the input&#8217;s relevance. # tool creation @tool def multiply(a: int, b: int) -&gt; int: &quot;&quot;&quot;Multiply a and b.&quot;&quot;&quot; return a * b tools = [multiply] # tool binding llm_with_tools = llm.bind_tools(tools) # tool calling output = llm_with_tools.invoke(&quot;What is 2 multiplied by 3?&quot;) output.content, output.tool_calls (&#39;&#39;, [{&#39;name&#39;: &#39;multiply&#39;, &#39;args&#39;: {&#39;a&#39;: 2, &#39;b&#39;: 3}, &#39;id&#39;: &#39;call_zerallda&#39;, &#39;type&#39;: &#39;tool_call&#39;}]) # model doesn&#39;t always need to call a tool output = llm_with_tools.invoke(&quot;Hello world!&quot;) output.content, output.tool_calls (&#39;Hello! How can I assist you today?&#39;, []) 7.A.9. Chat History Chat history is sequence of messages, each of which is associated with a specific role, such as user, assistant, system, or tool, a record of the conversation between the user and the chat model, which is used to maintain context and state throughout the conversation. A full conversation often starts with a system message that sets the context for the conversation, and follows a combination of two alternating message patterns: user and assistant, representing a back-and-forth conversation, or assistant and tool, representing an &quot;agentic&quot; workflow where the assistant invokes tools for specific tasks. All models have finite context windows, and trim_messages can be used to reduce the size of a chat history to a specified token count or specified message count. from langchain_core.messages import ( AIMessage, HumanMessage, SystemMessage, trim_messages, ) messages = [ SystemMessage(&quot;you&#39;re a good assistant, you always respond with a joke.&quot;), HumanMessage(&quot;i wonder why it&#39;s called langchain&quot;), AIMessage( &#39;Well, I guess they thought &quot;WordRope&quot; and &quot;SentenceString&quot; just didn\\&#39;t have the same ring to it!&#39; ), HumanMessage(&quot;and who is harrison chasing anyways&quot;), AIMessage( &quot;Hmmm let me think.\\n\\nWhy, he&#39;s probably chasing after the last cup of coffee in the office!&quot; ), HumanMessage(&quot;what do you call a speechless parrot&quot;), ] # trimming based on token count from langchain_core.messages.utils import count_tokens_approximately trim_messages( messages, strategy=&quot;last&quot;, token_counter=count_tokens_approximately, max_tokens=45, start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, allow_partial=False, ) SystemMessage(content=&quot;you&#39;re a good assistant, you always respond with a joke.&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;what do you call a speechless parrot&#39;, additional_kwargs={}, response_metadata={})] # trimming based on message count trim_messages( messages, strategy=&quot;last&quot;, token_counter=len, max_tokens=5, # message count start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, ) [SystemMessage(content=&quot;you&#39;re a good assistant, you always respond with a joke.&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;and who is harrison chasing anyways&#39;, additional_kwargs={}, response_metadata={}), AIMessage(content=&quot;Hmmm let me think.\\n\\nWhy, he&#39;s probably chasing after the last cup of coffee in the office!&quot;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;what do you call a speechless parrot&#39;, additional_kwargs={}, response_metadata={})] # using a chat model as a token counter from langchain_openai import ChatOpenAI trim_messages( messages, max_tokens=45, strategy=&quot;first&quot;, token_counter=ChatOpenAI(model=&quot;gpt-4o&quot;), ) # chaining from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=&quot;gpt-4o&quot;) trimmer = trim_messages( token_counter=llm, strategy=&quot;last&quot;, max_tokens=45, start_on=&quot;human&quot;, end_on=(&quot;human&quot;, &quot;tool&quot;), include_system=True, ) chain = trimmer | llm chain.invoke(messages) from langchain_core.chat_history import InMemoryChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory chat_history = InMemoryChatMessageHistory(messages=messages[:-1]) def dummy_get_session_history(session_id): if session_id != &quot;1&quot;: return InMemoryChatMessageHistory() return chat_history trimmer = trim_messages( max_tokens=45, strategy=&quot;last&quot;, token_counter=llm, include_system=True, start_on=&quot;human&quot;, ) chain = trimmer | llm chain_with_history = RunnableWithMessageHistory( chain, dummy_get_session_history ) chain_with_history.invoke( [HumanMessage(&quot;what do you call a speechless parrot&quot;)], config={&quot;configurable&quot;: {&quot;session_id&quot;: &quot;1&quot;}}, ) 7.A.10. Memory Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. Short-term memory, or thread-scoped memory, can be recalled at any time from within a single conversational thread with a user. Long-term memory is shared across conversational threads, and can be recalled at any time and in any thread. 7.A.11. LangChain Expression Language (LCEL) The LangChain Expression Language (LCEL) uses a declarative approach, similar to a Unix pipe, to build new Runnable components from existing ones, where a Runnable created with LCEL is often referred to as a &quot;chain&quot; and fully implements the Runnable interface. from langchain_core.vectorstores import InMemoryVectorStore from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) vectorstore = InMemoryVectorStore.from_texts( [&quot;harrison worked at kensho&quot;], embedding=embeddings, ) retriever = vectorstore.as_retriever() from langchain_core.prompts import ChatPromptTemplate template = &quot;&quot;&quot;Answer the question based only on the following context: {context} Question: {question} &quot;&quot;&quot; prompt = ChatPromptTemplate.from_template(template) from langchain_core.runnables import RunnablePassthrough prompt_chain = { &quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough(), } | prompt prompt_text = prompt_chain.invoke(&quot;where did harrison work?&quot;).to_string() print(prompt_text) Human: Answer the question based only on the following context: [Document(id=&#39;d03a67c7-a031-43aa-a27c-6411f9dd0dba&#39;, metadata={}, page_content=&#39;harrison worked at kensho&#39;)] Question: where did harrison work? from langchain_core.output_parsers import StrOutputParser from langchain_openai import ChatOpenAI llm = ChatOpenAI() retrieval_chain = ( {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()} | prompt | llm | StrOutputParser() ) output = retrieval_chain.invoke(&quot;where did harrison work?&quot;) print(output) Harrison worked at Kensho. In LCEL chains, the two main composition primitives are RunnableSequence and RunnableParallel. RunnableSequence is a composition primitive to chain multiple runnables sequentially, with the output of one runnable serving as the input to the next. from langchain_core.runnables import RunnableSequence chain = RunnableSequence([runnable1, runnable2]) final_output = chain.invoke(some_input) corresponds to the following: output1 = runnable1.invoke(some_input) final_output = runnable2.invoke(output1) RunnableParallel is a composition primitive to run multiple runnables concurrently, with the same input provided to each. from langchain_core.runnables import RunnableParallel chain = RunnableParallel({ &quot;key1&quot;: runnable1, &quot;key2&quot;: runnable2, }) final_output = chain.invoke(some_input) { &quot;key1&quot;: runnable1.invoke(some_input), &quot;key2&quot;: runnable2.invoke(some_input), } The | (pipe) operator have been overloaded to create a RunnableSequence from two Runnables. chain = runnable1 | runnable2 is Equivalent to: chain = RunnableSequence([runnable1, runnable2]) is Equivalent to: chain = runnable1.pipe(runnable2) LCEL applies automatic type coercion to make it easier to compose chains. Inside an LCEL expression, a dictionary is automatically converted to a RunnableParallel. mapping = { &quot;key1&quot;: runnable1, &quot;key2&quot;: runnable2, } chain = mapping | runnable3 is automatically converted to the following: chain = RunnableSequence([RunnableParallel(mapping), runnable3]) Inside an LCEL expression, a function is automatically converted to a RunnableLambda. def some_func(x): return x chain = some_func | runnable1 is automatically converted to the following: chain = RunnableSequence([RunnableLambda(some_func), runnable1]) A dict object defines data routing in LCEL by mapping keys to Runnables, functions, or static values, while RunnablePassthrough duplicates data across the pipeline as a data conduit to orchestrate chain flow. chain = ( {&quot;input&quot;: RunnablePassthrough()} # capture initial input | { &quot;output&quot;: llm_chain, # generate LLM output &quot;input&quot;: RunnablePassthrough() # maintain original input } ) # output: {&quot;output&quot;: &quot;LLM&#39;s answer&quot;, &quot;input&quot;: &quot;user&#39;s question&quot;} 8. Semantic Search and Retrieval-Augmented Generation Dense retrieval, reranking, and Retrieval-Augmented Generation (RAG) represent three significant strategies for enhancing search using language models. Dense retrieval systems rely on the concept of embeddings, and turn the search problem into retrieving the nearest neighbors of the search query (after both the query and the documents are converted into embeddings). Figure 89. Dense retrieval is one of the key types of semantic search, relying on the similarity of text embeddings to retrieve relevant results. A reranking language model is one of multiple steps in search system pipelines and is tasked with scoring the relevance of a subset of results against the query; the order of results is then changed based on these scores. Figure 90. Rerankers, the second key type of semantic search, take a search query and a collection of results, and reorder them by relevance, often resulting in vastly improved results. An RAG (Retrieval-Augmented Generation) system is a text generation system that incorporates search capabilities to reduce hallucinations, increase factuality, and/or ground the generation model on a specific dataset. Figure 91. A RAG system formulates an answer to a question and (preferably) cites its information sources. 8.1. Semantic Search with Language Models An embedding is a numeric representation of text, where each text is intuitively represented as a point (or a vector), and texts with similar meaning are close to each other in the high multi-dimensional embedding space. 8.1.1. Dense Retrieval Figure 92. Dense retrieval relies on the property that search queries will be close to their relevant results. # dense retrieval with FAISS from sentence_transformers import SentenceTransformer import faiss text = &quot;&quot;&quot; Artificial intelligence was founded as an academic discipline in 1956. Alan Turing was the first person to conduct substantial research in AI. Born in Maida Vale, London, Turing was raised in southern England. &quot;&quot;&quot; sentences = text.split(&quot;.&quot;) sentences = [s.strip() for s in sentences if s.strip()] model = SentenceTransformer(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;) # embedding the text chunks. xb = model.encode(sentences) # building the search index. d = xb.shape[1] index = faiss.IndexFlatL2(d) index.add(xb) # search the index q = &quot;Who is Alan Turing?&quot; xq = model.encode([q]) distances, indices = index.search(xq, 3) print(f&quot;Q: {q}&quot;) for i in range(len(indices[0])): sentence = sentences[indices[0][i]] distance = distances[0][i] print(f&quot; Sentence: {sentence}&quot;) print(f&quot; Distance: {distance:.4f}&quot;) Q: Who is Alan Turing? Sentence: Alan Turing was the first person to conduct substantial research in AI Distance: 0.4903 Sentence: Born in Maida Vale, London, Turing was raised in southern England Distance: 1.0674 Sentence: Artificial intelligence was founded as an academic discipline in 1956 Distance: 1.4276 # keyword search with BM25 import string import numpy as np from rank_bm25 import BM25Okapi from sklearn.feature_extraction import _stop_words from tqdm import tqdm def bm25_tokenizer(text: str): tokenized_doc = [] for token in text.lower().split(): token = token.strip(string.punctuation) if len(token) &gt; 0 and token not in _stop_words.ENGLISH_STOP_WORDS: tokenized_doc.append(token) return tokenized_doc tokenized_corpus = [] text = &quot;&quot;&quot; Artificial intelligence was founded as an academic discipline in 1956. Alan Turing was the first person to conduct substantial research in AI. Born in Maida Vale, London, Turing was raised in southern England. &quot;&quot;&quot; texts = text.split(&#39;.&#39;) for passage in tqdm(texts): tokenized_corpus.append(bm25_tokenizer(passage)) bm25 = BM25Okapi(tokenized_corpus) def keyword_search(q: str, k=3, n=3): print(&quot;Input question:&quot;, q) bm25_scores = bm25.get_scores(bm25_tokenizer(q)) top_n = np.argpartition(bm25_scores, -n)[-n:] bm25_hits = [ {&#39;corpus_id&#39;: idx, &#39;score&#39;: bm25_scores[idx]} for idx in top_n ] bm25_hits = sorted(bm25_hits, key=lambda x: x[&#39;score&#39;], reverse=True) print(&quot;Top-3 lexical search (BM25) hits&quot;) for hit in bm25_hits[0:k]: print( &quot;\\t{:.3f}\\t{}&quot;.format( hit[&#39;score&#39;], texts[hit[&#39;corpus_id&#39;]].replace(&quot;\\n&quot;, &quot; &quot;) ) ) q = &quot;Who is Alan Turing?&quot; keyword_search(q=q, k=3, n=len(texts)) Input question: Who is Alan Turing? Top-3 lexical search (BM25) hits 0.737 Alan Turing was the first person to conduct substantial research in AI 0.000 Artificial intelligence was founded as an academic discipline in 1956 0.000 Born in Maida Vale, London, Turing was raised in southern England It’s useful to be aware of some of the drawbacks of dense retrieval and how to address them. Lack of Answer in Retrieved Texts Dense retrieval always returns results based on semantic similarity, even if none of the texts actually contain the answer to the query. A potential solution is to implement a distance threshold to filter out results that are not sufficiently relevant. User feedback (click-through rates and satisfaction) can also help improve the system over time. Difficulty with Exact Phrase Matches Dense retrieval, relying on semantic similarity, may not perform well when a user is looking for an exact match of a specific phrase. In such cases, traditional keyword matching is more effective, suggesting the use of hybrid search systems that combine both approaches. Domain Specificity Dense retrieval models trained on data from one domain (e.g., internet and Wikipedia) may not generalize well to other, unseen domains (e.g., legal texts) without sufficient training data from that new domain. Handling Multi-Sentence Answers Dense retrieval systems face the challenge of how to best chunk long texts into embeddings. A key design parameter is deciding the optimal way to divide documents, as answers to some questions may span multiple sentences, and models have context size limitations. Chunking strategies include embedding per document (which can lose information) or embedding multiple chunks per document (which offers better coverage). Various chunking methods exist, such as by sentence, paragraph, or overlapping segments to retain context, with the best approach depending on the text and query types. Scalability and Efficiency While simple nearest neighbor search with tools like NumPy works for smaller datasets, for millions of vectors, optimized approximate nearest neighbor (ANN) search libraries like FAISS or Annoy are necessary for efficient retrieval. Vector databases like Weaviate or Pinecone offer additional functionalities like adding/deleting vectors without rebuilding the index and advanced filtering options. Need for Fine-Tuning To optimize dense retrieval for specific tasks, fine-tuning the embedding models with relevant query-result pairs (including negative examples) is crucial. This process aims to bring embeddings of relevant queries and results closer together in the vector space while pushing irrelevant ones further apart. 8.1.2. Reranking A reranker takes in the search query and a number of search results, and returns the optimal ordering of these documents so the most relevant ones to the query are higher in ranking. Figure 93. LLM rerankers operate as part of a search pipeline with the goal of reordering a number of shortlisted search results by relevance. Figure 94. A reranker assigns a relevance score to each document by looking at the document and the query at the same time. For the retrieval, either lexical search, e.g. with a vector engine like Elasticsearch, or dense retrieval with a SentenceTransformer (a.k.a. bi-encoder) can be used. However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, a re-ranker based on a CrossEncoder that scores the relevancy of all shortlisted candidates for the given search query can be used to output a ranked list. from sentence_transformers import SentenceTransformer bi_encoder = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;) corpus = [ &quot;A man is eating food.&quot;, &quot;A man is eating a piece of bread.&quot;, &quot;The girl is carrying a baby.&quot;, &quot;A man is riding a horse.&quot;, &quot;A woman is playing violin.&quot;, &quot;Two men pushed carts through the woods.&quot;, &quot;A man is riding a white horse on an enclosed ground.&quot;, &quot;A monkey is playing drums.&quot;, &quot;A cheetah is running behind its prey.&quot;, ] corpus_embeddings = bi_encoder.encode(corpus, convert_to_tensor=True) query = &quot;A man is eating pasta.&quot; query_embedding = bi_encoder.encode(query, convert_to_tensor=True) top_N = min(10, len(corpus)) similarity_scores = bi_encoder.similarity(query_embedding, corpus_embeddings)[0] import torch scores, indices = torch.topk(similarity_scores, k=top_N) documents = [] for score, index in zip(scores, indices): document = corpus[index] print(f&quot;({score:.4f})&quot;, document) documents.append(document) (0.7035) A man is eating food. (0.5272) A man is eating a piece of bread. (0.1889) A man is riding a horse. (0.1047) A man is riding a white horse on an enclosed ground. (0.0980) A cheetah is running behind its prey. (0.0819) A monkey is playing drums. (0.0336) A woman is playing violin. (-0.0594) Two men pushed carts through the woods. (-0.0898) The girl is carrying a baby. from sentence_transformers import CrossEncoder cross_encoder = CrossEncoder(&quot;cross-encoder/ms-marco-MiniLM-L-6-v2&quot;) top_K = min(5, top_N) ranking = cross_encoder.rank( query, documents, top_k=top_K, return_documents=True, ) for r in ranking: print(f&quot;({r[&#39;score&#39;]:.4f})&quot;, r[&quot;text&quot;]) (1.9005) A man is eating food. (1.4804) A man is eating a piece of bread. (-7.0890) A man is riding a horse. (-8.9042) A man is riding a white horse on an enclosed ground. (-10.7628) A monkey is playing drums. 8.2. Retrieval-Augmented Generation (RAG) RAG systems incorporate search capabilities in addition to generation capabilities to enhance factuality and reduce hallucinations. Figure 95. A basic RAG pipeline is made up of a search step followed by a grounded generation step where the LLM is prompted with the question and the information retrieved from the search step. Figure 96. Generative search formulates answers and summaries at the end of a search pipeline while citing its sources (returned by the previous steps in the search system). Figure 97. Find the most relevant information to an input prompt by comparing the similarities between embeddings. The most relevant information is added to the prompt before giving it to the LLM. from langchain_openai import ChatOpenAI llm = ChatOpenAI( model=&quot;mistral:7b-instruct&quot;, api_key=&#39;APK-KEY&#39;, base_url=&quot;http://localhost:11434/v1&quot;, # Ollama ) from langchain_text_splitters import HTMLHeaderTextSplitter headers_to_split_on = [ (&quot;h1&quot;, &quot;Header 1&quot;), (&quot;h2&quot;, &quot;Header 2&quot;), (&quot;h3&quot;, &quot;Header 3&quot;), (&quot;h4&quot;, &quot;Header 4&quot;), ] html_splitter = HTMLHeaderTextSplitter(headers_to_split_on) url = &quot;https://plato.stanford.edu/entries/goedel/&quot; documents = html_splitter.split_text_from_url(url) from langchain_community.vectorstores import FAISS from langchain_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot; ) db = FAISS.from_documents(documents, embeddings) from langchain_core.prompts import PromptTemplate template = &quot;&quot;&quot; Relevant information: {context} Provide a concise answer the following question using the relevant information provided above: {question} &quot;&quot;&quot; prompt = PromptTemplate.from_template(template=template) from langchain.chains.retrieval_qa.base import RetrievalQA rag = RetrievalQA.from_chain_type( llm=llm, chain_type=&quot;stuff&quot;, retriever=db.as_retriever(), chain_type_kwargs={&quot;prompt&quot;: prompt}, verbose=True, ) rag.invoke(&quot;Who is Kurt Gödel?&quot;) {&#39;query&#39;: &#39;Who is Kurt Gödel?&#39;, &#39;result&#39;: &quot; Kurt Gödel was an Austrian mathematician and logician. He is best known for his work on the incompleteness theorems, which were established in 1930 and prove that any sufficiently rich formal axiomatic system contains either statements that cannot be proven or disproven within the system itself. Some of Gödel&#39;s other notable contributions include his proof of the consistency of the continuum hypothesis using large cardinals, and his work on undecidable propositions in number theory, which led to the concept of Gödel numbers for representing mathematical statements in a formal system. Throughout his life, Gödel also explored philosophical questions related to logic, mathematics, and metaphysics, including questions about realism, the foundations of mathematics, set theory, and the nature of time and truth.&quot;} 9. Multimodal Large Language Models A multimodal model is a type of artificial intelligence model capable of processing and reasoning across different modalities, where a modality refers to a distinct type of data such as text, images, audio, video, or sensor data. Figure 98. Models that are able to deal with different types (or modalities) of data, such as images, audio, video, or sensors, are said to be multimodal. It’s possible for a model to accept a modality as input yet not be able to generate in that modality. 9.1. Vision Transformer (ViT) Vision Transformer (ViT) is a method that adapts the Transformer architecture to the field of computer vision, particularly for image recognition tasks, by treating an image as a sequence of flattened image patches which are then linearly embedded and processed by the Transformer encoder in a manner similar to textual tokens, allowing it to capture global relationships in the image more directly than the local receptive fields of convolutional neural networks (CNNs). Figure 99. The main algorithm behind ViT. After patching the images and linearly projecting them, the patch embeddings are passed to the encoder and treated as if they were textual tokens. 9.2. Multimodal Embedding Models A multimodal embedding model is a type of model that can create numerical representations (embeddings) for multiple modalities, such as text and imagery, within the same vector space, allowing for direct comparison of representations from different modalities based on their semantic content. Figure 100. Despite having coming from different modalities, embeddings with similar meaning will be close to each other in vector space. Contrastive Language-Image Pre-training (CLIP) is an embedding model to compute embeddings of both images and texts. Figure 101. In the first step of training CLIP, both images and text are embedded using an image and text encoder, respectively. Figure 102. In the second step of training CLIP, the similarity between the sentence and image embedding is calculated using cosine similarity. Figure 103. In the third step of training CLIP, the text and image encoders are updated to match what the intended similarity should be (called contrastive learning). This updates the embeddings such that they are closer in vector space if the inputs are similar. from urllib.request import urlopen from PIL import Image # load an AI-generated image of a puppy playing in the snow from a URL puppy_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/puppy.png&quot; ) # open the image from the URL and convert it to RGB format image = Image.open(urlopen(puppy_path)).convert(&quot;RGB&quot;) # define a text caption for the image caption = &quot;a puppy playing in the snow&quot; Figure 104. An AI-generated image of a puppy playing in the snow. from transformers import CLIPTokenizer, CLIPProcessor, CLIPModel model_id = &quot;openai/clip-vit-base-patch32&quot; # load the tokenizer associated with the CLIP model to preprocess text clip_tokenizer = CLIPTokenizer.from_pretrained(model_id, use_fast=True) # load the processor associated with the CLIP model to preprocess images and text clip_processor = CLIPProcessor.from_pretrained(model_id, use_fast=True) # load the main CLIP model for generating text and image embeddings model = CLIPModel.from_pretrained(model_id) # tokenize the input caption into numerical representations inputs = clip_tokenizer(caption, return_tensors=&quot;pt&quot;) inputs {&#39;input_ids&#39;: tensor([[49406, 320, 6829, 1629, 530, 518, 2583, 49407]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} # convert the token IDs back to the corresponding text tokens clip_tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0]) [&#39;&lt;|startoftext|&gt;&#39;, &#39;a&lt;/w&gt;&#39;, &#39;puppy&lt;/w&gt;&#39;, &#39;playing&lt;/w&gt;&#39;, &#39;in&lt;/w&gt;&#39;, &#39;the&lt;/w&gt;&#39;, &#39;snow&lt;/w&gt;&#39;, &#39;&lt;|endoftext|&gt;&#39;] # create a text embedding vector representing the semantic meaning of the caption text_embedding = model.get_text_features(**inputs) text_embedding.shape # (batch_size, embedding_dimension) torch.Size([1, 512]) # preprocess the image to match the input requirements of the CLIP model image_inputs = clip_processor(text=None, images=image, return_tensors=&quot;pt&quot;) image_pixel_values = image_inputs[&quot;pixel_values&quot;] image_pixel_values.shape # (batch_size, num_channels, height, width) torch.Size([1, 3, 224, 224]) import torch import numpy as np import matplotlib.pyplot as plt # prepare the preprocessed image tensor for visualization img = image_pixel_values.squeeze(0) # remove the batch dimension img = img.permute(*torch.arange(img.ndim - 1, -1, -1)) # transpose dimensions for correct visualization order (C, H, W -&gt; H, W, C) img = np.einsum(&quot;ijk-&gt;jik&quot;, img) # visualize the preprocessed image plt.imshow(img) # turn off axis labels and ticks plt.axis(&quot;off&quot;) Figure 105. The preprocessed input image by CLIP. # create the image embedding vector representing the visual content of the image image_embedding = model.get_image_features(image_pixel_values) image_embedding.shape # (batch_size, embedding_dimension): same as that of the text embedding torch.Size([1, 512]) # normalize the text and image embeddings text_embedding /= text_embedding.norm(dim=-1, keepdim=True) image_embedding /= image_embedding.norm(dim=-1, keepdim=True) # calculate the cosine similarity score text_embedding = text_embedding.detach().cpu().numpy() # move the text embedding to CPU and convert to NumPy array image_embedding = image_embedding.detach().cpu().numpy() # move the image embedding to CPU and convert to NumPy array score = np.dot(text_embedding, image_embedding.T) score array([[0.33146894]], dtype=float32) sentence-transformers implements a few CLIP-based models that make it much easier to create embeddings. It only takes a few lines of code: from urllib.request import urlopen from PIL import Image puppy_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/puppy.png&quot; ) image = Image.open(urlopen(puppy_path)).convert(&quot;RGB&quot;) caption = &quot;a puppy playing in the snow&quot; from sentence_transformers import SentenceTransformer, util model = SentenceTransformer(&quot;sentence-transformers/clip-ViT-B-32&quot;) image_embeddings = model.encode([image]) text_embeddings = model.encode([caption]) sim_matrix = util.cos_sim(image_embeddings, text_embeddings) sim_matrix # tensor([[0.3315]]) 9.3. Multimodal Text Generation Models BLIP-2 (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 2) is a multimodal text generation model designed to introduce vision capabilities to existing, pre-trained language models (LLMs) without requiring end-to-end training from scratch. Figure 106. The Querying Transformer is the bridge between vision (ViT) and text (LLM) that is the only trainable component of the pipeline. 9.3.1. BLIP-2: Bridging the Modality Gap BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), connecting a frozen (non-trainable) pre-trained image encoder like a Vision Transformer and a frozen pre-trained LLM. The Q-Former is trained in two stages, one for each modality to make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM to give information about the image in a similar manner to the context providing an LLM when prompting. Figure 107. In step 1, representation learning is applied to learn representations for vision and language simultaneously. In step 2, these representations are converted to soft visual prompts to feed the LLM. In step 1, image-document pairs are used to train the Q-Former to represent both images and text, which are generally captions of images similar tranning CLIP. Figure 108. In step 1, the output of the frozen ViT is used together with its caption and trained on three contrastive-like tasks to learn visual-text representations. The images are fed to the frozen ViT to extract vision embeddings, which are used as the input of Q-Former’s ViT, and the captions are used as the input of Q-Former’s Text Transformer. The Q-Former is then trained on three tasks: image-text contrastive learning that attempts to align pairs of image and text embeddings such that they maximize their mutual information, image-text matching that predicts whether an image and text pair is positive (matched) or negative (unmatched), and image-grounded text generation that generates text based on information extracted from the input image. In step 2, the learnable embeddings containing aligned visual and textual information in the same dimensional space from the Q-Former are projected to match the LLM&#8217;s input format and then serve as soft visual prompts, conditioning the LLM on the visual representations. Figure 109. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer. The projected embeddings serve as a soft visual prompt. 9.3.2. Preprocessing Multimodal Inputs from urllib.request import urlopen from PIL import Image # load image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) Figure 110. An orange supercar driving on the road at sunset. import torch from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=torch.float16, device_map=dev, ) model.vision_model # vision transformer in the loaded BLIP-2 model. Blip2VisionModel( (embeddings): Blip2VisionEmbeddings( (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14)) ) (encoder): Blip2Encoder( (layers): ModuleList( (0-38): 39 x Blip2EncoderLayer( (self_attn): Blip2Attention( (dropout): Dropout(p=0.0, inplace=False) (qkv): Linear(in_features=1408, out_features=4224, bias=True) (projection): Linear(in_features=1408, out_features=1408, bias=True) ) (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) (mlp): Blip2MLP( (activation_fn): GELUActivation() (fc1): Linear(in_features=1408, out_features=6144, bias=True) (fc2): Linear(in_features=6144, out_features=1408, bias=True) ) (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) ) ) ) (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True) ) model.language_model # text generative model in the loaded BLIP-2 model. OPTForCausalLM( (model): OPTModel( (decoder): OPTDecoder( (embed_tokens): Embedding(50304, 2560, padding_idx=1) (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560) (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) (layers): ModuleList( (0-31): 32 x OPTDecoderLayer( (self_attn): OPTSdpaAttention( (k_proj): Linear(in_features=2560, out_features=2560, bias=True) (v_proj): Linear(in_features=2560, out_features=2560, bias=True) (q_proj): Linear(in_features=2560, out_features=2560, bias=True) (out_proj): Linear(in_features=2560, out_features=2560, bias=True) ) (activation_fn): ReLU() (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=2560, out_features=10240, bias=True) (fc2): Linear(in_features=10240, out_features=2560, bias=True) (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True) ) ) ) ) (lm_head): Linear(in_features=2560, out_features=50304, bias=False) ) # preprocess the image image_inputs = blip_processor(image, return_tensors=&quot;pt&quot;).to(dev, torch.float16) image_pixel_values = image_inputs[&quot;pixel_values&quot;] image_pixel_values.shape # a 224 × 224-sized image torch.Size([1, 3, 224, 224]) # tokenizer used to tokenize the input text blip_processor.tokenizer GPT2TokenizerFast(name_or_path=&#39;Salesforce/blip2-opt-2.7b&#39;, vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;bos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;}, clean_up_tokenization_spaces=False, added_tokens_decoder={ 1: AddedToken(&quot;&lt;pad&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 2: AddedToken(&quot;&lt;/s&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 50265: AddedToken(&quot;&lt;image&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), } ) # preprocess the text text = &quot;Her vocalization was remarkably melodic&quot; token_ids = blip_processor(image, text=text, return_tensors=&quot;pt&quot;) token_ids = token_ids.to(dev, torch.float16)[&quot;input_ids&quot;][0] # convert input ids back to tokens tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids) tokens [&#39;&lt;/s&gt;&#39;, &#39;Her&#39;, &#39;Ġvocal&#39;, &#39;ization&#39;, &#39;Ġwas&#39;, &#39;Ġremarkably&#39;, &#39;Ġmel&#39;, &#39;odic&#39;] # replace the space token with an underscore tokens = [token.replace(&quot;Ġ&quot;, &quot;_&quot;) for token in tokens] tokens [&#39;&lt;/s&gt;&#39;, &#39;Her&#39;, &#39;_vocal&#39;, &#39;ization&#39;, &#39;_was&#39;, &#39;_remarkably&#39;, &#39;_mel&#39;, &#39;odic&#39;] 9.3.3. Use Case 1: Image Captioning from urllib.request import urlopen import torch from PIL import Image from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; dtype = torch.float16 if torch.cuda.is_available() else torch.float32 model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=dtype, device_map=dev, ) # load an AI-generated image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) # convert an image into inputs and preprocess it inputs = blip_processor(image, return_tensors=&quot;pt&quot;).to(dev, dtype) # {&#39;pixel_values&#39;: tensor([[[[-1.0039, -1.0039, -0.9893, ..., -0.0842, -0.0988, -0.0842], # generate image ids to be passed to the decoder (LLM) generated_ids = model.generate(**inputs, max_new_tokens=20) # generate text from the image ids generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text an orange supercar driving on the road at sunset 9.3.4. Use Case 2: Multimodal Chat-Based Prompting from urllib.request import urlopen import torch from PIL import Image from transformers import AutoProcessor, Blip2ForConditionalGeneration # load processor and main model dev = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; dtype = torch.float16 if torch.cuda.is_available() else torch.float32 model_id = &quot;Salesforce/blip2-opt-2.7b&quot; blip_processor = AutoProcessor.from_pretrained(model_id, use_fast=True) model = Blip2ForConditionalGeneration.from_pretrained( model_id, torch_dtype=dtype, device_map=dev, ) # load an AI-generated image of a supercar car_path = ( &quot;https://raw.githubusercontent.com/&quot; &quot;HandsOnLLM/Hands-On-Large-Language-Models/main/&quot; &quot;chapter09/images/car.png&quot; ) with Image.open(urlopen(car_path)) as i: image = i.convert(&quot;RGB&quot;) # visual question answering prompt = &quot;Question: Write down what you see in this picture. Answer:&quot; # process both the image and the prompt inputs = blip_processor(image, text=prompt, return_tensors=&quot;pt&quot;).to(dev, dtype) # generate text generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset # chat-like prompting: a follow-up question prompt = ( &quot;Question: Write down what you see in this picture. Answer: A sports &quot; &quot;car driving on the road at sunset. Question: What would it cost me to &quot; &quot;drive that car? Answer:&quot; ) # Generate output inputs = blip_processor(image, text=prompt, return_tensors=&quot;pt&quot;).to(dev, dtype) generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens=True ) generated_text = generated_text[0].strip() generated_text Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer: $1,000,000 10. Creating and Fine-Tuning Text Embedding Models Embedding models are Large Language Models (LLMs) used to convert unstructured textual data (like documents, sentences, or phrases) into dense numerical representations called embeddings. The primary goal of these models is to accurately capture the semantic meaning of the text, such that texts with similar meanings have embeddings that are close to each other in a high-dimensional vector space, while texts with different meanings have dissimilar embeddings. Figure 111. The idea of semantic similarity is that we expect textual data with similar meanings to be closer to each other in n-dimensional space (two dimensions are illustra‐ ted here). Embedding models can also be trained or fine-tuned for other purposes, such as capturing sentiment similarity, by guiding the model with appropriate training examples. Figure 112. In addition to semantic similarity, an embedding model can be trained to focus on sentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to positive reviews (green). 10.1. Contrastive Learning Contrastive learning is a self-supervised or supervised machine learning technique that aims to learn representations of data by contrasting similar (&quot;positive&quot;) and dissimilar (&quot;negative&quot;) examples (Why P and not Q?) to create an embedding space where similar data points are located close to each other, while dissimilar data points are far apart, which is effective in various domains, including computer vision and natural language processing, for tasks like representation learning, similarity search, and few-shot learning. Reporter: “Why did you rob a bank?” Robber: “Because that is where the money is.” Reporter (alternatively): “Why did you rob a bank (P) instead of obeying the law (Q)?” 10.2. Sentence Transformers (SBERT) A cross-encoder is a Transformer-based model that processes two sentences together to directly predict their similarity score via a classification head, but it&#8217;s computationally expensive for large-scale pairwise comparisons and doesn&#8217;t typically generate individual sentence embeddings. Figure 113. The architecture of a cross-encoder. Both sentences are concatenated, separated with a &lt;SEP&gt; token, and fed to the model simultaneously. The authors of sentence-transformers addressed the limitations of cross-encoders (slow speed, no embeddings) by developing a fast alternative that generates semantically comparable, fixed-size embeddings by using a Siamese architecture, also known as a bi-encoder or SBERT, with two identical BERT models (sharing weights) that process sentences independently and then apply mean pooling to the final layer. Figure 114. The architecture of the original sentence-transformers model, which leverages a Siamese network, also called a bi-encoder. 10.3. Creating an Embedding Model Natural Language Inference (NLI) datasets, used in pretraining embedding models, classify premise-hypothesis pairs as entailment (similar meaning), contradiction (opposite meaning), or neutral. Figure 115. We can leverage the structure of NLI datasets to generate negative examples (contradiction) and positive examples (entailments) for contrastive learning. Entailments serve as positive examples for contrastive learning (similar pairs), while contradictions serve as negative examples (dissimilar pairs). The Multi-Genre Natural Language Inference (MNLI) corpus from the General Language Understanding Evaluation (GLUE) benchmark contains annotated sentence pairs with these relationships, and is a common source for generating such contrastive training data. A subset of MNLI is often used for faster experimentation, though larger, quality datasets are generally preferred for stable training. from datasets import load_dataset # Load MNLI dataset from GLUE # 0 = entailment, 1 = neutral, 2 = contradiction train_dataset = load_dataset( &quot;glue&quot;, # load a dataset from the GLUE benchmark &quot;mnli&quot;, # load the MNLI dataset split=&quot;train&quot;, # load the training split ).select(range(50_000)) train_dataset = train_dataset.remove_columns(&quot;idx&quot;) train_dataset[2] {&#39;premise&#39;: &#39;One of our number will carry out your instructions minutely.&#39;, &#39;hypothesis&#39;: &#39;A member of my team will execute your orders with immense precision.&#39;, &#39;label&#39;: 0} # train model from sentence_transformers import SentenceTransformer # use a base model model = SentenceTransformer(&quot;google-bert/bert-base-uncased&quot;) from sentence_transformers import losses # define the softmax loss function. train_loss = losses.SoftmaxLoss( model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=3, ) from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # create an embedding similarity evaluator for STSB val_sts = load_dataset(&quot;glue&quot;, &quot;stsb&quot;, split=&quot;validation&quot;) evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[&quot;sentence1&quot;], sentences2=val_sts[&quot;sentence2&quot;], scores=[score / 5 for score in val_sts[&quot;label&quot;]], main_similarity=&quot;cosine&quot;, ) from sentence_transformers.training_args import ( SentenceTransformerTrainingArguments, ) args = SentenceTransformerTrainingArguments( output_dir=&quot;base_embedding_model&quot;, num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32, warmup_steps=100, fp16=True, eval_steps=100, logging_steps=100, ) from sentence_transformers.trainer import SentenceTransformerTrainer # train embedding model trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator, ) trainer.train() # evaluate the trained model evaluator(model) References [1] Jay Alammar, Maarten Grootendorst Hands-On Large Language Models: Language Understanding and Generation. O&#8217;Reilly Media; 1st edition (October 15, 2024)","headline":"Hands-On Large Language Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.codefarm.me/2025/03/25/hands-on-large-language-models/"},"url":"https://blog.codefarm.me/2025/03/25/hands-on-large-language-models/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="/assets/css/style.css"><!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SN88FJ18E5"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-SN88FJ18E5');
    </script></head>
  <body>
    <header class="c-header">
  <div class="o-container">
    <a class="c-header-title" href="/">CODE FARM</a>
    <button class="c-header-nav-toggle" id="nav-toggle" aria-label="Toggle navigation">
      <span class="c-header-nav-toggle-icon"></span>
    </button>
    <div class="c-header-nav-wrapper" id="nav-wrapper">
      <nav class="c-header-nav">
        <a href="/">Home</a>
        <a href="/categories/">Category</a>
        <a href="/tags/">Tag</a>
        <a href="/archives/">Archive</a>
        <a href="/about/">About</a>
        <a href="https://resume.github.io/?looogos" target="_blank">R&eacute;sum&eacute;</a>
      </nav>
    </div>
  </div>
  



<div class="o-container">
  <div class="c-banner">
    <img src="/assets/images/galaxy.svg" alt="Galaxy background" class="c-banner-bg">
    <div class="c-banner-quote">
      <p>"The Renaissance was a time when art, science, and philosophy flourished."</p>
      <cite>- Michelangelo</cite>
    </div>
  </div>
</div>
</header>

    <main class="o-container">
      <article class="c-post">
  <header class="c-post-header">
    <h1 class="c-post-title">Hands-On Large Language Models</h1><p class="c-post-meta">25 Mar 2025</p>
  </header>

  <div class="c-post-content">
    <div id="toc" class="toc">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#language-ai">1. Language AI</a></li>
<li><a href="#tokens-and-embeddings">2. Tokens and Embeddings</a>
<ul class="sectlevel2">
<li><a href="#llm-tokenization">2.1. LLM Tokenization</a></li>
<li><a href="#token-embeddings">2.2. Token Embeddings</a></li>
<li><a href="#text-embeddings">2.3. Text Embeddings</a></li>
</ul>
</li>
<li><a href="#large-language-models">3. Large Language Models</a>
<ul class="sectlevel2">
<li><a href="#inputs-and-outputs">3.1. Inputs and Outputs</a></li>
<li><a href="#components">3.2. Components</a></li>
<li><a href="#probability-distribution-samplingdecoding">3.3. Probability Distribution (Sampling/Decoding)</a></li>
<li><a href="#parallel-token-processing-and-context-size">3.4. Parallel Token Processing and Context Size</a></li>
<li><a href="#keys-and-values-caching">3.5. Keys and Values Caching</a></li>
<li><a href="#transformer-block">3.6. Transformer Block</a></li>
</ul>
</li>
<li><a href="#text-classification">4. Text Classification</a>
<ul class="sectlevel2">
<li><a href="#representation-models">4.1. Representation Models</a>
<ul class="sectlevel3">
<li><a href="#task-specific-model">4.1.1. Task-Specific Model</a></li>
<li><a href="#embedding-model">4.1.2. Embedding model</a></li>
</ul>
</li>
<li><a href="#generative-models">4.2. Generative Models</a>
<ul class="sectlevel3">
<li><a href="#text-to-text-transfer-transformer">4.2.1. Text-to-Text Transfer Transformer</a></li>
<li><a href="#chatgpt-for-classification">4.2.2. ChatGPT for Classification</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#text-clustering-and-topic-modeling">5. Text Clustering and Topic Modeling</a>
<ul class="sectlevel2">
<li><a href="#arxivs-articles-computation-and-language">5.1. ArXiv’s Articles: Computation and Language</a></li>
<li><a href="#a-common-pipeline-for-text-clustering">5.2. A Common Pipeline for Text Clustering</a>
<ul class="sectlevel3">
<li><a href="#embedding-documents">5.2.1. Embedding Documents</a></li>
<li><a href="#reducing-the-dimensionality-of-embeddings">5.2.2. Reducing the Dimensionality of Embeddings</a></li>
<li><a href="#cluster-the-reduced-embeddings">5.2.3. Cluster the Reduced Embeddings</a></li>
<li><a href="#inspecting-the-clusters">5.2.4. Inspecting the Clusters</a></li>
</ul>
</li>
<li><a href="#from-text-clustering-to-topic-modeling">5.3. From Text Clustering to Topic Modeling</a>
<ul class="sectlevel3">
<li><a href="#bertopic-a-modular-topic-modeling-framework">5.3.1. BERTopic: A Modular Topic Modeling Framework</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#prompt-engineering">6. Prompt Engineering</a>
<ul class="sectlevel2">
<li><a href="#using-text-generation-models">6.1. Using Text Generation Models</a>
<ul class="sectlevel3">
<li><a href="#prompt-template">6.1.1. Prompt Template</a></li>
<li><a href="#controlling-model-output">6.1.2. Controlling Model Output</a></li>
</ul>
</li>
<li><a href="#prompt-engineering-2">6.2. Prompt Engineering</a></li>
<li><a href="#instruction-based-prompting">6.3. Instruction-Based Prompting</a></li>
<li><a href="#advanced-prompt-engineering">6.4. Advanced Prompt Engineering</a>
<ul class="sectlevel3">
<li><a href="#prompt-components">6.4.1. Prompt Components</a></li>
<li><a href="#in-context-learning-providing-examples">6.4.2. In-Context Learning: Providing Examples</a></li>
<li><a href="#chain-prompting-breaking-up-the-problem">6.4.3. Chain Prompting: Breaking up the Problem</a></li>
</ul>
</li>
<li><a href="#reasoning-with-generative-models">6.5. Reasoning with Generative Models</a>
<ul class="sectlevel3">
<li><a href="#chain-of-thought-think-before-answering">6.5.1. Chain-of-Thought: Think Before Answering</a></li>
<li><a href="#self-consistency-sampling-outputs">6.5.2. Self-Consistency: Sampling Outputs</a></li>
<li><a href="#tree-of-thought-exploring-intermediate-steps">6.5.3. Tree-of-Thought: Exploring Intermediate Steps</a></li>
</ul>
</li>
<li><a href="#output-verification">6.6. Output Verification</a>
<ul class="sectlevel3">
<li><a href="#providing-examples">6.6.1. Providing Examples</a></li>
<li><a href="#grammar-constrained-sampling">6.6.2. Grammar: Constrained Sampling</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#advanced-text-generation-techniques-and-tools">7. Advanced Text Generation Techniques and Tools</a>
<ul class="sectlevel2">
<li><a href="#model-io-loading-quantized-models-with-langchain">7.1. Model I/O: Loading Quantized Models with LangChain</a></li>
<li><a href="#chains-extending-the-capabilities-of-llms">7.2. Chains: Extending the Capabilities of LLMs</a>
<ul class="sectlevel3">
<li><a href="#a-single-link-in-the-chain-prompt-template">7.2.1. A Single Link in the Chain: Prompt Template</a></li>
<li><a href="#a-chain-with-multiple-prompts">7.2.2. A Chain with Multiple Prompts</a></li>
</ul>
</li>
<li><a href="#memory-helping-llms-to-remember-conversations">7.3. Memory: Helping LLMs to Remember Conversations</a>
<ul class="sectlevel3">
<li><a href="#conversation-buffer">7.3.1. Conversation Buffer</a></li>
<li><a href="#windowed-conversation-buffer">7.3.2. Windowed Conversation Buffer</a></li>
<li><a href="#conversation-summary">7.3.3. Conversation Summary</a></li>
</ul>
</li>
<li><a href="#agents-creating-a-system-of-llms">7.4. Agents: Creating a System of LLMs</a></li>
<li><a href="#langchain">Appendix A: LangChain</a>
<ul class="sectlevel3">
<li><a href="#chat-models-and-messages">7.A.1. Chat Models and Messages</a></li>
<li><a href="#prompt-templates">7.A.2. Prompt Templates</a></li>
<li><a href="#structured-outputs">7.A.3. Structured Outputs</a></li>
<li><a href="#output-parsers">7.A.4. Output Parsers</a></li>
<li><a href="#embedding-vector-stores-and-retrievers">7.A.5. Embedding, Vector Stores, and Retrievers</a></li>
<li><a href="#document-loaders">7.A.6. Document Loaders</a></li>
<li><a href="#text-splitters">7.A.7. Text Splitters</a></li>
<li><a href="#tools">7.A.8. Tools</a></li>
<li><a href="#chat-history">7.A.9. Chat History</a></li>
<li><a href="#memory">7.A.10. Memory</a></li>
<li><a href="#langchain-expression-language-lcel">7.A.11. LangChain Expression Language (LCEL)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#semantic-search-and-retrieval-augmented-generation">8. Semantic Search and Retrieval-Augmented Generation</a>
<ul class="sectlevel2">
<li><a href="#semantic-search-with-language-models">8.1. Semantic Search with Language Models</a>
<ul class="sectlevel3">
<li><a href="#dense-retrieval">8.1.1. Dense Retrieval</a></li>
<li><a href="#reranking">8.1.2. Reranking</a></li>
</ul>
</li>
<li><a href="#retrieval-augmented-generation-rag">8.2. Retrieval-Augmented Generation (RAG)</a></li>
</ul>
</li>
<li><a href="#multimodal-large-language-models">9. Multimodal Large Language Models</a>
<ul class="sectlevel2">
<li><a href="#vision-transformer-vit">9.1. Vision Transformer (ViT)</a></li>
<li><a href="#multimodal-embedding-models">9.2. Multimodal Embedding Models</a></li>
<li><a href="#multimodal-text-generation-models">9.3. Multimodal Text Generation Models</a>
<ul class="sectlevel3">
<li><a href="#blip-2-bridging-the-modality-gap">9.3.1. BLIP-2: Bridging the Modality Gap</a></li>
<li><a href="#preprocessing-multimodal-inputs">9.3.2. Preprocessing Multimodal Inputs</a></li>
<li><a href="#use-case-1-image-captioning">9.3.3. Use Case 1: Image Captioning</a></li>
<li><a href="#use-case-2-multimodal-chat-based-prompting">9.3.4. Use Case 2: Multimodal Chat-Based Prompting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#creating-and-fine-tuning-text-embedding-models">10. Creating and Fine-Tuning Text Embedding Models</a>
<ul class="sectlevel2">
<li><a href="#contrastive-learning">10.1. Contrastive Learning</a></li>
<li><a href="#sentence-transformers-sbert">10.2. Sentence Transformers (SBERT)</a></li>
<li><a href="#creating-an-embedding-model">10.3. Creating an Embedding Model</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>
<div class="sect1">
<h2 id="language-ai">1. Language AI</h2>
<div class="sectionbody">
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p><a href="https://colab.research.google.com">Google Colab</a> offers free, cloud-based GPU and TPU access for accelerated computation, subject to usage limits, and requires changing the runtime type to GPU to enable it.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><em>Artificial Intelligence (AI)</em> is the science and engineering of creating intelligent machines, particularly intelligent computer programs, that can perform tasks similar to human intelligence.</p>
</div>
<div class="paragraph">
<p><em>Language AI</em> is a subfield of AI focused on developing technologies that can understand, process, and generate human language, which is often used interchangeably with Natural Language Processing (NLP).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/history-of-lang-ai.png" alt="A peek into the history of Language AI." width="45%" height="45%">
</div>
<div class="title">Figure 1. A peek into the history of Language AI.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/lang-ai-text-tasks.png" alt="Language AI is capable of many tasks by processing textual input." width="35%" height="35%">
</div>
<div class="title">Figure 2. Language AI is capable of many tasks by processing textual input.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The Bag-of-Words, a representation model, converts text to numerical vectors by tokenizing it—splitting sentences into individual words or subwords (tokens)—creating a vocabulary, and counting token occurrences to form a vector representation (the 'bag of words').</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/bag-of-words.png" alt="A bag-of-words is created by counting individual words" width="35%" height="35%">
</div>
<div class="title">Figure 3. A bag-of-words is created by counting individual words. These values are referred to as vector representations.</div>
</div>
</li>
<li>
<p>Word2vec introduced dense vector embeddings, a significant improvement over Bag-of-Words, by using neural networks to capture the semantic meaning of words based on their context within large datasets, allowing for the measurement of semantic similarity.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/word2vec-embedding.png" alt="Embeddings of words that are similar will be close to each other in dimensional space." width="35%" height="35%">
</div>
<div class="title">Figure 4. Embeddings of words that are similar will be close to each other in dimensional space.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/types-of-embedding.png" alt="Embeddings can be created for different types of input." width="45%" height="45%">
</div>
<div class="title">Figure 5. Embeddings can be created for different types of input.</div>
</div>
</li>
<li>
<p>Attention-based Transformer models, replacing RNNs which struggled with long sentences, enabled parallel processing and context-aware language representation by using stacked encoders and decoders to focus on relevant input, revolutionizing language AI.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/word2vec-context-embedding.png" alt="Using word2vec embeddings, a context embedding is generated that represents the entire sequence." width="35%" height="35%">
</div>
<div class="title">Figure 6. Using word2vec embeddings, a context embedding is generated that represents the entire sequence.</div>
</div>
</li>
<li>
<p>The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/transformer-encoder-decoder.png" alt="The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder." width="25%" height="25%">
</div>
<div class="title">Figure 7. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/transformer-decoder-block.png" alt="An encoder block revolves around self-attention to generate intermediate representations." width="30%" height="30%">
</div>
<div class="title">Figure 8. The encoder block revolves around self-attention to generate intermediate representations.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/transformer-decoder-block.png" alt="The decoder has an additional attention layer that attends to the output of the encoder." width="30%" height="30%">
</div>
<div class="title">Figure 9. The decoder has an additional attention layer that attends to the output of the encoder.</div>
</div>
</li>
<li>
<p>Encoder-only models (a.k.a., representation models) like Bidirectional Encoder Representations from Transformers(BERT) excel at language representation through masked language modeling, while decoder-only models (a.k.a., generative models) like Generative Pre-trained Transformer (GPT) focus on text generation and are the foundation for large language models.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/bert-arch.png" alt="The architecture of a BERT base model with 12 encoders." width="30%" height="30%">
</div>
<div class="title">Figure 10. The architecture of a BERT base model with 12 encoders.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/gpt-1-arch.png" alt="The architecture of a GPT-1" width="30%" height="30%">
</div>
<div class="title">Figure 11. The architecture of a GPT-1. It uses a decoder-only architecture and removes the encoder-attention block.</div>
</div>
</li>
<li>
<p>Generative LLMs function as sequence-to-sequence machines, initially designed for text completion, but their capability to be fine-tuned into chatbots or instruct models that can follow user prompts revealed their true potential.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/gen-llm-chat-io.png" alt="Generative LLMs take in some input and try to complete it" width="30%" height="30%">
</div>
<div class="title">Figure 12. Generative LLMs take in some input and try to complete it. With instruct models, this is more than just autocomplete and attempts to answer the question.</div>
</div>
</li>
<li>
<p>The context length, or window, represents the maximum number of tokens the model can process, enabling the generative LLM to handle larger documents, and the current length expands as the model generates new tokens due to its autoregressive nature.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/context-length-llm.png" alt="The context length is the maximum context an LLM can handle." width="35%" height="35%">
</div>
<div class="title">Figure 13. The context length is the maximum context an LLM can handle.</div>
</div>
</li>
<li>
<p>LLMs differ from traditional machine learning by using a two-step training process: <em>pretraining</em>, for general language learning, and <em>fine-tuning</em> (or post-training), to adapt the pretrained (foundation/base) model for specific tasks.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-pretraining-fine-tuning.png" alt="The Training Paradigm of Large Language Models" width="35%" height="35%">
</div>
<div class="title">Figure 14. Compared to traditional machine learning, LLM training takes a multistep approach.</div>
</div>
</li>
<li>
<p>Closed-source LLMs, like GPT-4 and Claude, are models that do not have their weights and architecture shared with the public, which are accessed via APIs, and offer high performance with managed hosting, but are costly and limit user control; open LLMs, such as Llama, share their architecture, enabling local use, fine-tuning, and privacy, but require powerful hardware and expertise.</p>
</li>
<li>
<p>The main source for finding and downloading LLMs is the Hugging Face Hub. Hugging Face is the organization behind the well-known Transformers package, which for years has driven the development of language models in general.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># If a connection to the Hugging Face URL (https://huggingface.co/) fails, try to set the HF_ENDPOINT environment variable to the mirror URL.
</span><span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">HF_ENDPOINT</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://hf-mirror.com</span><span class="sh">"</span></code></pre>
</div>
</div>
</li>
<li>
<p>Hugging Face, the organization behind the Transformers package, is the primary source for finding and downloading LLMs, built upon the Transformer framework.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># HF_ENDPOINT controls the base URL used by the transformers library
# to download models and other resources from the Hugging Face Hub.
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">HF_ENDPOINT</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://hf-mirror.com</span><span class="sh">'</span>

<span class="c1"># determine the device
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="c1"># load model and tokenizer
</span><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="sh">'</span><span class="s">microsoft/Phi-4-mini-instruct</span><span class="sh">'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">MODEL_NAME</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="c1"># create a pipeline
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">text-generation</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">return_full_text</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># the prompt (user input / query)
</span><span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Create a funny joke about chickens.</span><span class="sh">"</span><span class="p">}]</span>

<span class="c1"># generate output
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Why did the chicken join the band?

Because he heard they had the "cluck-loudest" performers around!</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># clear memory and empty the VRAM
</span><span class="kn">import</span> <span class="n">gc</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># attempt to delete the model, tokenizer, and pipeline objects from memory
</span><span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">pipe</span>

<span class="c1"># flush memory
</span><span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
    <span class="c1"># if a GPU is available, empty the CUDA cache to free up GPU memory
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="tokens-and-embeddings">2. Tokens and Embeddings</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Tokens and embeddings are two of the central concepts of using large language models (LLMs).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/token-embedding.png" alt="Language models deal with text in small chunks called tokens." width="30%" height="30%">
</div>
<div class="title">Figure 15. Language models deal with text in small chunks called tokens. For the lan‐ guage model to compute language, it needs to turn tokens into numeric representations called embeddings.</div>
</div>
<div class="sect2">
<h3 id="llm-tokenization">2.1. LLM Tokenization</h3>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># HF_ENDPOINT controls the base URL used by the transformers library
# to download models and other resources from the Hugging Face Hub.
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">HF_ENDPOINT</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://hf-mirror.com</span><span class="sh">'</span>

<span class="c1"># determine the device
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="c1"># load model and tokenizer
</span><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="sh">'</span><span class="s">microsoft/Phi-4-mini-instruct</span><span class="sh">'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">MODEL_NAME</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="sh">'</span><span class="s">&lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;</span><span class="sh">'</span>

<span class="c1"># tokenize the input prompt
</span><span class="n">input_ids</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">input_ids: </span><span class="si">{</span><span class="n">input_ids</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># generate the text
</span><span class="n">output_ids</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">output_ids: </span><span class="si">{</span><span class="n">output_ids</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># print the output
</span><span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="txt">input_ids: tensor([[101950,     29,  16465,    448,   3719,  39950,   6396,    316,  32145,
            395,    290,  62374,  66241,  80785,    403,     13, 115474,   1495,
            480,  12570,     13, 200019]])
output_ids: tensor([[101950,     29,  16465,    448,   3719,  39950,   6396,    316,  32145,
            395,    290,  62374,  66241,  80785,    403,     13, 115474,   1495,
            480,  12570,     13, 200019,  18174,     25,    336,   2768,    512,
           6537,  10384,    395,    290, 193145, 147276,    403,    279,  36210,
          32145,   4464,     40,   5498,    495,   3719]])
&lt;s&gt; Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;Subject: Sincere Apologies for the Gardening Mishap

Dear Sarah,

I hope this email</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Tokens, the units into which text prompts are broken for model input, also form the model&#8217;s output.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/tokenizer-input-output-id.png" alt="Tokenizer" width="30%" height="30%">
</div>
<div class="title">Figure 16. A tokenizer encodes input prompts into token ID lists for the language model and decodes the model&#8217;s output token IDs back into words or tokens.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Each ID corresponds to a specific token (character, word, or subword) in the tokenizer&#8217;s vocabulary.</p>
</li>
<li>
<p>The tokenizer&#8217;s vocabulary acts as a lookup table, allowing the model to convert between text and these integer representations.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">101950</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">16465</span><span class="p">,</span> <span class="mi">448</span><span class="p">,</span> <span class="mi">3719</span><span class="p">,</span> <span class="mi">39950</span><span class="p">]:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="nb">id</span><span class="p">))</span>
<span class="c1"># &lt;s
# &gt;
#  Write
#  an
#  email
#  apolog
</span>
<span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">18174</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">336</span><span class="p">,</span> <span class="mi">2768</span><span class="p">,</span> <span class="mi">512</span><span class="p">]:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
<span class="c1"># Subject
# :
#  S
# inc
# ere</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Tokenization is determined by three major design decisions: the tokenizer algorithm (e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size, special tokens, capitalization, treatment of capitalization and different languages), and the dataset the tokenizer is trained on (a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset).</p>
</li>
<li>
<p>Tokenization methods vary in granularity, from word-level to byte-level, with subword tokenization offering a balance of vocabulary expressiveness and efficiency, making it the most common approach in modern language models.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="token-embeddings">2.2. Token Embeddings</h3>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="txt">Text --&gt; Tokens --&gt; Token IDs --&gt; Embeddings (Vectors)</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>A tokenizer, once trained, becomes intrinsically linked to its language model during the model&#8217;s training; consequently, a pretrained language model cannot function with a different tokenizer without retraining, as their vocabularies and tokenization schemes are aligned.</p>
</li>
<li>
<p>An embedding is a dense, numerical vector representation of a token (like a word or subword) that captures its semantic meaning within a high-dimensional space, enabling language models to understand and process relationships between words.</p>
</li>
<li>
<p>A language model stores static embedding vectors for each token in its vocabulary, but also generates contextualized word embeddings, dynamically representing a token based on its context instead of a single, fixed vector.</p>
<div class="ulist">
<ul>
<li>
<p>A language model holds an embedding vector associated with each token in its tokenizer.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-token-embedding.png" alt="A language model holds an embedding vector associated with each token in its tokenizer." width="30%" height="30%">
</div>
<div class="title">Figure 17. A language model holds an embedding vector associated with each token in its tokenizer.</div>
</div>
</li>
<li>
<p>A language model operates on raw, static embeddings as its input and produces contextual text embeddings.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-context-embedding.png" alt="A language model operates on raw, static embeddings as its input and produces contextual text embeddings." width="30%" height="30%">
</div>
<div class="title">Figure 18. A language model operates on raw, static embeddings as its input and produces contextual text embeddings.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># load a tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">microsoft/deberta-base</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># load a language model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">microsoft/deberta-v3-xsmall</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># tokenize the sentence: convert text to token IDs
</span><span class="n">tokens</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">'</span><span class="s">Hello world</span><span class="sh">'</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># print the decoded tokens to show tokenization
</span><span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">token_id</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># process the token IDs through the model to get contextualized embeddings
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># show the shape of the embedding result
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># output contains the contextualized embedding vectors
</span><span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">[CLS]
Hello
 world
[SEP]


torch.Size([1, 4, 384])

tensor([[[-3.4816,  0.0861, -0.1819,  ..., -0.0612, -0.3911,  0.3017],
         [ 0.1898,  0.3208, -0.2315,  ...,  0.3714,  0.2478,  0.8048],
         [ 0.2071,  0.5036, -0.0485,  ...,  1.2175, -0.2292,  0.8582],
         [-3.4278,  0.0645, -0.1427,  ...,  0.0658, -0.4367,  0.3834]]],
</span><span class="gp">       grad_fn=&lt;NativeLayerNormBackward0&gt;</span><span class="o">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="text-embeddings">2.3. Text Embeddings</h3>
<div class="paragraph">
<p>Text embeddings are single, dense vectors that represent the semantic meaning of entire sentences, paragraphs, or documents, in contrast to token embeddings, which represent individual words or subwords.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># load model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">'</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># convert text to text embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">Best movie ever!</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (384,)</span></code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Input Sequence Length: <a href="https://www.sbert.net/" class="bare">https://www.sbert.net/</a></div>
<div class="paragraph">
<p>For transformer models like BERT, RoBERTa, DistilBERT etc., the runtime and memory requirement grows quadratic with the input length. This limits transformers to inputs of certain lengths. A common value for BERT-based models are 512 tokens, which corresponds to about 300-400 words (for English).</p>
</div>
<div class="paragraph">
<p>Each model has a maximum sequence length under <code>model.max_seq_length</code>, which is the maximal number of tokens that can be processed. Longer texts will be truncated to the first <code>model.max_seq_length</code> tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Max Sequence Length:</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">max_seq_length</span><span class="p">)</span>
<span class="c1"># =&gt; Max Sequence Length: 256
</span>
<span class="c1"># Change the length to 200
</span><span class="n">model</span><span class="p">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">200</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Max Sequence Length:</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">max_seq_length</span><span class="p">)</span>
<span class="c1"># =&gt; Max Sequence Length: 200</span></code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="large-language-models">3. Large Language Models</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># determine the device
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="c1"># load model and tokenizer
</span><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="sh">'</span><span class="s">microsoft/Phi-4-mini-instruct</span><span class="sh">'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">MODEL_NAME</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="c1"># create a pipeline
</span><span class="n">generator</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">text-generation</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">return_full_text</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="sect2">
<h3 id="inputs-and-outputs">3.1. Inputs and Outputs</h3>
<div class="paragraph">
<p>The most common picture of understanding the behavior of a Transformer LLM is to think of it as a software system that takes in text and generates text in response.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Once a large enough text-in-text-out model is trained on a large enough high-quality dataset, it becomes able to generate impressive and useful outputs.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-text-to-text.png" alt="At a high level of abstraction, Transformer LLMs take a text prompt and output generated text." width="30%" height="30%">
</div>
<div class="title">Figure 19. At a high level of abstraction, Transformer LLMs take a text prompt and output generated text.</div>
</div>
</li>
<li>
<p>The model does not generate the text all in one operation; it actually generates one token at a time.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-text-gen-token.png" alt="Transformer LLMs generate one token at a time" width="30%" height="30%">
</div>
<div class="title">Figure 20. Transformer LLMs generate one token at a time, not the entire text at once.</div>
</div>
</li>
<li>
<p>Each token generation step is one forward pass through the model (that’s machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph).</p>
</li>
<li>
<p>After each token generation, the input prompt for the next generation step is tweaked by appending the output token to the end of the input prompt.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-text-gen-forward-pass.png" alt="An output token is appended to the prompt" width="30%" height="30%">
</div>
<div class="title">Figure 21. An output token is appended to the prompt, then this new text is presented to the model again for another forward pass to generate the next token.</div>
</div>
</li>
<li>
<p>Text generation LLMs are called <em>autoregressive</em> models because they generate text sequentially, using prior outputs as input, unlike text representation models like BERT, which process the entire input at once.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="components">3.2. Components</h3>
<div class="ulist">
<ul>
<li>
<p>A language model consists of a <em>tokenizer</em>, a stack of <em>Transformer blocks</em> for processing, and an <em>LM head</em> that converts the processed information into probability scores for the next token.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-tokenizer-transformers-lm-head.png" alt="A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head." width="35%" height="35%">
</div>
<div class="title">Figure 22. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head.</div>
</div>
</li>
<li>
<p>The model has a vector representation associated with each of these tokens in the vocabulary (token embeddings).</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-tokenizer-embedding.png" alt="The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings." width="35%" height="35%">
</div>
<div class="title">Figure 23. The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings.</div>
</div>
</li>
<li>
<p>For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probability distribution for the next token.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-forward-pass.png" alt="At the end of the forward pass, the model predicts a probability score for each token in the vocabulary." width="35%" height="35%">
</div>
<div class="title">Figure 24. At the end of the forward pass, the model predicts a probability score for each token in the vocabulary.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># determine the device
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="c1"># load model and tokenizer
</span><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="sh">'</span><span class="s">microsoft/Phi-4-mini-instruct</span><span class="sh">'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">MODEL_NAME</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=199999)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="probability-distribution-samplingdecoding">3.3. Probability Distribution (Sampling/Decoding)</h3>
<div class="paragraph">
<p>Language models use a probability distribution to determine the next token, which  is called the <em>decoding strategy</em>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The easiest strategy would be to always pick the token with the highest probability score, which is called <em>greedy decoding</em> (equivalent to setting the temperature to zero in an LLM).</p>
<div class="paragraph">
<p>In practice, this doesn’t tend to lead to the best outputs for most use cases.</p>
</div>
</li>
<li>
<p>A better approach is to introduce randomness by <em>sampling</em> from the probability distribution, sometimes choosing the second or third highest probability token.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="parallel-token-processing-and-context-size">3.4. Parallel Token Processing and Context Size</h3>
<div class="ulist">
<ul>
<li>
<p>Transformers excel at parallel processing, unlike earlier architectures, which is evident in how they handle token generation.</p>
<div class="ulist">
<ul>
<li>
<p>Each input token is processed simultaneously through its own computation path or stream.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-parallel-token-processing.png" alt="llm parallel token processing" width="30%" height="30%">
</div>
<div class="title">Figure 25. Each token is processed through its own stream of computation (with some interaction between them in attention steps).</div>
</div>
</li>
<li>
<p>A model with 4K context length or context size can only process 4K tokens and would only have 4K of these streams.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Each of the token streams starts with an input vector (the embedding vector and some positional information).</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-token-vector-processing.png" alt="llm token vector processing" width="30%" height="30%">
</div>
<div class="title">Figure 26. Each processing stream takes a vector as input and produces a final resulting vector of the same size (often referred to as the model dimension).</div>
</div>
<div class="ulist">
<ul>
<li>
<p>At the end of the stream, another vector emerges as the result of the model’s processing.</p>
<div class="ulist">
<ul>
<li>
<p>For text generation, only the output result of the last stream is used to predict the next token.</p>
</li>
<li>
<p>That output vector is the only input into the LM head as it calculates the probabilities of the next token.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="keys-and-values-caching">3.5. Keys and Values Caching</h3>
<div class="paragraph">
<p>Transformer models use a <a href="https://kipp.ly/transformer-inference-arithmetic/">key/value (KV) cache</a> to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), speeding up text generation by avoiding redundant calculations.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-text-gen-kv-cache.png" alt="llm text gen kv cache" width="30%" height="30%">
</div>
<div class="title">Figure 27. When generating text, it’s important to cache the computation results of previous tokens instead of repeating the same calculation over and over again.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>In Hugging Face Transformers, cache is enabled by default, and can be disabled it by setting <code>use_cache</code> to <code>False</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">prompt</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.</span><span class="sh">'</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="transformer-block">3.6. Transformer Block</h3>
<div class="paragraph">
<p>Transformer LLMs are composed of a series Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs) and each block processes its inputs, then passes the results of its processing to the next block.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/transformer-blocks.png" alt="transformer blocks" width="35%" height="35%">
</div>
<div class="title">Figure 28. The bulk of the Transformer LLM processing happens inside a series of Transformer blocks, each handing the result of its processing as input to the subsequent block.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>A Transformer block is made up of two successive components:</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/transformer-block-attention-feedforward-neural-network.png" alt="transformer block attention feedforward neural network" width="35%" height="35%">
</div>
<div class="title">Figure 29. A Transformer block is made up of a self-attention layer and a feedforward neural network.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The <em>attention layer</em> is mainly concerned with incorporating relevant information from other input tokens and positions</p>
</li>
<li>
<p>The <em>feedforward layer</em> houses the majority of the model’s processing capacity</p>
</li>
</ul>
</div>
</li>
<li>
<p>The feedforward network in a Transformer model stores learned information, such as 'The Shawshank' and 'Redemption,' and enables interpolation and generalization for generating text on unseen inputs.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/transformer-feedforeward.png" alt="transformer feedforeward" width="35%" height="35%">
</div>
<div class="title">Figure 30. The feedforward neural network component of a Transformer block likely does the majority of the model’s memorization and interpolation.</div>
</div>
</li>
<li>
<p>The attention layer in a Transformer model enables context awareness, crucial for language understanding beyond simple memorization.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/transformer-attention.png" alt="transformer attention" width="35%" height="35%">
</div>
<div class="title">Figure 31. The self-attention layer incorporates relevant information from previous positions that help process the current token.</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="text-classification">4. Text Classification</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A common task in natural language processing is classification, where the goal is to train a model to assign a label or class to input text, a technique widely used in applications like sentiment analysis and intent detection, significantly impacted by both representative and generative language models.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/text-classification-gen-repr-models.png" alt="Both representation and generative models can be used for classification" width="45%" height="45%">
</div>
<div class="title">Figure 32. Although both representation and generative models can be used for classification, their approaches differ.</div>
</div>
<div class="paragraph">
<p>The Hugging Face Hub is a collaborative platform for machine learning resources (models, datasets, applications), and the <code>datasets</code> package can be used to load datasets.</p>
</div>
<div class="paragraph">
<p>The dataset is split into train (for training), test (for final evaluation), and validation (for intermediate generalization checks, especially during hyperparameter tuning).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load data
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">rotten_tomatoes</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># the well-known 'rotten_tomatoes' dataset
</span><span class="n">data</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 8530
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 1066
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 1066
    })
})</span></code></pre>
</div>
</div>
<div class="sect2">
<h3 id="representation-models">4.1. Representation Models</h3>
<div class="ulist">
<ul>
<li>
<p>Classification with pretrained representation models generally comes in two flavors, either using a task-specific model or an embedding model.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/text-classification-repr-models.png" alt="Text Classification with Representation Models" width="45%" height="45%">
</div>
<div class="title">Figure 33. A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings.</div>
</div>
</li>
<li>
<p>A task-specific model is a representation model, such as BERT, trained for a specific task, like sentiment analysis.</p>
</li>
<li>
<p>An embedding model generates general-purpose embeddings that can be used for a variety of tasks not limited to classification, like semantic search.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/text-classification-repr-frozen-models.png" alt="Perform classification directly with a task-specific model or indirectly with general-purpose embeddings." width="45%" height="45%">
</div>
<div class="title">Figure 34. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings.</div>
</div>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="task-specific-model">4.1.1. Task-Specific Model</h4>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load the well-known 'rotten_tomatoes' dataset for sentiment analysis
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">rotten_tomatoes</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># determine the device to use for computation (GPU if available, otherwise CPU)
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># specify the path to the pre-trained Twitter-RoBERTa-base for Sentiment Analysis model
</span><span class="n">model_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cardiffnlp/twitter-roberta-base-sentiment-latest</span><span class="sh">"</span>
<span class="c1"># load the pre-trained sentiment analysis model into a pipeline for easy inference
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
    <span class="n">return_all_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># return the scores for all sentiment labels
</span>    <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span> <span class="c1"># specify the device to run the pipeline on
</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span> <span class="c1"># for progress bar during inference
</span><span class="kn">from</span> <span class="n">transformers.pipelines.pt_utils</span> <span class="kn">import</span> <span class="n">KeyDataset</span> <span class="c1"># utility to feed data to the pipeline
</span>
<span class="c1"># run inference on the test dataset
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># list to store the predicted sentiment labels
</span><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span>
    <span class="c1"># iterate through the 'text' column of the test dataset
</span>    <span class="nf">pipe</span><span class="p">(</span><span class="nc">KeyDataset</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">)),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">])</span>
<span class="p">):</span>
    <span class="c1"># extract the negative sentiment score
</span>    <span class="n">negative_score</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">]</span>
    <span class="c1"># extract the positive sentiment score (assuming labels are ordered: negative, neutral, positive)
</span>    <span class="n">positive_score</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">]</span>
    <span class="c1"># predict the sentiment based on the highest score (0 for negative, 1 for positive)
</span>    <span class="n">assignment</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">([</span><span class="n">negative_score</span><span class="p">,</span> <span class="n">positive_score</span><span class="p">])</span>
    <span class="c1"># add the predicted label to the list
</span>    <span class="n">y_pred</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">assignment</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>


<span class="k">def</span> <span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">Create and print the classification report comparing true and predicted labels</span><span class="sh">'''</span>
    <span class="n">performance</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Negative Review</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Positive Review</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">performance</span><span class="p">)</span>


<span class="c1"># evaluate the performance of the sentiment analysis model on the test set
</span><span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span> <span class="c1"># compare the true labels with the predicted labels</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">                 precision    recall  f1-score   support

Negative Review       0.76      0.88      0.81       533
Positive Review       0.86      0.72      0.78       533

       accuracy                           0.80      1066
      macro avg       0.81      0.80      0.80      1066
   weighted avg       0.81      0.80      0.80      1066</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The above generated classification report shows four such methods: precision, recall, accuracy, and the F1 score.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>Precision</em> measures how many of the items found are relevant, which indicates the accuracy of the relevant results.</p>
</li>
<li>
<p><em>Recall</em> refers to how many relevant classes were found, which indicates its ability to find all relevant results.</p>
</li>
<li>
<p><em>Accuracy</em> refers to how many correct predictions the model makes out of all predictions, which indicates the overall correctness of the model.</p>
</li>
<li>
<p>The <em>F1 score</em> balances both precision and recall to create a model’s overall performance.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A confusion matrix visualizes the performance of a classification model by showing the counts of four prediction outcomes: True Positives, True Negatives, False Positives, and False Negatives, which serves as the basis for calculating various metrics to evaluate the model&#8217;s quality.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/confusion-matrix.png" alt="The confusion matrix describes four types of predictions." width="35%" height="35%">
</div>
<div class="title">Figure 35. The confusion matrix describes four types of predictions.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/classification-metrics.png" alt="classification metrics" width="35%" height="35%">
</div>
<div class="title">Figure 36. The classification report describes several metrics for evaluating a model’s performance.</div>
</div>
</div>
<div class="sect3">
<h4 id="embedding-model">4.1.2. Embedding model</h4>
<div class="ulist">
<ul>
<li>
<p>Without fine-tuning a representation model, a general-purpose embedding model can generate features that are then fed into a separate, trainable classifier (like logistic regression, which can be trained efficiently on a CPU), creating a two-step classification approach.</p>
</li>
<li>
<p>A major benefit of this separation is avoiding the costly fine-tuning of the embedding model, instead, a classifier, such as logistic regression, can be trained efficiently on the CPU.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load the well-known 'rotten_tomatoes' dataset for sentiment analysis
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">rotten_tomatoes</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># load the SentenceTransformer model for generating text embeddings
</span><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">sentence-transformers/all-mpnet-base-v2</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># convert the text data from the train and test splits into embeddings
</span><span class="n">train_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># train a logistic regression classifier on the generated training embeddings
#   initialize the logistic regression model with a random state for reproducibility
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1">#   train the classifier using the training embeddings and their corresponding labels
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_embeddings</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">])</span>

<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>


<span class="k">def</span> <span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">Create and print the classification report comparing true and predicted labels</span><span class="sh">'''</span>
    <span class="n">performance</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Negative Review</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Positive Review</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">performance</span><span class="p">)</span>


<span class="c1"># predict the sentiment labels for the test embeddings using the trained classifier
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">test_embeddings</span><span class="p">)</span>

<span class="c1"># evaluate the performance of the classifier on the test set
</span><span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">                 precision    recall  f1-score   support

Negative Review       0.85      0.86      0.85       533
Positive Review       0.86      0.85      0.85       533

       accuracy                           0.85      1066
      macro avg       0.85      0.85      0.85      1066
   weighted avg       0.85      0.85      0.85      1066</span></code></pre>
</div>
</div>
</li>
<li>
<p>Zero-shot classification can be used on unlabeled data by leveraging the model&#8217;s pre-existing knowledge to predict labels based solely on their definitions.</p>
<div class="ulist">
<ul>
<li>
<p>In zero-shot classification, without any labeled examples, the model determines the relationship between input text and predefined candidate labels.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/zero-shot-classification.png" alt="zero shot classification" width="30%" height="30%">
</div>
<div class="title">Figure 37. In zero-shot classification, we have no labeled data, only the labels them‐ selves. The zero-shot model decides how the input is related to the candidate labels.</div>
</div>
</li>
<li>
<p>Zero-shot classification generates target labels without labeled data by describing and embedding labels (e.g., "negative movie review") and documents.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/zero-shot-classification-embedding.png" alt="zero shot classification embedding" width="30%" height="30%">
</div>
<div class="title">Figure 38. To embed the labels, we first need to give them a description, such as “a negative movie review.” This can then be embedded through sentence-transformers.</div>
</div>
</li>
<li>
<p>To assign labels to documents in zero-shot classification, cosine similarity, representing the cosine of the angle between the embedding vectors, can be applied to document-label embedding pairs.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load the well-known 'rotten_tomatoes' dataset for sentiment analysis
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">rotten_tomatoes</span><span class="sh">'</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># load model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">'</span><span class="s">sentence-transformers/all-mpnet-base-v2</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># convert text to embeddings
</span><span class="n">train_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">],</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">],</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># create embeddings for our labels
</span><span class="n">label_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="sh">'</span><span class="s">A negative review</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">A positive review</span><span class="sh">'</span><span class="p">])</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="c1"># find the best matching label for each document using cosine similarity
</span><span class="n">sim_matrix</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">test_embeddings</span><span class="p">,</span> <span class="n">label_embeddings</span><span class="p">)</span>
<span class="c1"># get the index of the label with the highest similarity score for each test embedding
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="k">def</span> <span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">Create and print the classification report comparing true and predicted labels</span><span class="sh">'''</span>
    <span class="n">performance</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Negative Review</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Positive Review</span><span class="sh">'</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">performance</span><span class="p">)</span>

<span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">                 precision    recall  f1-score   support

Negative Review       0.78      0.77      0.78       533
Positive Review       0.77      0.79      0.78       533

       accuracy                           0.78      1066
      macro avg       0.78      0.78      0.78      1066
   weighted avg       0.78      0.78      0.78      1066</span></code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>From Wikipedia, the free encyclopedia</p>
</div>
<div class="paragraph">
<p>In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval <code>[−1, 1]</code>.</p>
</div>
</blockquote>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/cosine-similarity.svg" alt="{\displaystyle {\text{cosine similarity}}=S_{C}(A,B):=\cos(\theta )={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|\|\mathbf {B} \|}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}\cdot {\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},}" width="45%" height="45%">
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>  <span class="c1"># import the NumPy library for numerical operations
</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># create a NumPy array named A
</span><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>  <span class="c1"># create a NumPy array named B
</span>
<span class="c1"># calculate the cosine similarity using the formula: (A dot B) / (||A|| * ||B||)
</span><span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>  <span class="c1"># calculate the dot product of A and B
</span><span class="n">norm_A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>  <span class="c1"># calculate the Euclidean norm (magnitude) of A
</span><span class="n">norm_B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>  <span class="c1"># calculate the Euclidean norm (magnitude) of B
</span><span class="n">cosine_similarity</span> <span class="o">=</span> <span class="n">dot_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_A</span> <span class="o">*</span> <span class="n">norm_B</span><span class="p">)</span>  <span class="c1"># calculate the cosine similarity
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">cosine_similarity</span><span class="p">)</span>  <span class="c1"># 0.9746318461970762</span></code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="generative-models">4.2. Generative Models</h3>
<div class="ulist">
<ul>
<li>
<p>Text classification with generative language models (like GPT) involves feeding input text to the model and having it generate text as output, in contrast to task-specific models that directly output a class label.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/classification-text-generation-models.png" alt="A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens." width="45%" height="45%">
</div>
<div class="title">Figure 39. A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens.</div>
</div>
</li>
<li>
<p>Generative models are generally trained on a wide variety of tasks and usually don&#8217;t inherently know how to handle specific tasks like classifying a movie review without explicit instructions.</p>
</li>
<li>
<p>Prompt engineering is the skill of crafting effective instructions, or prompts, to guide generative AI models towards producing desired and high-quality outputs for specific tasks, like text classification, which often involves iterative refinement of these prompts based on the model&#8217;s responses.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/prompt-engineering-iteration.png" alt="Prompt engineering allows prompts to be updated to improve the output generated by the model." width="35%" height="35%">
</div>
<div class="title">Figure 40. Prompt engineering allows prompts to be updated to improve the output generated by the model.</div>
</div>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="text-to-text-transfer-transformer">4.2.1. Text-to-Text Transfer Transformer</h4>
<div class="ulist">
<ul>
<li>
<p>Text-to-Text Transfer Transformer or T5, like the original Transformer, is a generative encoder-decoder sequence-to-sequence model, contrasting with encoder-only BERT and decoder-only GPT.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/t5-arch.png" alt="The T5 architecture is similar to the original Transformer model, a decoder- encoder architecture." width="35%" height="35%">
</div>
<div class="title">Figure 41. The T5 architecture is similar to the original Transformer model, a decoder- encoder architecture.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>In the first step of training, namely pretraining, encoder-decoder models like T5 are initially trained with a masked language modeling objective that masks sets of tokens (or token spans), differing from BERT&#8217;s individual token masking approach.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/t5-pretraining.png" alt="t5 pretraining" width="35%" height="35%">
</div>
<div class="title">Figure 42. In the first step of training, namely pretraining, the T5 model needs to predict masks that could contain multiple tokens.</div>
</div>
</li>
<li>
<p>In the second step of training, namely fine-tuning the base model, instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/t5-fine-tuning.png" alt="t5 fine tuning" width="35%" height="35%">
</div>
<div class="title">Figure 43. By converting specific tasks to textual instructions, the T5 model can be trained on a variety of tasks during fine-tuning.</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load the well-known 'rotten_tomatoes' dataset for sentiment analysis
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">rotten_tomatoes</span><span class="sh">'</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># determine the device to use for computation (GPU if available, otherwise CPU)
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># specify the path to the pre-trained FLAN-T5-small model for text-to-text generation
</span><span class="n">model_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">google/flan-t5-small</span><span class="sh">'</span>
<span class="c1"># load the pre-trained text-to-text generation model into a pipeline for easy inference
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">text2text-generation</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># prepare our data by creating a prompt and combining it with the text
</span><span class="n">prompt</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Is the following sentence positive or negative? </span><span class="sh">'</span>
<span class="c1"># apply the prompt to each example in the dataset's 'text' column to create a new 't5' column
</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">example</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">t5</span><span class="sh">'</span><span class="p">:</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">example</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]})</span>
<span class="c1"># data # uncomment to inspect the modified dataset
</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>  <span class="c1"># for progress bar during inference
</span><span class="kn">from</span> <span class="n">transformers.pipelines.pt_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">KeyDataset</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># utility to feed data to the pipeline
</span>
<span class="c1"># Run inference
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># iterate through the test dataset using the pipeline for text generation
</span><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span>
    <span class="nf">pipe</span><span class="p">(</span><span class="nc">KeyDataset</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">],</span> <span class="sh">'</span><span class="s">t5</span><span class="sh">'</span><span class="p">)),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">])</span>
<span class="p">):</span>
    <span class="c1"># extract the generated text from the pipeline's output
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">generated_text</span><span class="sh">'</span><span class="p">]</span>
    <span class="c1"># classify the generated text as 0 (negative) if it equals 'negative', otherwise 1 (positive)
</span>    <span class="n">y_pred</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">text</span> <span class="o">==</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>


<span class="k">def</span> <span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">Create and print the classification report comparing true and predicted labels</span><span class="sh">'''</span>
    <span class="n">performance</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Negative Review</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Positive Review</span><span class="sh">'</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">performance</span><span class="p">)</span>


<span class="c1"># evaluate the performance of the model by comparing the true labels with the predicted labels
</span><span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">                 precision    recall  f1-score   support

Negative Review       0.83      0.85      0.84       533
Positive Review       0.85      0.83      0.84       533

       accuracy                           0.84      1066
      macro avg       0.84      0.84      0.84      1066
   weighted avg       0.84      0.84      0.84      1066</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="chatgpt-for-classification">4.2.2. ChatGPT for Classification</h4>
<div class="paragraph">
<p>OpenAI shared <a href="https://openai.com/index/chatgpt/">an overview of the training procedure</a> that involved an important component, namely preference tuning.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>OpenAI first manually created the desired output to an input prompt (instruction data) and used that data to create a first variant of its model.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/chatgpt-instruction-tuning.png" alt="chatgpt instruction tuning" width="35%" height="35%">
</div>
<div class="title">Figure 44. Manually labeled data consisting of an instruction (prompt) and output was used to perform fine-tuning (instruction-tuning).</div>
</div>
</li>
<li>
<p>OpenAI used the resulting model to generate multiple outputs that were manually ranked from best to worst.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/chatgpt-preference-tuning.png" alt="chatgpt preference tuning" width="35%" height="35%">
</div>
<div class="title">Figure 45. Manually ranked preference data was used to generate the final model, ChatGPT.</div>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="python"><span class="kn">import</span> <span class="n">openai</span>

<span class="c1"># create client for interacting with OpenAI API
</span><span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="nc">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="sh">'</span><span class="s">YOUR_KEY_HERE</span><span class="sh">'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">chatgpt_generation</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">document</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="sh">'</span><span class="s">gpt-3.5-turbo-0125</span><span class="sh">'</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">Generate an output based on a prompt and an input document using ChatGPT.</span><span class="sh">'''</span>
    <span class="c1"># define the message structure for the OpenAI API
</span>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">system</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">You are a helpful assistant.</span><span class="sh">'</span><span class="p">},</span>
        <span class="p">{</span><span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="n">prompt</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">[DOCUMENT]</span><span class="sh">'</span><span class="p">,</span> <span class="n">document</span><span class="p">)},</span>
    <span class="p">]</span>
    <span class="c1"># call the OpenAI Chat Completions API to get a response
</span>    <span class="n">chat_completion</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span> <span class="c1"># temperature=0 for deterministic output
</span>    <span class="p">)</span>
    <span class="c1"># return the content of the first choice's message
</span>    <span class="k">return</span> <span class="n">chat_completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>


<span class="c1"># define a prompt template as a base for sentiment classification
</span><span class="n">prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">Predict whether the following document is a positive or negative
movie review:
[DOCUMENT]
If it is positive return 1 and if it is negative return 0. Do not give any
other answers.
</span><span class="sh">'''</span>

<span class="c1"># predict the target for a single document using GPT
</span><span class="n">document</span> <span class="o">=</span> <span class="sh">'</span><span class="s">unpretentious , charming , quirky , original</span><span class="sh">'</span>
<span class="nf">chatgpt_generation</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">document</span><span class="p">)</span>



<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load the well-known 'rotten_tomatoes' dataset for sentiment analysis
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">rotten_tomatoes</span><span class="sh">'</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># generate predictions for all documents in the test set
</span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nf">chatgpt_generation</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">])</span>
<span class="p">]</span>

<span class="c1"># convert the string predictions ('0' or '1') to integers
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">]</span>

<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>


<span class="k">def</span> <span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">Create and print the classification report comparing true and predicted labels</span><span class="sh">'''</span>
    <span class="n">performance</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Negative Review</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Positive Review</span><span class="sh">'</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">performance</span><span class="p">)</span>


<span class="c1"># evaluate the performance of ChatGPT on the test set
</span><span class="nf">evaluate_performance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="text-clustering-and-topic-modeling">5. Text Clustering and Topic Modeling</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Although supervised techniques, such as classification, have reigned supreme over the last few years in the industry, the potential of unsupervised techniques such as text clustering cannot be understated.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Text clustering aims to group similar texts based on their semantic content, meaning, and relationships.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/text-clustering.png" alt="Clustering unstructured textual data." width="35%" height="35%">
</div>
<div class="title">Figure 46. Clustering unstructured textual data.</div>
</div>
</li>
<li>
<p>Text clustering is also applied in topic modeling to uncover abstract topics within large textual datasets.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/topic-modeling.png" alt="Topic modeling is a way to give meaning to clusters of textual documents." width="35%" height="35%">
</div>
<div class="title">Figure 47. Topic modeling is a way to give meaning to clusters of textual documents.</div>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="arxivs-articles-computation-and-language">5.1. ArXiv’s Articles: Computation and Language</h3>
<div class="paragraph">
<p><a href="https://arxiv.org/">ArXiv</a> is an open-access platform for scholarly articles, mostly in the fields of <a href="https://huggingface.co/datasets/MaartenGr/arxiv_nlp">computer science</a>, mathematics, and physics.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load the 'arxiv_nlp' dataset from Hugging Face Datasets library
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">maartengr/arxiv_nlp</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># extract metadata
</span><span class="n">abstracts</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">Abstracts</span><span class="sh">"</span><span class="p">]</span>
<span class="n">titles</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">Titles</span><span class="sh">"</span><span class="p">]</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="a-common-pipeline-for-text-clustering">5.2. A Common Pipeline for Text Clustering</h3>
<div class="paragraph">
<p>Text clustering enables the discovery of both known and unknown data patterns, providing an intuitive understanding of tasks like classification and their complexity, making it valuable beyond just exploratory data analysis.</p>
</div>
<div class="paragraph">
<p>Although there are many methods for text clustering, from graph-based neural networks to centroid-based clustering techniques, a common pipeline that has gained popularity involves three steps and algorithms:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Convert the input documents to embeddings with an embedding model.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/text-cluster-embedding-documents.png" alt="text cluster embedding documents" width="35%" height="35%">
</div>
<div class="title">Figure 48. Step 1: We convert documents to embeddings using an embedding model.</div>
</div>
</li>
<li>
<p>Reduce the dimensionality of embeddings with a dimensionality reduction model.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/dimensionality-compression.png" alt="dimensionality compression" width="35%" height="35%">
</div>
<div class="title">Figure 49. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction.</div>
</div>
</li>
<li>
<p>Find groups of semantically similar documents with a cluster model.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/text-cluster-3-step.png" alt="Cluster the Reduced Embeddings" width="35%" height="35%">
</div>
<div class="title">Figure 50. Step 3: We cluster the documents using the embeddings with reduced dimensionality.</div>
</div>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="embedding-documents">5.2.1. Embedding Documents</h4>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># create an embedding model using a pre-trained Sentence Transformer model
</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">'</span><span class="s">thenlper/gte-small</span><span class="sh">'</span><span class="p">)</span> <i class="conum" data-value="1"></i><b>(1)</b>

<span class="c1"># generate embeddings for each abstract in the 'abstracts' list
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">abstracts</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># check the dimensions (shape) of the resulting embeddings
</span><span class="n">embeddings</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># (44949, 384) <i class="conum" data-value="2"></i><b>(2)</b>
</span></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>thenlper/gte-small</code> model is a more recent model that outperforms the previous model on clustering tasks and due to its small size is even faster for inference.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>embeddings.shape</code> of <code>(44949, 384)</code> shows that there are 44,949 abstract embeddings, each with a dimensionality of 384.</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="reducing-the-dimensionality-of-embeddings">5.2.2. Reducing the Dimensionality of Embeddings</h4>
<div class="ulist">
<ul>
<li>
<p>Reducing the dimensionality of embeddings is essential before clustering high-dimensional data to simplify the representation and enhance clustering effectiveness.</p>
</li>
<li>
<p>Dimensionality reduction is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/dimensionality-reduction.png" alt="dimensionality reduction" width="35%" height="35%">
</div>
<div class="title">Figure 51. Dimensionality reduction allows data in high-dimensional space to be compressed to a lower-dimensional representation.</div>
</div>
</li>
<li>
<p>Well-known methods for dimensionality reduction are Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP).</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">umap</span> <span class="kn">import</span> <span class="n">UMAP</span>

<span class="c1"># reduce the input embeddings from 384 dimensions to 5 dimensions using UMAP
</span><span class="n">umap_model</span> <span class="o">=</span> <span class="nc">UMAP</span><span class="p">(</span>
    <span class="c1"># generally, values between 5 and 10 work well to capture high-dimensional global structures.
</span>    <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>   <span class="c1"># the number of dimensions to reduce to
</span>    <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>     <span class="c1"># the effective minimum distance between embedded points
</span>    <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">cosine</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># the metric to use to compute distances in high dimensional space
</span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>  <span class="c1"># for reproducibility of the embedding
</span><span class="p">)</span>
<span class="c1"># fit and then transform the embeddings to the lower-dimensional space
</span><span class="n">reduced_embeddings</span> <span class="o">=</span> <span class="n">umap_model</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="cluster-the-reduced-embeddings">5.2.3. Cluster the Reduced Embeddings</h4>
<div class="ulist">
<ul>
<li>
<p>While k-means, a centroid-based algorithm needing a predefined number of clusters, is common, density-based algorithms are preferable when the number of clusters is unknown as they automatically determine the clusters and don&#8217;t require all data points to belong to one.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/centroid-based-and-density-based-algorithm.png" alt="centroid based and density based algorithm" width="35%" height="35%">
</div>
<div class="title">Figure 52. The clustering algorithm not only impacts how clusters are generated but also how they are viewed.</div>
</div>
</li>
<li>
<p>A common density-based model is Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN).</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">hdbscan</span> <span class="kn">import</span> <span class="n">HDBSCAN</span>

<span class="c1"># initialize and fit the HDBSCAN clustering model
</span><span class="n">hdbscan_model</span> <span class="o">=</span> <span class="nc">HDBSCAN</span><span class="p">(</span>
    <span class="c1"># the minimum number of samples in a group for it to be considered a cluster
</span>    <span class="n">min_cluster_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="c1"># the metric to use when calculating pairwise distances between data points
</span>    <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">euclidean</span><span class="sh">'</span><span class="p">,</span>
    <span class="c1"># the method used to select clusters from the hierarchy ('eom' stands for Excess of Mass)
</span>    <span class="n">cluster_selection_method</span><span class="o">=</span><span class="sh">'</span><span class="s">eom</span><span class="sh">'</span>
<span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">reduced_embeddings</span><span class="p">)</span> <span class="c1"># fit the HDBSCAN model to the reduced dimensionality embeddings
# extract the cluster labels assigned to each data point (-1 indicates noise)
</span><span class="n">clusters</span> <span class="o">=</span> <span class="n">hdbscan_model</span><span class="p">.</span><span class="n">labels_</span>
<span class="c1"># How many clusters did we generate? (excluding the noise cluster labeled -1)
</span><span class="n">num_clusters</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">clusters</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="n">clusters</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="inspecting-the-clusters">5.2.4. Inspecting the Clusters</h4>
<div class="ulist">
<ul>
<li>
<p>To inspect each cluster manually and explore the assigned documents to get an understanding of its content.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># print first three documents in cluster 0
</span><span class="n">cluster</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">cluster</span><span class="p">)[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">3</span><span class="p">]:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">abstracts</span><span class="p">[</span><span class="n">index</span><span class="p">][:</span><span class="mi">300</span><span class="p">]</span> <span class="o">+</span> <span class="sh">"</span><span class="s">... </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>To visualize clustering approximation results without manual review, further reduce document embeddings to two dimensions for plotting on an 2D plane.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">umap</span> <span class="kn">import</span> <span class="n">UMAP</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># reduce 384-dimensional embeddings to two dimensions for easier visualization
</span><span class="n">reduced_embeddings</span> <span class="o">=</span> <span class="nc">UMAP</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="sh">"</span><span class="s">cosine</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="p">).</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="c1"># create dataframe
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">reduced_embeddings</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">titles</span>
<span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">cluster</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">]</span>
<span class="c1"># select outliers (cluster -1) and non-outliers (clusters)
</span><span class="n">to_plot</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="n">cluster</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">-1</span><span class="sh">"</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">outliers</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="n">cluster</span> <span class="o">==</span> <span class="sh">"</span><span class="s">-1</span><span class="sh">"</span><span class="p">,</span> <span class="p">:]</span>

<span class="c1"># plot outliers and non-outliers separately
</span><span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">outliers</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">outliers</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">grey</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Outliers</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
    <span class="n">to_plot</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
    <span class="n">to_plot</span><span class="p">.</span><span class="n">y</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">to_plot</span><span class="p">.</span><span class="n">cluster</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">tab20b</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Clusters</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span> <span class="c1"># Add a legend to distinguish outliers and clusters
</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Visualization of Clustered Abstracts</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Add a title for context
</span><span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/text-clusters-outliers-visualization.png" alt="text clusters outliers visualization" width="35%" height="35%">
</div>
<div class="title">Figure 53. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization.</div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="from-text-clustering-to-topic-modeling">5.3. From Text Clustering to Topic Modeling</h3>
<div class="paragraph">
<p>Text clustering is a powerful tool for finding structure among large collections of documents, whereas topic modeling is the process of discovering underlying themes or latent topics within a collection of textual data, which typically involves finding a set of keywords or phrases that best represent and capture the meaning of the topic.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/textual-to-topics.png" alt="topics are represented by a number of keywords but can take other forms." width="35%" height="35%">
</div>
<div class="title">Figure 54. Traditionally, topics are represented by a number of keywords but can take other forms. Instead of labeling a topic as “sign language,” these techniques use keywords such as “sign,” “language,” and “translation” to describe the topic. As such, this does not give a single label to a topic and instead requires the user to understand the meaning of the topic through those keywords.</div>
</div>
<div class="sect3">
<h4 id="bertopic-a-modular-topic-modeling-framework">5.3.1. BERTopic: A Modular Topic Modeling Framework</h4>
<div class="paragraph">
<p>BERTopic is a topic modeling technique that leverages clusters of semantically similar texts to extract various types of topic representations.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/pipline-of-bertopic-clustering-topic-representation.png" alt="The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation." width="35%" height="35%">
</div>
<div class="title">Figure 55. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>First, similar to text clustering, it embeds documents, reduces their dimensionality, and then clusters these embeddings to group semantically similar texts.
.The first part of BERTopic’s pipeline is to create clusters of semantically similar documents.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/bertopic-clustering.png" alt="embed documents, reduce dimensionality, and cluster the reduced embedding to create groups of semantically similar documents." width="35%" height="35%">
</div>
</div>
</li>
<li>
<p>Second, it models word distributions using a bag-of-words approach, counting word frequencies within documents to help extract the most frequent terms.</p>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>The bag-of-words approach does exactly what its name implies: it counts the number of times each word appears in a document, which can then be used to extract the most frequent words within that document.</p>
</div>
</blockquote>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/doc-bag-of-words.png" alt="A bag-of-words counts the number of times each word appears inside a document." width="35%" height="35%">
</div>
<div class="title">Figure 56. A bag-of-words counts the number of times each word appears inside a document.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/gen-c-tf-word-counting.png" alt="Generating c-TF by counting the frequency of words per cluster instead of per document." width="35%" height="35%">
</div>
<div class="title">Figure 57. Generating c-TF by counting the frequency of words per cluster instead of per document.</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="prompt-engineering">6. Prompt Engineering</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Prompt engineering is the art and science of crafting effective prompts to guide large language models (LLMs) and other generative AI systems to produce desired and high-quality outputs. It involves understanding how these models interpret and respond to different phrasings, instructions, and contexts within a prompt to achieve specific goals, such as generating creative text, answering questions accurately, or performing tasks effectively.</p>
</div>
<div class="sect2">
<h3 id="using-text-generation-models">6.1. Using Text Generation Models</h3>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># determine the device
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="c1"># load model and tokenizer
</span><span class="n">model_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">microsoft/Phi-4-mini-instruct</span><span class="sh">'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="c1"># create a pipeline
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">text-generation</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">return_full_text</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># prompt
</span><span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Create a funny joke about chickens.</span><span class="sh">'</span><span class="p">}]</span>

<span class="c1"># generate the output
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">generated_text</span><span class="sh">'</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="sect3">
<h4 id="prompt-template">6.1.1. Prompt Template</h4>
<div class="ulist">
<ul>
<li>
<p>Under the hood, <code>transformers.pipeline</code> first converts the messages into a specific prompt template which was used during the training of the model.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># apply prompt template
</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">&lt;s&gt;</span>&lt;|user|&gt;
<span class="gp">Create a funny joke about chickens.&lt;|end|&gt;</span><span class="w">
</span><span class="gp">&lt;|assistant|&gt;</span></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/phi-3-prompt-template.png" alt="The template Phi-3 expects when interacting with the model." width="35%" height="35%">
</div>
<div class="title">Figure 58. The template Phi-3 expects when interacting with the model.</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="controlling-model-output">6.1.2. Controlling Model Output</h4>
<div class="ulist">
<ul>
<li>
<p>Each time an LLM needs to generate a token, it assigns a likelihood number to each possible token to generate different responses for the exact same prompt.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/next-token-likelihood-score.png" alt="The model chooses the next token to generate based on their likelihood scores." width="35%" height="35%">
</div>
<div class="title">Figure 59. The model chooses the next token to generate based on their likelihood scores.</div>
</div>
</li>
<li>
<p>The <code>temperature</code> controls the randomness or creativity of the text generated; a higher temperature increases creativity by making less probable tokens more likely, while a temperature of <code>0</code> results in deterministic output by always selecting the most probable token.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># using a high temperature
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/temperature.png" alt="A higher temperature increases the likelihood that less probable tokens are generated and vice versa." width="35%" height="35%">
</div>
<div class="title">Figure 60. A higher temperature increases the likelihood that less probable tokens are generated and vice versa.</div>
</div>
</li>
<li>
<p>The <code>top-p</code>, or nucleus sampling, is a technique that controls the subset of tokens (the nucleus) an LLM considers for generation by including tokens until their cumulative probability reaches a specified threshold.</p>
<div class="paragraph">
<p>For instance, if <code>top_p</code> is set to <code>0.1</code>, the model will consider tokens until their cumulative probability reaches 10%, and if <code>top_p</code> is set to <code>1</code>, all tokens will be considered.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># using a high top_p
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/top_p.png" alt="A higher top_p increases the number of tokens that can be selected to generate and vice versa." width="35%" height="35%">
</div>
<div class="title">Figure 61. A higher top_p increases the number of tokens that can be selected to generate and vice versa.</div>
</div>
</li>
<li>
<p>The <code>top_k</code> parameter directly limits the number of most probable tokens an LLM considers; setting it to 100 restricts the selection to only the top 100 tokens.</p>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Use case examples when selecting values for temperature and top_p.</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 13.3333%;">
<col style="width: 6.6666%;">
<col style="width: 60.0001%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Example use case</th>
<th class="tableblock halign-left valign-top">temperature</th>
<th class="tableblock halign-left valign-top">top_p</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Brainstorming session</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High randomness with large pool of potential tokens. The results will be highly diverse, often leading to very creative and unexpected results.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Email generation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deterministic output with high probable predicted tokens. This results in predictable, focused, and conservative outputs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Creative writing</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High randomness with a small pool of potential tokens. This combination produces creative outputs but still remains coherent.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Translation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deterministic output with high probable predicted tokens. Produces coherent output with a wider range of vocabulary, leading to outputs with linguistic variety.</p></td>
</tr>
</tbody>
</table>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="prompt-engineering-2">6.2. Prompt Engineering</h3>
<div class="paragraph">
<p>Prompt engineering is the iterative process of designing effective prompts, including questions, statements, or instructions, to elicit useful and relevant outputs from LLMs through experimentation and optimization.</p>
</div>
<div class="paragraph">
<p>A prompt is the input provided to a large language model to elicit a desired response, which generally consists of multiple components such as instructions, data, and output indicators, and can be as complex as needed.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/basic-prompt.png" alt="Basic prompt" width="20%" height="20%">
</div>
<div class="title">Figure 62. A basic example of a prompt. No instruction is given so the LLM will simply try to complete the sentence.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/instruction-prompt.png" alt="Instruction prompt" width="25%" height="25%">
</div>
<div class="title">Figure 63. Two components of a basic instruction prompt: the instruction itself and the data it refers to.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/extend-prompt.png" alt="Extend instruction prompt" width="25%" height="25%">
</div>
<div class="title">Figure 64. Extending the prompt with an output indicator that allows for a specific output.</div>
</div>
</div>
<div class="sect2">
<h3 id="instruction-based-prompting">6.3. Instruction-Based Prompting</h3>
<div class="paragraph">
<p>Instruction-based prompting is a method of prompting where the primary goal is to have the LLM answer a specific question or resolve a certain task by providing it with specific instructions.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/prompt-example-common-use-cases.png" alt="Prompt examples of common use cases." width="35%" height="35%">
</div>
<div class="title">Figure 65. Prompt examples of common use cases. Notice how within a use case, the structure and location of the instruction can be changed.</div>
</div>
<div class="paragraph">
<p>Each of these tasks requires different prompting formats and more specifically, asking different questions of the LLM. A non-exhaustive list of the prompting techniques includes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Specificity</p>
<div class="paragraph">
<p>Accurately describe the desired output, for example, instead of "Write a product description," ask "Write a product description in under two sentences using a formal tone."</p>
</div>
<div class="paragraph">
<p>Specificity is arguably the most important aspect; by restricting and specifying what the model should generate, there is a smaller chance of it generating something unrelated to a use case.</p>
</div>
</li>
<li>
<p>Hallucination</p>
<div class="paragraph">
<p>LLMs may generate incorrect information confidently, which is referred to as hallucination.</p>
</div>
<div class="paragraph">
<p>To reduce its impact, ask the LLM to only generate an answer if it knows the answer, and to respond with "I don’t know" if it does not know the answer.</p>
</div>
</li>
<li>
<p>Order</p>
<div class="paragraph">
<p>Either begin or end the prompt with the instruction.</p>
</div>
<div class="paragraph">
<p>Especially with long prompts, information in the middle is often forgotten.</p>
</div>
<div class="paragraph">
<p>LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect).</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="advanced-prompt-engineering">6.4. Advanced Prompt Engineering</h3>
<div class="paragraph">
<p>While creating a good prompt might initially seem straightforward—just ask a specific question, be accurate, and add examples—prompting can quickly become complex and is often an underestimated aspect of effectively using LLMs.</p>
</div>
<div class="sect3">
<h4 id="prompt-components">6.4.1. Prompt Components</h4>
<div class="paragraph">
<p>A prompt generally consists of multiple components, such as instruction, data, and output indicators, and other advanced components that can quickly make a prompt quite complex.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/prompt-with-many-components.png" alt="An example of a complex prompt with many components." width="45%" height="45%">
</div>
<div class="title">Figure 66. An example of a complex prompt with many components.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/iteration-over-modular-components-prompt.png" alt="Iterating over modular components is a vital part of prompt engineering." width="35%" height="35%">
</div>
<div class="title">Figure 67. Iterating over modular components is a vital part of prompt engineering.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># prompt components
</span><span class="n">persona</span> <span class="o">=</span> <span class="sh">'</span><span class="s">You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.</span><span class="se">\n</span><span class="sh">'</span>
<span class="n">instruction</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Summarize the key findings of the paper provided.</span><span class="se">\n</span><span class="sh">'</span>
<span class="n">context</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.</span><span class="se">\n</span><span class="sh">'</span>
<span class="n">data_format</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.</span><span class="se">\n</span><span class="sh">'</span>
<span class="n">audience</span> <span class="o">=</span> <span class="sh">'</span><span class="s">The summary is designed for busy researchers that quickly need to grasp the newest trends in Large Language Models.</span><span class="se">\n</span><span class="sh">'</span>
<span class="n">tone</span> <span class="o">=</span> <span class="sh">'</span><span class="s">The tone should be professional and clear.</span><span class="se">\n</span><span class="sh">'</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">'</span><span class="s">MY TEXT TO SUMMARIZE</span><span class="sh">'</span>
<span class="n">data</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">Text to summarize: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">'</span>

<span class="c1"># the full prompt - remove and add pieces to view its impact on the generated output
</span><span class="n">query</span> <span class="o">=</span> <span class="n">persona</span> <span class="o">+</span> <span class="n">instruction</span> <span class="o">+</span> <span class="n">context</span> <span class="o">+</span> <span class="n">data_format</span> <span class="o">+</span> <span class="n">audience</span> <span class="o">+</span> <span class="n">tone</span> <span class="o">+</span> <span class="n">data</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="in-context-learning-providing-examples">6.4.2. In-Context Learning: Providing Examples</h4>
<div class="paragraph">
<p><em>In-context learning</em> (ICL) is a prompting technique that demonstrates the desired task to an LLM through direct examples, rather than solely describing it to provide the model with context to learn from within the prompt.</p>
</div>
<div class="paragraph">
<p>Zero-shot prompting does not leverage examples, one-shot prompts use a single example, and few-shot prompts use two or more examples.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/prompt-shots.png" alt="prompt shots" width="45%" height="45%">
</div>
<div class="title">Figure 68. An example of a complex prompt with many components.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># use a single example of using the made-up word in a sentence
</span><span class="n">one_shot_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">A </span><span class="se">\'</span><span class="s">Gigamuru</span><span class="se">\'</span><span class="s"> is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:</span><span class="sh">'</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">assistant</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.</span><span class="sh">'</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">To </span><span class="se">\'</span><span class="s">screeg</span><span class="se">\'</span><span class="s"> something is to swing a sword at it. An example of a sentence that uses the word screeg is:</span><span class="sh">'</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">apply_chat_template</span><span class="p">(</span><span class="n">one_shot_prompt</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="txt">&lt;|user|&gt;A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:&lt;|end|&gt;&lt;|assistant|&gt;I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.&lt;|end|&gt;&lt;|user|&gt;To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:&lt;|end|&gt;&lt;|endoftext|&gt;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># generate the output
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">one_shot_prompt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">In the medieval fantasy novel, the knight would screeg his enemies with his gleaming sword.</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="chain-prompting-breaking-up-the-problem">6.4.3. Chain Prompting: Breaking up the Problem</h4>
<div class="paragraph">
<p>Prompt chaining is a technique that addresses complex tasks by breaking them down across multiple prompts, where the output of one prompt serves as the input for the subsequent prompt, creating a sequence of interactions that collectively solve the problem.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/chain-prompts-create-product.png" alt="Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch." width="40%" height="40%">
</div>
<div class="title">Figure 69. Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># create name and slogan for a product
</span><span class="n">product_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Create a name and slogan for a chatbot that leverages LLMs.</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">product_prompt</span><span class="p">)</span>
<span class="n">product_description</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">product_description</span><span class="p">)</span>

<span class="c1"># based on a name and slogan for a product, generate a sales pitch
</span><span class="n">sales_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Generate a very short sales pitch for the following product: </span><span class="sh">'</span><span class="si">{</span><span class="n">product_description</span><span class="si">}</span><span class="sh">'"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">sales_prompt</span><span class="p">)</span>
<span class="n">sales_pitch</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">sales_pitch</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Name: LexiBot

Slogan: "Unlock the Power of Language with LexiBot – Your AI Conversation Partner!"

Discover the future of communication with LexiBot – your AI conversation partner. Say goodbye to language barriers and hello to seamless, intelligent interactions. LexiBot is here to unlock the power of language, making every conversation more engaging and productive. Embrace the power of AI with LexiBot today!</span></code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="reasoning-with-generative-models">6.5. Reasoning with Generative Models</h3>
<div class="paragraph">
<p>Reasoning is a core component of human intelligence and is often compared to the emergent behavior of LLMs that often resembles reasoning (through memorization of training data and pattern matching, rather than true reasoning).</p>
</div>
<div class="paragraph">
<p>Human reasoning can be broadly categorized into two systems.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>System 1 thinking represents an automatic, intuitive, and near-instantaneous process, which shares similarities with generative models that automatically generate tokens without any self-reflective behavior.</p>
</li>
<li>
<p>System 2 thinking, in contrast, is a conscious, slow, and logical process, akin to brainstorming and self-reflection.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking, would be emulated by giving a generative model the ability to mimic a form of self-reflection.</p>
</div>
<div class="sect3">
<h4 id="chain-of-thought-think-before-answering">6.5.1. Chain-of-Thought: Think Before Answering</h4>
<div class="paragraph">
<p>Chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps ("thoughts") before giving a final answer.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Although chain-of-thought is a great method for enhancing the output of a generative model, it does require one or more examples of reasoning in the prompt, which the user might not have access to.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/chain-of-thoughts-prompting.png" alt="Chain-of-thought prompting" width="45%" height="45%">
</div>
<div class="title">Figure 70. Chain-of-thought prompting uses reasoning examples to persuade the generative model to use reasoning in its answer.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># answering with chain-of-thought
</span><span class="n">cot_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">]</span>

<span class="c1"># generate the output
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">cot_prompt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">The cafeteria started with 23 apples. They used 20, so they had 23 - 20 = 3 apples left. Then they bought 6 more, so they now have 3 + 6 = 9 apples. The answer is 9.</span></code></pre>
</div>
</div>
</li>
<li>
<p>Instead of providing examples, zero-shot chain-of-thought allows a generative model to provide reasoning without explicit examples by directly prompting it for its thought process.</p>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Although the prompt “Let’s think step by step” can improve the output, you are not constrained by this exact formulation. Alterna‐ tives exist like “Take a deep breath and think step-by-step” and “Let’s work through this problem step-by-step.”</p>
</div>
</blockquote>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/cot-step-by-step.png" alt="Zero-shot chain-of-thought" width="35%" height="35%">
</div>
<div class="title">Figure 71. Chain-of-thought prompting without using examples. Instead, it uses the phrase “Let’s think step-by-step” to prime reasoning in its answer.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># zero-shot chain-of-thought prompt
</span><span class="n">zeroshot_cot_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let</span><span class="sh">'</span><span class="s">s think step-by-step.</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># generate the output
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">zeroshot_cot_prompt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Sure, let's break it down step-by-step:

1. The cafeteria starts with 23 apples.
2. They use 20 apples to make lunch.
3. After using 20 apples, they have:
   23 apples - 20 apples = 3 apples left.
4. They then buy 6 more apples.
5. Adding the 6 new apples to the 3 apples they have left:
   3 apples + 6 apples = 9 apples.

So, the cafeteria now has 9 apples.</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="self-consistency-sampling-outputs">6.5.2. Self-Consistency: Sampling Outputs</h4>
<div class="paragraph">
<p>Self-consistency is a technique that reduces randomness in generative models by prompting them multiple times with the same input, using varied sampling parameters like <code>temperature</code> and <code>top_p</code> to enhance diversity, and selecting the majority result as the final answer for robustness.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/sampling-multiple-reasoning-paths.png" alt="Self-Consistency: Sampling Outputs" width="45%" height="45%">
</div>
<div class="title">Figure 72. By sampling from multiple reasoning paths, we can use majority voting to extract the most likely answer.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># zero-shot chain-of-thought prompt
</span><span class="n">zeroshot_cot_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let</span><span class="sh">'</span><span class="s">s think step-by-step.</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># self-consistency settings
</span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>
<span class="n">top_p</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>


<span class="c1"># extract final numerical answers
</span><span class="k">def</span> <span class="nf">extract_answer</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">numbers</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\d+</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>  <span class="c1"># find all numbers in the output
</span>    <span class="nf">return </span><span class="p">(</span>
        <span class="n">numbers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">numbers</span> <span class="k">else</span> <span class="bp">None</span>
    <span class="p">)</span>  <span class="c1"># take the last number as the final answer
</span>

<span class="c1"># generate multiple answers
</span><span class="n">answers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span>
        <span class="n">zeroshot_cot_prompt</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">temperature</span><span class="p">)],</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">top_p</span><span class="p">)],</span>
    <span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">].</span><span class="nf">strip</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="se">\n</span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">'</span>
    <span class="n">final_answer</span> <span class="o">=</span> <span class="nf">extract_answer</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">final_answer</span><span class="p">:</span>
        <span class="n">answers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">final_answer</span><span class="p">)</span>

<span class="c1"># perform majority voting on numerical answers
</span><span class="n">most_common_answer</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="n">answers</span><span class="p">).</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">generated answers:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ans</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">answers</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">. </span><span class="si">{</span><span class="n">ans</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">final answer (majority vote): </span><span class="si">{</span><span class="n">most_common_answer</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Sure, let's break it down step-by-step:

1. The cafeteria starts with 23 apples.
2. They use 20 apples to make lunch.
3. After using 20 apples, they have:
   23 apples - 20 apples = 3 apples left.
4. They then buy 6 more apples.
5. Adding the 6 apples to the 3 apples they have left gives:
   3 apples + 6 apples = 9 apples.

So, the cafeteria

Sure, let's break it down step-by-step:

1. The cafeteria starts with 23 apples.
2. They use 20 apples to make lunch.
3. After using 20 apples, they have:
   23 apples - 20 apples = 3 apples left.
4. They then buy 6 more apples.
5. Adding the 6 new apples to the 3 apples they have left, they now have:
   3 apples + 6 apples = 9 apples.

Sure, let's break it down step by step:

1. The cafeteria starts with 23 apples.
2. They use 20 apples to make lunch.
   - 23 apples - 20 apples = 3 apples remaining.
3. They then buy 6 more apples.
   - 3 apples + 6 apples = 9 apples.

So, after these transactions, the cafeteria has 9 apples.

generated answers:
1. 9
2. 9
3. 9

final answer (majority vote): 9</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="tree-of-thought-exploring-intermediate-steps">6.5.3. Tree-of-Thought: Exploring Intermediate Steps</h4>
<div class="paragraph">
<p>Tree-of-Thought (ToT) is a problem-solving technique structuring reasoning as a decision tree that explores multiple potential solutions at each step, evaluates them, and branches forward with the most promising, similar to brainstorming, to enhance the final outcome.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/tree-of-thought.png" alt="Tree-of-though" width="30%" height="30%">
</div>
<div class="title">Figure 73. By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned.</div>
</div>
<div class="paragraph">
<p>Tree-of-Thought excels at tasks requiring exploration of multiple paths, such as creative writing, but its reliance on numerous generative model calls can be slow.</p>
</div>
<div class="paragraph">
<p>A more efficient approach involves prompting the model to simulate a multi-expert discussion to reach a consensus, mimicking the ToT framework with a single call.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># zero-shot tree-of-thought prompt
</span><span class="n">zeroshot_tot_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">"</span><span class="s">Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they</span><span class="sh">'</span><span class="s">re wrong at any point then they leave. The question is </span><span class="sh">'</span><span class="s">The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?</span><span class="sh">'</span><span class="s"> Make sure to discuss the results.</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># generate the output
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">zeroshot_tot_prompt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">generated_text</span><span class="sh">'</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">**Expert 1:**
Step 1: Start with the initial number of apples, which is 23.

**Expert 2:**
Step 1: Subtract the apples used for lunch, which is 20, from the initial 23 apples. This leaves 3 apples.

**Expert 3:**
Step 1: Add the 6 apples that were bought to the remaining 3 apples. This results in 9 apples.

**Discussion:**
All three experts agree on the final result. The cafeteria started with 23 apples, used 20 for lunch, leaving them with 3 apples. Then, they bought 6 more apples, bringing the total to 9 apples. Therefore, the cafeteria now has 9 apples.</span></code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="output-verification">6.6. Output Verification</h3>
<div class="paragraph">
<p>Systems and applications built with generative models might eventually end up in production. When that happens, it is important to verify and control the output of the model to prevent breaking the application and to create a robust generative AI application.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>By default, most generative models create free-form text without adhering to specific structures other than those defined by natural language.</p>
<div class="paragraph">
<p>Some use cases require their output to be structured in certain formats, like JSON.</p>
</div>
</li>
<li>
<p>Even allowing the model to generate structured output, it still has the capability to freely generate its content.</p>
<div class="paragraph">
<p>For instance, when a model is asked to output either one of two choices, it should not come up with a third.</p>
</div>
</li>
<li>
<p>Some open source generative models have no guardrails and will generate outputs that do not consider safety or ethical considerations.</p>
<div class="paragraph">
<p>For instance, use cases might require the output to be free of profanity, personally identifiable information (PII), bias, cultural stereotypes, etc.</p>
</div>
</li>
<li>
<p>Many use cases require the output to adhere to certain standards or performance.</p>
<div class="paragraph">
<p>The aim is to double-check whether the generated information is factually accurate, coherent, or free from hallucination.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Generally, there are three ways of controlling the output of a generative model:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Examples: Provide a number of examples of the expected output.</p>
</li>
<li>
<p>Grammar: Control the token selection process.</p>
</li>
<li>
<p>Fine-tuning: Tune a model on data that contains the expected output.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="providing-examples">6.6.1. Providing Examples</h4>
<div class="paragraph">
<p>A simple and straightforward method to fix the output is to provide the generative model with examples of what the output should look like.</p>
</div>
<div class="paragraph">
<p>The <em>few-shot learning</em> is a helpful technique that guides the output of the generative model, which can be generalized to guide the structure of the output as well.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
An important note here is that it is still up to the model whether it will adhere to your suggested format or not. Some models are better than others at following instructions.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># zero-shot learning: providing no in-context examples
</span><span class="n">zeroshot_prompt</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Create a character profile for an RPG game in JSON format.</span><span class="sh">'</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># generate the output
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">zeroshot_prompt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">generated_text</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># one-shot learning: providing a single in-context example of the desired output structure
</span><span class="n">one_shot_template</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">Create a short character profile for an RPG game. Make
sure to only use this format:
{
  </span><span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="s">: </span><span class="sh">"</span><span class="s">A SHORT DESCRIPTION</span><span class="sh">"</span><span class="s">,
  </span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="s">: </span><span class="sh">"</span><span class="s">THE CHARACTER</span><span class="sh">'</span><span class="s">S NAME</span><span class="sh">"</span><span class="s">,
  </span><span class="sh">"</span><span class="s">armor</span><span class="sh">"</span><span class="s">: </span><span class="sh">"</span><span class="s">ONE PIECE OF ARMOR</span><span class="sh">"</span><span class="s">,
  </span><span class="sh">"</span><span class="s">weapon</span><span class="sh">"</span><span class="s">: </span><span class="sh">"</span><span class="s">ONE OR MORE WEAPONS</span><span class="sh">"</span><span class="s">
}
</span><span class="sh">'''</span>
<span class="n">one_shot_prompt</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="n">one_shot_template</span><span class="p">}]</span>

<span class="c1"># generate the output
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">one_shot_prompt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">generated_text</span><span class="sh">'</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{
  "name": "Eldrin Shadowbane",
  "class": "Rogue",
  "level": 10,
  "race": "Elf",
  "background": "Eldrin was born into a noble family in the elven city of Luminara. He was trained in the arts of stealth and combat from a young age. However, Eldrin always felt a deep connection to the shadows and the mysteries of the night. He left his family to become a rogue
{
  "description": "A skilled archer with a mysterious past, known for their agility and precision.",
  "name": "Lyra Swiftarrow",
  "armor": "Leather bracers and a lightweight leather tunic",
  "weapon": "Longbow, throwing knives"
}</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="grammar-constrained-sampling">6.6.2. Grammar: Constrained Sampling</h4>
<div class="paragraph">
<p>Few-shot learning has a significant disadvantage: explicitly preventing certain output is not possible. Although the model is guided and given instructions, it might still not follow them completely.</p>
</div>
<div class="paragraph">
<p>Grammar-constrained sampling is a technique used during the token generation process of a Large Language Model (LLM) that enforces adherence to predefined grammars or rules when selecting the next token.</p>
</div>
<div class="paragraph">
<p>Instead, packages have been rapidly developed to constrain and validate the output of generative models, like <a href="https://github.com/guidance-ai/guidance">Guidance</a>, <a href="https://github.com/guardrails-ai/guardrails">Guardrails</a>, and <a href="https://github.com/eth-sri/lmql">LMQL</a>, which leverage generative models to validate their own output.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-check-rules.png" alt="Use an LLM to check whether the output correctly follows our rules." width="35%" height="35%">
</div>
<div class="title">Figure 74. The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/llm-generate-pieces.png" alt="llm generate pieces" width="35%" height="35%">
</div>
<div class="title">Figure 75. Use an LLM to generate only the pieces of information we do not know beforehand.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/constrain-token-selection.png" alt="constrain token selection" width="35%" height="35%">
</div>
<div class="title">Figure 76. Constrain the token selection to only three possible tokens: “positive,” “neutral,” and “negative.”</div>
</div>
<div class="paragraph">
<p>Like transformers, <a href="https://github.com/abetlen/llama-cpp-python">llama-cpp-python</a> is a library, generally used to efficiently load and use compressed models (quantization) in the GGUF format but can also be used to apply a JSON grammar.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">llama_cpp.llama</span> <span class="kn">import</span> <span class="n">Llama</span>

<span class="c1"># load the Phi-3 language model using the llama-cpp-python library
</span><span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="sh">"</span><span class="s">microsoft/Phi-3-mini-4k-instruct-gguf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="sh">"</span><span class="s">*fp16.gguf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">n_gpu_layers</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_ctx</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># generate output using the loaded language model for a chat completion task
</span><span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">create_chat_completion</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Create a warrior for an RPG in JSON for mat.</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">],</span>
    <span class="n">response_format</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">json_object</span><span class="sh">"</span><span class="p">},</span> <span class="c1"># specify the response_format as a JSON
</span>    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)[</span><span class="sh">'</span><span class="s">choices</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">message</span><span class="sh">'</span><span class="p">][</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span>

<span class="kn">import</span> <span class="n">json</span>

<span class="c1"># check whether the output actually is JSON
</span><span class="n">json_output</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">json_output</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{
    "warrior": {
        "name": "Aldarion the Brave",
        "class": "Warrior",
        "level": 10,
        "attributes": {
            "strength": 18,
            "dexterity": 10,
            "constitution": 16,
            "intelligence": 8,
            "wisdom": 10,
            "charisma": 12
        },</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="advanced-text-generation-techniques-and-tools">7. Advanced Text Generation Techniques and Tools</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://python.langchain.com/">LangChain</a> is a framework for developing applications powered by large language models (LLMs), which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/langchian.png" alt="LangChain" width="45%" height="45%">
</div>
<div class="title">Figure 77. LangChain is a complete framework for using LLMs. It has modular compo‐ nents that can be chained together to allow for complex LLM systems.</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Hugging Face models can be run locally through the <a href="https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/"><code>HuggingFacePipeline</code></a> class.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># determine the device
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="c1"># load model and tokenizer
</span><span class="n">model_id</span> <span class="o">=</span> <span class="sh">'</span><span class="s">microsoft/Phi-4-mini-instruct</span><span class="sh">'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># create a pipeline
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">text-generation</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">return_full_text</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_huggingface.llms</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">HuggingFacePipeline</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">pipe</span><span class="p">)</span></code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="model-io-loading-quantized-models-with-langchain">7.1. Model I/O: Loading Quantized Models with LangChain</h3>
<div class="paragraph">
<p>A GGUF model represents a compressed version of its original counterpart through a method called quantization, which reduces the number of bits needed to represent the parameters of an LLM.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/pi-bit-representations.png" alt="pi with float 32-bit and float 16-bit representations" width="35%" height="35%">
</div>
<div class="title">Figure 78. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the lowered accuracy when we halve the number of bits.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Bits, a series of 0s and 1s, represent values through binary encoding; more bits allow for a wider range of values but demand greater memory for storage.</p>
</li>
<li>
<p>Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information.</p>
<div class="paragraph">
<p>Quantization comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original.</p>
</div>
<div class="paragraph">
<p>Like rounding the time to the nearest minute ("14:16") instead of including seconds ("14:16 and 12 seconds"), quantization reduces the precision of a value without losing essential information.</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>As a rule of thumb, look for at least 4-bit quantized models. These models have a good balance between compression and accuracy. Although it is possible to use 3-bit or even 2-bit quantized mod‐ els, the performance degradation becomes noticeable and it would instead be preferable to choose a smaller model with a higher precision.</p>
</div>
</blockquote>
</div>
</li>
<li>
<p>To download a specific bit-variant file (e.g., fp16) of the <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf">microsoft/Phi-3-mini-4k-instruct-gguf</a> model, which includes multiple files with different bit-variants (see the 'Files and versions' tab).</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="c"># download from the primary Hugging Face URL:</span>
wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf

<span class="c"># alternatively, download from the HF mirror:</span>
wget https://hf-mirror.com/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf</code></pre>
</div>
</div>
</li>
<li>
<p>Use <a href="https://python.langchain.com/docs/integrations/llms/llamacpp/">Llama.cpp</a> together with LangChain to load the GGUF file, and generate output.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf
# !pip install llama-cpp-python langchain_communit
</span><span class="kn">from</span> <span class="n">langchain_community.llms</span> <span class="kn">import</span> <span class="n">LlamaCpp</span>

<span class="c1"># initialize the LlamaCpp language model integration from Langchain
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">LlamaCpp</span><span class="p">(</span>
    <span class="c1"># path to the downloaded GGUF model file (ensure this file exists!)
</span>    <span class="n">model_path</span><span class="o">=</span><span class="sh">"</span><span class="s">Phi-3-mini-4k-instruct-fp16.gguf</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">n_gpu_layers</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">n_ctx</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># invoke the language model with a prompt.
</span><span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">Hi! My name is Maarten. What is 1 + 1?</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># no/meanless output! Phi-3 requires a specific prompt template.
</span><span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="chains-extending-the-capabilities-of-llms">7.2. Chains: Extending the Capabilities of LLMs</h3>
<div class="paragraph">
<p>In Langchain, a "chain" is a core concept that goes beyond running LLMs in isolation, which involves connecting an LLM with other components like prompts, tools, or even other chains, to enhance its capabilities and create more complex systems.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/single-chain.png" alt="A single chain connects some modular component" width="25%" height="25%">
</div>
<div class="title">Figure 79. A single chain connects some modular component, like a prompt template or external memory, to the LLM.</div>
</div>
<div class="sect3">
<h4 id="a-single-link-in-the-chain-prompt-template">7.2.1. A Single Link in the Chain: Prompt Template</h4>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/prompt-template-chain.png" alt="A Single Link in the Chain" width="25%" height="25%">
</div>
<div class="title">Figure 80. By chaining a prompt template with an LLM, we only need to define the input prompts. The template will be constructed for you.</div>
</div>
<div class="paragraph">
<p>By chaining a prompt template with an LLM to get the output, only the user and system prompts need to be defined for each interaction, eliminating the need to repeatedly define the full prompt template.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/prompt-chain-phi3-example.png" alt="prompt chain phi3 example" width="25%" height="25%">
</div>
<div class="title">Figure 81. An example of a single chain using Phi-3’s template.</div>
</div>
<div class="paragraph">
<p>The template for Phi-3 is comprised of four main components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>&lt;s&gt;</code> to indicate when the prompt starts</p>
</li>
<li>
<p><code>&lt;|user|&gt;</code> to indicate the start of the user’s prompt</p>
</li>
<li>
<p><code>&lt;|assistant|&gt;</code> to indicate the start of the model’s output</p>
</li>
<li>
<p><code>&lt;|end|&gt;</code> to indicate the end of either the prompt or the model’s output</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/phi3-prompt-template.png" alt="The prompt template Phi-3 expects." width="35%" height="35%">
</div>
<div class="title">Figure 82. The prompt template Phi-3 expects.</div>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># create a prompt template with a placeholder for the user's input
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">&lt;s&gt;&lt;|user|&gt; {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;</span><span class="sh">"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">input_prompt</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># create a simple chain with the prompt template and the language model
</span><span class="n">basic_chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span>

<span class="c1"># invoke the chain with the input for the prompt template
</span><span class="n">output</span> <span class="o">=</span> <span class="n">basic_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">input_prompt</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi! My name is Maarten. What is 1 + 1?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># the 'output' variable now contains the generated text
</span><span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Hello Maarten! The answer to 1 + 1 is 2.</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="a-chain-with-multiple-prompts">7.2.2. A Chain with Multiple Prompts</h4>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/multiple-prompt-chain.png" alt="A Chain with Multiple Prompts" width="25%" height="25%">
</div>
<div class="title">Figure 83. With sequential chains, the output of a prompt is used as the input for the next prompt.</div>
</div>
<div class="paragraph">
<p>A multiple prompt chain, or sequential chain, processes a complex task by dividing it into a series of smaller, sequential subtasks, where each subtask utilizes a distinct prompt and LLM call, with the output from one step feeding directly into the input of the subsequent step.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/prompt-multiple-template-chain.png" alt="A Chain with Multiple Prompts Example" width="25%" height="25%">
</div>
<div class="title">Figure 84. An example to generate a story that has three components: a title, a description of the main character, a summary of the story. The output of the title prompt is used as the input of the character prompt. To generate the story, the output of all previous prompts is used.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">json</span>
<span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span><span class="p">,</span> <span class="n">RunnableLambda</span>
<span class="kn">from</span> <span class="n">langchain.schema</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">'</span><span class="s">qwen2.5:0.5b-instruct</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="sh">'</span><span class="s">http://localhost:11434/v1</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># Ollama API
</span>    <span class="n">api_key</span><span class="o">=</span><span class="sh">'</span><span class="s">API-KEY</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">title_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">&lt;s&gt;&lt;|user|&gt;</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">Create a title for a story about {summary}.</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">Only return the title.</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">&lt;|end|&gt; &lt;|assistant|&gt;</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">character_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">&lt;s&gt;&lt;|user|&gt;</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">Describe the main character of a story about {summary} with the title {title}. </span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">Use only two sentences.</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">&lt;|end|&gt;&lt;|assistant|&gt;</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">story_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">&lt;s&gt;&lt;|user|&gt;</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">Create a story about {summary} with the title {title}.</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">The main character is: {character}. </span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">Only return the story and it cannot be longer than one paragraph.</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">&lt;|end|&gt;&lt;|assistant|&gt;</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># LCEL-style chain using Runnables
</span><span class="n">title_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">()}</span> <span class="o">|</span> <span class="n">title_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">character_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">(),</span> <span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">:</span> <span class="n">title_chain</span><span class="p">}</span>
    <span class="o">|</span> <span class="n">character_prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">story_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">:</span> <span class="n">title_chain</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">character</span><span class="sh">"</span><span class="p">:</span> <span class="n">character_chain</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="o">|</span> <span class="n">story_prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">aggregate_chain</span> <span class="o">=</span> <span class="nc">RunnableLambda</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">inputs</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">character</span><span class="sh">"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">character</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">story</span><span class="sh">"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">story</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">final_chain</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">:</span> <span class="n">title_chain</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">character</span><span class="sh">"</span><span class="p">:</span> <span class="n">character_chain</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">story</span><span class="sh">"</span><span class="p">:</span> <span class="n">story_chain</span><span class="p">,</span>
<span class="p">}</span> <span class="o">|</span> <span class="n">aggregate_chain</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">final_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">a girl that lost her mother</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{
  "summary": {
    "summary": "a girl that lost her mother"
  },
  "title": "\"Lost Mother Girl\"",
  "character": "In the story, the main character named Lily, who was born to an ordinary family, unexpectedly finds herself the daughter of a rich individual after losing her mother. She navigates this new reality with courage and strength, learning valuable lessons about empathy, perseverance, and the power of resilience.",
  "story": "In the quiet village where Linxue lived, her mother had been gone for many years. As an only child, she often felt distant from the other children in the village. One day,</span></code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="memory-helping-llms-to-remember-conversations">7.3. Memory: Helping LLMs to Remember Conversations</h3>
<div class="paragraph">
<p>Memory can be added to the LLM chain using methods like conversation buffers and conversation summaries to make chat models stateful to remember previous conversations.</p>
</div>
<div class="sect3">
<h4 id="conversation-buffer">7.3.1. Conversation Buffer</h4>
<div class="paragraph">
<p>In Langchain, <code>ConversationBufferMemory</code> provides an intuitive way to give LLMs memory by updating the prompt to include the full chat history.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/conversation-buffer.png" alt="Conversation Buffer" width="35%" height="35%">
</div>
<div class="title">Figure 85. We can remind an LLM of what previously happened by simply appending the entire conversation history to the input prompt.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history}

{input}&lt;|end|&gt;
&lt;|assistant|&gt;</span><span class="sh">"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationBufferMemory</span>

<span class="n">memory</span> <span class="o">=</span> <span class="nc">ConversationBufferMemory</span><span class="p">(</span><span class="n">memory_key</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.chains.llm</span> <span class="kn">import</span> <span class="n">LLMChain</span>

<span class="n">llm_chain</span> <span class="o">=</span> <span class="nc">LLMChain</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi! My name is Maarten. What is 1 + 1?</span><span class="sh">"</span><span class="p">})</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'input': 'Hi! My name is Maarten. What is 1 + 1?',
 'chat_history': '',
 'text': 'Nice to meet you, Maarten!\n\nThe answer to 1 + 1 is... 2!'}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is my name?</span><span class="sh">"</span><span class="p">})</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'input': 'What is my name?',
 'chat_history': 'Human: Hi! My name is Maarten. What is 1 + 1?\nAI: Nice to meet you, Maarten!\n\nThe answer to 1 + 1 is... 2!',
 'text': 'Nice to meet you too, Maarten! Your name is indeed Maarten. Would you like to ask another question or have a conversation?'}</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="windowed-conversation-buffer">7.3.2. Windowed Conversation Buffer</h4>
<div class="paragraph">
<p>In LangChain, <code>ConversationBufferWindowMemory</code> decides how many the last <em>k</em> conversations are passed to the input prompt.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history}

{input}&lt;|end|&gt;
&lt;|assistant|&gt;</span><span class="sh">"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationBufferWindowMemory</span>

<span class="n">memory</span> <span class="o">=</span> <span class="nc">ConversationBufferWindowMemory</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">memory_key</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.chains.llm</span> <span class="kn">import</span> <span class="n">LLMChain</span>

<span class="n">llm_chain</span> <span class="o">=</span> <span class="nc">LLMChain</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>

<span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="sh">"</span><span class="s">Hi! My name is Maarten and I am 33 years old. What is 1 + 1?</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="sh">"</span><span class="s">What is 3 + 3?</span><span class="sh">"</span><span class="p">)</span>
<span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is my name?</span><span class="sh">"</span><span class="p">})</span>
<span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is my age?</span><span class="sh">"</span><span class="p">})</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="conversation-summary">7.3.3. Conversation Summary</h4>
<div class="paragraph">
<p>In LangChain, <code>ConversationSummaryMemory</code> summarizes the entire conversation history (typically using an external LLM) before providing it to the input prompt.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/conversation-summary.png" alt="Conversation Summary" width="35%" height="35%">
</div>
<div class="title">Figure 86. Instead of passing the conversation history directly to the prompt, we use another LLM to summarize it first.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history}

{input}&lt;|end|&gt;
&lt;|assistant|&gt;</span><span class="sh">"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationSummaryMemory</span>

<span class="c1"># prepare a summarization template as the summarization prompt
</span><span class="n">summary_prompt_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">&lt;s&gt;&lt;|user|&gt;Summarize the conversations and update
with the new lines.
Current summary:
{summary}
new lines of conversation:
{new_lines}
New summary:&lt;|end|&gt;
&lt;|assistant|&gt;</span><span class="sh">"""</span>
<span class="n">summary_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">summary_prompt_template</span><span class="p">)</span>

<span class="n">memory</span> <span class="o">=</span> <span class="nc">ConversationSummaryMemory</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">memory_key</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">summary_prompt</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.chains.llm</span> <span class="kn">import</span> <span class="n">LLMChain</span>

<span class="n">llm_chain</span> <span class="o">=</span> <span class="nc">LLMChain</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi! My name is Maarten. What is 1 + 1?</span><span class="sh">"</span><span class="p">})</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'input': 'Hi! My name is Maarten. What is 1 + 1?',
 'chat_history': '',
 'text': 'Hi Maarten!\n\nThe answer to 1 + 1 is 2.'}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is my name?</span><span class="sh">"</span><span class="p">})</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'input': 'What is my name?',
 'chat_history': "Here is the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n\nNew lines of conversation:\nHuman: That's correct, what's 2 * 2?\nAI: Let me calculate... The answer to 2 * 2 is 4.",
 'text': 'Hi Maarten! Your name was mentioned earlier in our conversation. You said "Hi! My name is Maarten." What can I help you with next?'}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">llm_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What was the first question I asked?</span><span class="sh">"</span><span class="p">})</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'input': 'What was the first question I asked?',
 'chat_history': 'Here\'s the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n* Human: That\'s correct, what\'s 2 * 2?\n* AI: Let me calculate... The answer to 2 * 2 is 4.\n* Human: What is my name?\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said "Hi! My name is Maarten." What can I help you with next?',
 'text': 'The first question you asked was: "what\'s 1 + 1?"'}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># check what the summary is thus far
</span><span class="n">memory</span><span class="p">.</span><span class="nf">load_memory_variables</span><span class="p">({})</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'chat_history': 'Here is the updated summary:\n\nCurrent summary:\n\n* Human: Hi! My name is Maarten. What is 1 + 1?\n* AI: Hi Maarten!\n* Answer: The answer to 1 + 1 is 2.\n* Human: That\'s correct, what\'s 2 * 2?\n* AI: Let me calculate... The answer to 2 * 2 is 4.\n* Human: What is my name?\n* AI: Hi Maarten! Your name was mentioned earlier in our conversation. You said "Hi! My name is Maarten." What can I help you with next?\n* Human: What was the first question I asked?\n* AI: The first question you asked was: "what\'s 1 + 1?"'}</span></code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="agents-creating-a-system-of-llms">7.4. Agents: Creating a System of LLMs</h3>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/agents/">Agents</a> are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.</p>
</div>
<div class="paragraph">
<p>ReAct (Reasoning and Acting) is a cognitive framework for language models that interleaves reasoning ("Thoughts") and acting ("Actions") with observations, allowing the model to dynamically plan, execute, and learn from its interactions with external tools or environments to solve complex tasks.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/react-prompt-template-example.png" alt="An example of a ReAct prompt template." width="35%" height="35%">
</div>
<div class="title">Figure 87. An example of a ReAct prompt template.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/two-cycles-react-pipeline-example.png" alt="An example of two cycles in a ReAct pipeline." width="45%" height="45%">
</div>
<div class="title">Figure 88. An example of two cycles in a ReAct pipeline.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="c1"># an LLM that is powerful enough to properly follow complex instructions
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">mistral:7b-instruct</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># "llama3.1:8b", # "llama3.2:1b",
</span>    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="sh">"</span><span class="s">http://localhost:11434/v1</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="sh">"</span><span class="s">API-KEY</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># create the ReAct template
</span><span class="n">react_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the following questions as best you can. You have
access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Agents: Creating a System of LLMs
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
</span><span class="gp">...</span> <span class="p">(</span><span class="n">this</span> <span class="n">Thought</span><span class="o">/</span><span class="n">Action</span><span class="o">/</span><span class="n">Action</span> <span class="n">Input</span><span class="o">/</span><span class="n">Observation</span> <span class="n">can</span> <span class="n">repeat</span> <span class="n">N</span> <span class="n">times</span><span class="p">)</span>
<span class="n">Thought</span><span class="p">:</span> <span class="n">I</span> <span class="n">now</span> <span class="n">know</span> <span class="n">the</span> <span class="n">final</span> <span class="n">answer</span>
<span class="n">Final</span> <span class="n">Answer</span><span class="p">:</span> <span class="n">the</span> <span class="n">final</span> <span class="n">answer</span> <span class="n">to</span> <span class="n">the</span> <span class="n">original</span> <span class="nb">input</span> <span class="n">question</span>

<span class="s">Begin!

Question: {input}
Thought:{agent_scratchpad}</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="n">react_template</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">tools</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tool_names</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">agent_scratchpad</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.agents</span> <span class="kn">import</span> <span class="n">load_tools</span><span class="p">,</span> <span class="n">Tool</span>
<span class="kn">from</span> <span class="n">langchain_community.tools.ddg_search.tool</span> <span class="kn">import</span> <span class="n">DuckDuckGoSearchResults</span>

<span class="n">search</span> <span class="o">=</span> <span class="nc">DuckDuckGoSearchResults</span><span class="p">()</span>
<span class="n">search_tool</span> <span class="o">=</span> <span class="nc">Tool</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">duckduck</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">A web search engine. Use this to as a search engine for general queries.</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">func</span><span class="o">=</span><span class="n">search</span><span class="p">.</span><span class="n">run</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">tools</span> <span class="o">=</span> <span class="nf">load_tools</span><span class="p">([</span><span class="sh">"</span><span class="s">llm-math</span><span class="sh">"</span><span class="p">],</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span>
<span class="n">tools</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">search_tool</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.agents</span> <span class="kn">import</span> <span class="n">AgentExecutor</span><span class="p">,</span> <span class="n">create_react_agent</span>

<span class="n">agent</span> <span class="o">=</span> <span class="nf">create_react_agent</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">tools</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
<span class="n">agent_executor</span> <span class="o">=</span> <span class="nc">AgentExecutor</span><span class="p">(</span>
    <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">handle_parsing_errors</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">agent_executor</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is 123 + 456?</span><span class="sh">"</span>
    <span class="p">}</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">&gt;</span><span class="w"> </span>Entering new AgentExecutor chain...
<span class="go"> To solve this, I will use the Calculator tool. The input for the calculator will be the equation "123 + 456".

Action: Calculator
Action Input: "123 + 456"Answer: 579 I now know the final answer.
Final Answer: The result of the calculation (123 + 456) is 579.

</span><span class="gp">&gt;</span><span class="w"> </span>Finished chain.
<span class="go">
{'input': 'What is 123 + 456?',
 'output': 'The result of the calculation (123 + 456) is 579.'}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">agent_executor</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.</span><span class="sh">"</span>
    <span class="p">}</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">&gt;</span><span class="w"> </span>Entering new AgentExecutor chain...
<span class="go"> I need to find the current price of a MacBook Pro and then convert that price from USD to EUR using the given exchange rate.

Agents: Calculator, duckduck
Action: duckduck
</span><span class="gp">Action Input: What is the current price of a MacBook Pro in USD?snippet: Apple resellers are hosting a variety of MacBook Pro sales that discount current M4, M4 Pro and M4 Max 14-inch and 16-inch models, in addition to blowout bargains on M3 models. Apple offers two ..., title: Best MacBook Pro Deals for March 2025 | Save up to $</span>1,200 - AppleInsider, <span class="nb">link</span>: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: The newly launched M4 Pro and M4 Max 14-inch MacBook Pros have shown notable performance improvements over their M1, M2, and M3 counterparts, especially <span class="k">in </span>single-core scores. In recent benchmarks, the M4 Pro 14-inch MacBook Pro achieved a single-core score of approximately 3,850, surpassing the M3 Pro<span class="s1">'s single-core score by about 15-20%., title: Apple 14″ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/14-macbook-pro/, snippet: Apple MacBook Pro 14" (M4/512GB): was $1,599 now $1,399 at Amazon. The M4-based MacBook Pro M4 is pretty close to being the perfect laptop. You get fantastic performance from the M4 chip, useful ..., title: Epic Apple MacBook sale is live — shop the best deals from $629 right ..., link: https://www.tomsguide.com/sales-events/epic-apple-macbook-sale-is-live-shop-the-best-deals-from-usd629-right-now, snippet: The M4 Max MacBook Pro is Apple'</span>s most powerful option, and both the silver and space black options are on sale. ... List price Best price <span class="o">(</span>current<span class="o">)</span> Best price <span class="o">(</span>all-time<span class="o">)</span> M2 MacBook Air <span class="o">(</span>13-inch ..., title: Best MacBook Deals: Save on Apple<span class="s1">'s Latest Laptops and Previous-Gen ..., link: https://www.cnet.com/deals/best-macbook-deals/ The current price of a MacBook Pro in USD can be found from the search results. Let me filter the results a bit more specifically to find the price.
</span><span class="go">
Agents: duckduck
Action: duckduck
</span><span class="gp">Action Input: What is the price of a new 14-inch MacBook Pro (M4/512GB) in USD?snippet: - 14″ M4 MacBook Pro (16GB/1TB/Gray): $</span>1599, <span class="nv">$200</span> off MSRP - 14″ M4 MacBook Pro <span class="o">(</span>24GB/1TB/Gray<span class="o">)</span>: <span class="nv">$1799</span>, <span class="nv">$200</span> off MSRP. These are currently the lowest prices available <span class="k">for </span>new M4-powered 14″ MacBook Pros among the Apple retailers we track. For the latest sales and prices, keep an eye on our 14-inch MacBook Pro Price Tracker, updated daily., title: 14-inch M4 MacBook Pros on sale today <span class="k">for</span> <span class="nv">$150</span>-<span class="nv">$200</span> off MSRP, <span class="nb">link</span>: https://www.macprices.net/2025/01/14/14-inch-m4-macbook-pros-on-sale-today-for-150-200-off-msrp/, snippet: Every M4 Pro and M4 Max model is also on sale at up to <span class="nv">$300</span> off <span class="k">in </span>our Mac Price Guide. Prices start at <span class="nv">$1</span>,699. Here are a few top picks from the MacBook Pro sale: 14-inch M4, 16GB, 512GB, Space ..., title: Apple M4 MacBook Pro Drops to <span class="nv">$1</span>,399, Free Next Day Shipping - AppleInsider, <span class="nb">link</span>: https://appleinsider.com/articles/24/12/25/snag-an-m4-macbook-pro-14-inch-for-1399-with-free-next-day-delivery, snippet: The M4 Pro MacBook Pro 14-inch has hit a new record low price of <span class="nv">$1</span>,699, with units <span class="k">in </span>stock with free store pickup as early as today. But don<span class="s1">'t delay, as the deal ends on Christmas Eve., title: Apple MacBook Pro 14-inch M4 Pro Drops to Best $1,699 Price - AppleInsider, link: https://appleinsider.com/articles/24/12/24/apples-14-inch-macbook-pro-with-m4-pro-chip-plunges-to-record-low-1699-today-only, snippet: Right now the 14-inch MacBook Pro is available with a discount that slashes its price to the lowest yet, and you won'</span>t want to miss out. Amazon is now selling the M4 MacBook Pro <span class="k">for </span>just <span class="nv">$1</span>,398 ..., title: Apple<span class="s1">'s Latest M4 14-inch MacBook Pro Is Now Yours for Its Best-Ever Price, link: https://www.cnet.com/deals/apples-latest-m4-14-inch-macbook-pro-is-now-yours-for-its-best-ever-price/ The current price of a new 14-inch MacBook Pro (M4/512GB) in USD is $1399. To find the cost in EUR, we can use the given exchange rate of 0.85 EUR for 1 USD. So, the cost of the MacBook Pro in EUR would be 1399 * 0.85 = €1176.21.
</span><span class="go">
Final Answer: The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR.

</span><span class="gp">&gt;</span><span class="w"> </span>Finished chain.
<span class="go">
{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.',
 'output': 'The current price of a new 14-inch MacBook Pro (M4/512GB) is approximately €1176.21 in EUR.'}</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="langchain">Appendix A: LangChain</h3>
<div class="paragraph">
<p>LangChain is a framework that consists of a number of packages, which implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://python.langchain.com/svg/langchain_stack_112024.svg" alt="Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers." width="45%" height="45%">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>langchain-core</code> is a lightweight package containing base abstractions and interfaces for core Langchain components like chat models, vector stores, and tools, without including any third-party integrations and with minimal dependencies.</p>
</li>
<li>
<p><code>langchain</code> is the main package containing generic chains and retrieval strategies that form an application&#8217;s cognitive architecture, independent of specific third-party integrations.</p>
</li>
<li>
<p>Integrations are a list of lightweight packages (e.g., <code>langchain-openai</code>, <code>langchain-anthropic</code>) that contain specific integrations and are co-maintained for proper versioning.</p>
</li>
<li>
<p><code>langchain-community</code> is a package containing third-party integrations for various components (chat models, vector stores, tools, etc.), maintained by the Langchain community, with all dependencies being optional to ensure a lightweight package.</p>
</li>
<li>
<p><code>langgraph</code> is an extension of <code>langchain</code> aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.</p>
</li>
<li>
<p><code>langserve</code> is a package to deploy LangChain chains as REST APIs that makes it easy to get a production ready API up and running.</p>
</li>
<li>
<p>LangSmith is a developer platform for debugging, testing, evaluating, and monitoring LLM applications.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="chat-models-and-messages">7.A.1. Chat Models and Messages</h4>
<div class="paragraph">
<p>Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.</p>
</div>
<div class="paragraph">
<p>LangChain provides a consistent interface for working with <a href="https://python.langchain.com/docs/concepts/chat_models/">chat models</a> from different providers that takes a list of <a href="https://python.langchain.com/docs/messages/">messages</a> as input and returns a <a href="https://python.langchain.com/docs/messages/">message</a> as output while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.</p>
</div>
<div class="paragraph">
<p>LangChain supports two message formats to interact with chat models:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>LangChain Message Format: LangChain&#8217;s own message format, which is used by default and is used internally by LangChain.</p>
</li>
<li>
<p>OpenAI&#8217;s Message Format: OpenAI&#8217;s message format.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/messages/">Messages</a> are the unit of communication in <a href="https://python.langchain.com/docs/concepts/chat_models/">chat models</a>, which are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Each message has a role (e.g., "user", "assistant") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.</p>
</li>
<li>
<p>LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.</p>
</li>
<li>
<p>LangChain messages are Python objects that subclass from a <code>BaseMessage</code>.</p>
<div class="ulist">
<ul>
<li>
<p><code>SystemMessage</code>: corresponds to <code>system</code> role</p>
</li>
<li>
<p><code>HumanMessage</code>: corresponds to <code>user</code> role</p>
</li>
<li>
<p><code>AIMessage</code>: corresponds to <code>assistant</code> role</p>
</li>
<li>
<p><code>AIMessageChunk</code>: corresponds to <code>assistant</code> role, used for streaming responses</p>
</li>
<li>
<p><code>ToolMessage</code>: corresponds to <code>tool</code> role</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When invoking a chat model with a string as input, LangChain will automatically convert the string into a <code>HumanMessage</code> object.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">model</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello, how are you?</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">llm</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">'</span><span class="s">What is LangChain?</span><span class="sh">'</span><span class="p">)</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="prompt-templates">7.A.2. Prompt Templates</h4>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/prompt_templates">Prompt Templates</a> are responsible for formatting user input into a format that can be passed to a language model, take as input a dictionary, where each key represents a variable in the prompt template to fill in, and output a PromptValue.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="sh">"</span><span class="s">Tell me a joke about {topic}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="sh">"</span><span class="s">topic</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cats</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="c1"># Tell me a joke about cats</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="nc">ChatPromptTemplate</span><span class="p">([</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">You are a helpful assistant</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Tell me a joke about {topic}</span><span class="sh">"</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="sh">"</span><span class="s">topic</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cats</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="c1"># System: You are a helpful assistant
# Human: Tell me a joke about cats</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">MessagesPlaceholder</span>
<span class="kn">from</span> <span class="n">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="nc">ChatPromptTemplate</span><span class="p">([</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">You are a helpful assistant</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">MessagesPlaceholder</span><span class="p">(</span><span class="sh">"</span><span class="s">msgs</span><span class="sh">"</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="sh">"</span><span class="s">msgs</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="nc">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">hi!</span><span class="sh">"</span><span class="p">)]})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="c1"># System: You are a helpful assistant
# Human: hi!
</span>
<span class="c1"># alternatively
</span><span class="n">prompt_template</span> <span class="o">=</span> <span class="nc">ChatPromptTemplate</span><span class="p">([</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">You are a helpful assistant</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">placeholder</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{msgs}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># &lt;-- This is the changed part
</span><span class="p">])</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="sh">"</span><span class="s">msgs</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="nc">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">hi!</span><span class="sh">"</span><span class="p">)]})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="c1"># System: You are a helpful assistant
# Human: hi!</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="structured-outputs">7.A.3. Structured Outputs</h4>
<div class="paragraph">
<p>Structured outputs are a concept where language models are instructed to respond in a structured format, rather than in direct natural language, which is useful in scenarios where the output needs to be machine-readable, such as storing output in a database and ensure that the output conforms to the database schema.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://python.langchain.com/assets/images/structured_output-2c42953cee807dedd6e96f3e1db17f69.png" alt="Structured output" width="45%" height="45%">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>LangChain provides a method, <code>with_structured_output()</code>, that automates the process of binding the schema to the model and parsing the output.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>

<span class="k">class</span> <span class="nc">ResponseFormatter</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Always use this tool to structure your response to the user.</span><span class="sh">"""</span>
    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">The answer to the user</span><span class="sh">'</span><span class="s">s question</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">followup_question</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">A followup question the user could ask</span><span class="sh">"</span><span class="p">)</span>

<span class="n">llm_with_structure</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">with_structured_output</span><span class="p">(</span><span class="n">ResponseFormatter</span><span class="p">)</span>
<span class="n">structured_output</span> <span class="o">=</span> <span class="n">llm_with_structure</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">What is the powerhouse of the cell?</span><span class="sh">"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">structured_output</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">ResponseFormatter(answer='The powerhouse of the cell is the mitochondria.', followup_question='What is the organelle that powers the cell?')</span></code></pre>
</div>
</div>
</li>
<li>
<p>While one approach is to include defined schema in the prompt and ask nicely for the model to use it, it is not recommended.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain.output_parsers.structured</span> <span class="kn">import</span> <span class="n">ResponseSchema</span><span class="p">,</span> <span class="n">StructuredOutputParser</span>

<span class="n">response_schemas</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">ResponseSchema</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">The answer to the user</span><span class="sh">'</span><span class="s">s question</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="nc">ResponseSchema</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">followup_question</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">A followup question the user could ask</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">]</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">StructuredOutputParser</span><span class="p">.</span><span class="nf">from_response_schemas</span><span class="p">(</span><span class="n">response_schemas</span><span class="p">)</span>
<span class="n">format_instructions</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()</span>

<span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="sh">"</span><span class="s">{query}</span><span class="se">\n</span><span class="s">{format_instructions}</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">format_instructions</span><span class="sh">"</span><span class="p">:</span> <span class="n">format_instructions</span><span class="p">},</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is the powerhouse of the cell?</span><span class="sh">"</span><span class="p">}))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">What is the powerhouse of the cell?
The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":

```json
{
	"answer": string  // The answer to the user's question
	"followup_question": string  // A followup question the user could ask
}
```</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is the powerhouse of the cell?</span><span class="sh">"</span><span class="p">})</span>
<span class="n">output</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'answer': 'The powerhouse of the cell is the nucleus.',
 'followup_question': 'What does the nucleus play a crucial role in?'}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="output-parsers">7.A.4. Output Parsers</h4>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/output_parsers/">Output Parsers</a> are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks, which are useful when using LLMs to generate structured data, or to normalize output from chat models and LLMs.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># parse text from message objects
</span><span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">llm</span> <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">'</span><span class="s">What is 2 + 2 ?</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># 2 + 2 equals 4.</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># use output parsers to parse an LLM response into structured format
</span><span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">PydanticOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">model_validator</span>

<span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">question to set up a joke</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">punchline</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">answer to resolve the joke</span><span class="sh">"</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="nc">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">Joke</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="sh">"</span><span class="s">Answer the user query.</span><span class="se">\n</span><span class="s">{format_instructions}</span><span class="se">\n</span><span class="s">{query}</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">format_instructions</span><span class="sh">"</span><span class="p">:</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()},</span>
<span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Tell me a joke.</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">model_dump_json</span><span class="p">(</span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># {
#   "setup": "Why did the tomato turn red?",
#   "punchline": "Because it saw the salad dressing!"
# }</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># parse JSON output
</span><span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">JsonOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>

<span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">question to set up a joke</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">punchline</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">answer to resolve the joke</span><span class="sh">"</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="nc">JsonOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">Joke</span><span class="p">)</span>

<span class="n">instructions</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="se">\n</span><span class="si">{</span><span class="n">instructions</span><span class="si">}</span><span class="se">\n</span><span class="s">---------------</span><span class="sh">'</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="sh">"</span><span class="s">Answer the user query.</span><span class="se">\n</span><span class="s">{format_instructions}</span><span class="se">\n</span><span class="s">{query}</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">format_instructions</span><span class="sh">"</span><span class="p">:</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()},</span>
<span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Tell me a joke.</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># The output should be formatted as a JSON instance that conforms to the JSON schema below.
#
# As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
# the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.
#
# Here is the output schema:
# ```
# {"properties": {"setup": {"description": "question to set up a joke", "title": "Setup", "type": "string"}, "punchline": {"description": "answer to resolve the joke", "title": "Punchline", "type": "string"}}, "required": ["setup", "punchline"]}
# ```
# ---------------
# {'setup': 'Why did the tomato turn red?', 'punchline': 'Because it saw the salad dressing!'}</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="embedding-vector-stores-and-retrievers">7.A.5. Embedding, Vector Stores, and Retrievers</h4>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/embedding_models/">Embedding models</a> are machine learning models that transform human language or multimodal data (text, audio, images, video - not currently fully supported by Langchain) into numerical vector representations (embeddings), which are fixed-length arrays capturing the semantic meaning of the input, enabling machines to understand and compare data based on conceptual similarity, not just keywords.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://python.langchain.com/assets/images/embeddings_concept-975a9aaba52de05b457a1aeff9a7393a.png" alt="Embedding Model Conceptual Overview" width="55%" height="55%">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>(1) Embed text as a vector: Embeddings transform text into a numerical vector representation.</p>
</li>
<li>
<p>(2) Measure similarity: Embedding vectors can be compared using simple mathematical operations.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>LangChain provides a universal interface for working with embedding models, providing standard methods for common operations, and simplifies interaction with various embedding providers through two central methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>embed_documents</code>: For embedding multiple texts (documents)</p>
</li>
<li>
<p><code>embed_query</code>: For embedding a single text (query)</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># for embedding multiple texts (documents)
</span><span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="n">embeddings_model</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">()</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="p">.</span><span class="nf">embed_documents</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="sh">"</span><span class="s">Hi there!</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Oh, hello!</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What</span><span class="sh">'</span><span class="s">s your name?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">My friends call me World</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Hello World!</span><span class="sh">"</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nf">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1536</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># for embedding a single text (query)
</span><span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="p">.</span><span class="nf">embed_query</span><span class="p">(</span><span class="sh">"</span><span class="s">What is the meaning of life?</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># measure similarity
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">):</span>
    <span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
    <span class="n">norm_vec1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">vec1</span><span class="p">)</span>
    <span class="n">norm_vec2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">vec2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dot_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_vec1</span> <span class="o">*</span> <span class="n">norm_vec2</span><span class="p">)</span>

<span class="n">similarity</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">query_result</span><span class="p">,</span> <span class="n">document_result</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Cosine Similarity:</span><span class="sh">"</span><span class="p">,</span> <span class="n">similarity</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># hugging face embeddings
</span><span class="kn">from</span> <span class="n">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">embed_query</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello, world!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">))</span> <span class="c1"># 384</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/vectorstores/">Vector stores</a> are databases that can efficiently store and retrieve embeddings, which are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png" alt="Vector stores" width="45%" height="45%">
</div>
</div>
<div class="paragraph">
<p>LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations. The key methods are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>add_documents</code>: Add a list of texts to the vector store.</p>
</li>
<li>
<p><code>delete</code>: Delete a list of documents from the vector store.</p>
</li>
<li>
<p><code>similarity_search</code>: Search for similar documents to a given query.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_core.vectorstores</span> <span class="kn">import</span> <span class="n">InMemoryVectorStore</span>

<span class="c1"># initialize with an embedding model
</span><span class="n">vector_store</span> <span class="o">=</span> <span class="nc">InMemoryVectorStore</span><span class="p">(</span><span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># add documents
</span><span class="kn">from</span> <span class="n">langchain_core.documents</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">document_1</span> <span class="o">=</span> <span class="nc">Document</span><span class="p">(</span>
    <span class="n">page_content</span><span class="o">=</span><span class="sh">"</span><span class="s">I had chocalate chip pancakes and scrambled eggs for breakfast this morning.</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">source</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">tweet</span><span class="sh">"</span><span class="p">},</span>
<span class="p">)</span>

<span class="n">document_2</span> <span class="o">=</span> <span class="nc">Document</span><span class="p">(</span>
    <span class="n">page_content</span><span class="o">=</span><span class="sh">"</span><span class="s">The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">source</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">news</span><span class="sh">"</span><span class="p">},</span>
<span class="p">)</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">document_1</span><span class="p">,</span> <span class="n">document_2</span><span class="p">]</span>

<span class="n">vector_store</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">)</span>
<span class="c1"># ['df0f6926-c824-4114-a2c5-2b19d9d8740c', 'fa105761-9dd6-4c1c-860a-28e3e4ba181a']
</span>
<span class="c1"># provide IDs for the documents to the vector store
</span><span class="n">vector_store</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">doc1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">doc2</span><span class="sh">"</span><span class="p">])</span>
<span class="c1"># ['doc1', 'doc2']
</span>
<span class="c1"># delete documents
</span><span class="n">vector_store</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">doc1</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># similarity search
</span><span class="n">query</span> <span class="o">=</span> <span class="sh">"</span><span class="s">my query</span><span class="sh">"</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">page_content</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/retrievers/">Retrievers</a> in Langchain are components that provide a unified way to interact with various retrieval systems, including vector stores, graph databases, and relational databases, and take a natural language query as input to return a list of relevant documents.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>LangChain provides a uniform interface for interacting with different types of retrieval systems that accepts a query and return documents.</p>
</li>
<li>
<p>A Langchain retriever is a <code>runnable</code>, which is a standard interface for Langchain components, and it has a few common methods, including <code>invoke</code>, that are used to interact with it.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Lost in the Middle is the phenomenon where Large Language Models (LLMs) have difficulty effectively using information located in the middle of a long input context, often performing better when relevant details are at the beginning or end.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Documents retrieved from vector stores are typically returned in descending order of relevance, often measured by cosine similarity of embeddings.</p>
</li>
<li>
<p>To mitigate the "lost in the middle" effect, re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle.</p>
</li>
<li>
<p>The <code>LongContextReorder</code> document transformer implements the re-ordering procedure.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_core.vectorstores</span> <span class="kn">import</span> <span class="n">InMemoryVectorStore</span>

<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Basquetball is a great sport.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Fly me to the moon is one of my favourite songs.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">The Celtics are my favourite team.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">This is a document about the Boston Celtics</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">I simply love going to the movies</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">The Boston Celtics won the game by 20 points</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">This is just a random text.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Elden Ring is one of the best games in the last 15 years.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">L. Kornet is one of the best Celtics players.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Larry Bird was an iconic NBA player.</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">vector_store</span> <span class="o">=</span> <span class="n">InMemoryVectorStore</span><span class="p">.</span><span class="nf">from_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="n">langchain_core.documents</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="c1"># create a retriever
</span><span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">retriever</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Document</span><span class="p">]:</span>
    <span class="n">docs</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">vector_store</span><span class="p">.</span><span class="nf">similarity_search_with_score</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">):</span>
        <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>

    <span class="k">return</span> <span class="n">docs</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="n">max_score_length</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>

<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
    <span class="n">score_str</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">.</span><span class="nf">rjust</span><span class="p">(</span><span class="n">max_score_length</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- </span><span class="si">{</span><span class="n">score_str</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">- 0.675469: This is a document about the Boston Celtics
- 0.638917: The Celtics are my favourite team.
- 0.552694: L. Kornet is one of the best Celtics players.
- 0.460651: The Boston Celtics won the game by 20 points
- 0.320224: Larry Bird was an iconic NBA player.
- 0.244521: Elden Ring is one of the best games in the last 15 years.
- 0.231564: Basquetball is a great sport.
- 0.106447: I simply love going to the movies
- 0.059917: Fly me to the moon is one of my favourite songs.
- 0.034081: This is just a random text.</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_community.document_transformers</span> <span class="kn">import</span> <span class="n">LongContextReorder</span>

<span class="c1"># Reorder the documents:
# Less relevant document will be at the middle of the list and more
# relevant elements at beginning / end.
</span><span class="n">reordering</span> <span class="o">=</span> <span class="nc">LongContextReorder</span><span class="p">()</span>
<span class="n">reordered_docs</span> <span class="o">=</span> <span class="n">reordering</span><span class="p">.</span><span class="nf">transform_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="c1"># Confirm that the 4 relevant documents are at beginning and end.
</span><span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">reordered_docs</span><span class="p">:</span>
    <span class="n">score_str</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">.</span><span class="nf">rjust</span><span class="p">(</span><span class="n">max_score_length</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- </span><span class="si">{</span><span class="n">score_str</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">- 0.638917: The Celtics are my favourite team.
- 0.460651: The Boston Celtics won the game by 20 points
- 0.244521: Elden Ring is one of the best games in the last 15 years.
- 0.106447: I simply love going to the movies
- 0.034081: This is just a random text.
- 0.059917: Fly me to the moon is one of my favourite songs.
- 0.231564: Basquetball is a great sport.
- 0.320224: Larry Bird was an iconic NBA player.
- 0.552694: L. Kornet is one of the best Celtics players.
- 0.675469: This is a document about the Boston Celtics</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="document-loaders">7.A.6. Document Loaders</h4>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/document_loaders/">Document Loaders</a> are responsible for loading documents from a variety of sources.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># simple and fast text extraction
</span>
<span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">PyPDFLoader</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./books/llm-book.pdf</span><span class="sh">"</span>

<span class="n">loader</span> <span class="o">=</span> <span class="nc">PyPDFLoader</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="n">pages</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">.</span><span class="nf">lazy_load</span><span class="p">():</span>
    <span class="n">pages</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">page</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">pages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">metadata</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">pages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">page_content</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'source': './books/llm-book.pdf', 'page': 0, 'page_label': 'Cover'}

Hands-On
Large Language
Models
Language Understanding
and Generation
Jay Alammar &amp;
Maarten Grootendorst</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># vector search over PDFs
</span><span class="kn">from</span> <span class="n">langchain_core.vectorstores</span> <span class="kn">import</span> <span class="n">InMemoryVectorStore</span>
<span class="kn">from</span> <span class="n">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">vector_store</span> <span class="o">=</span> <span class="n">InMemoryVectorStore</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">pages</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">vector_store</span><span class="p">.</span><span class="nf">similarity_search</span><span class="p">(</span><span class="sh">"</span><span class="s">What is Prompt Engineering?</span><span class="sh">"</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Page </span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">]</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">[</span><span class="si">:</span><span class="mi">300</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Page 194: Intro to Prompt Engineering
An essential part of working with text-generative LLMs is prompt engineering. By
carefully designing our prompts we can guide the LLM to generate desired responses.
Whether the prompts are questions, statements, or instructions, the main goal of
prompt engineering is to e

Page 219: Summary
In this chapter, we explored the basics of using generative models through prompt
engineering and output verification. We focused on the creativity and potential com‐
plexity that comes with prompt engineering. These components of a prompt are key
in generating and optimizing output appropri</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="text-splitters">7.A.7. Text Splitters</h4>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/text_splitters/">Text splitters</a> split documents into smaller, manageable chunks for use in downstream applications, particularly retrieval systems, to handle non-uniform document lengths, overcome model limitations, improve representation quality, enhance retrieval precision, and optimize computational resources.</p>
</div>
<div class="paragraph">
<p>Text splitting approaches include length-based methods (token or character), text-structure based methods (like recursive splitting that respects paragraphs and sentences), document-structure based methods (leveraging formats like Markdown or HTML), and semantic meaning based methods (analyzing content for significant meaning shifts).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">length_function</span><span class="o">=</span><span class="nb">len</span><span class="p">,</span>
    <span class="n">is_separator_regex</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">state_of_the_union.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">state_of_the_union</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">texts</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_text</span><span class="p">(</span><span class="n">state_of_the_union</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and
of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_community.document_loaders.text</span> <span class="kn">import</span> <span class="n">TextLoader</span>

<span class="n">loader</span> <span class="o">=</span> <span class="nc">TextLoader</span><span class="p">(</span><span class="sh">"</span><span class="s">state_of_the_union.txt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>
<span class="n">split_documents</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">split_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">split_documents</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' metadata={'source': 'state_of_the_union.txt'}
page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' metadata={'source': 'state_of_the_union.txt'}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">PyPDFLoader</span>

<span class="n">loader</span> <span class="o">=</span> <span class="nc">PyPDFLoader</span><span class="p">(</span><span class="sh">"</span><span class="s">./books/llm-book.pdf</span><span class="sh">"</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>
<span class="n">split_documents</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">split_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">split_documents</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">page_content='Hands-On
Large Language
Models
Language Understanding
and Generation
Jay Alammar &amp;' metadata={'source': './books/llm-book.pdf', 'page': 0, 'page_label': 'Cover'}
page_content='Jay Alammar &amp;
Maarten Grootendorst' metadata={'source': './books/llm-book.pdf', 'page': 0, 'page_label': 'Cover'}</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="tools">7.A.8. Tools</h4>
<div class="paragraph">
<p>LangChain&#8217;s <a href="https://python.langchain.com/docs/concepts/tools/">tool</a> abstraction links a Python <em>function</em> to a <em>schema</em> defining its <em>name</em>, <em>description</em>, and <em>expected arguments</em>, which <a href="https://python.langchain.com/docs/concepts/chat_models/">chat models</a> that support <a href="https://python.langchain.com/docs/concepts/tool_calling/">tool calling</a> (or <a href="https://platform.openai.com/docs/guides/function-calling">function calling</a>) can use to request the execution of a specific function with specific inputs</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://python.langchain.com/assets/images/tool_calling_components-bef9d2bcb9d3706c2fe58b57bf8ccb60.png" alt="Conceptual parts of tool calling" width="55%" height="55%">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>A key principle of tool calling is that the model decides when to use a tool based on the input&#8217;s relevance.</p>
<div class="imageblock">
<div class="content">
<img src="https://python.langchain.com/assets/images/tool_call_example-2348b869f9a5d0d2a45dfbe614c177a4.png" alt="Diagram of a tool call by a model" width="45%" height="45%">
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># tool creation
</span><span class="nd">@tool</span>
<span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Multiply a and b.</span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>

<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">multiply</span><span class="p">]</span>

<span class="c1"># tool binding
</span><span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">bind_tools</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># tool calling
</span><span class="n">output</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">What is 2 multiplied by 3?</span><span class="sh">"</span><span class="p">)</span>
<span class="n">output</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">tool_calls</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">('',
 [{'name': 'multiply',
   'args': {'a': 2, 'b': 3},
   'id': 'call_zerallda',
   'type': 'tool_call'}])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># model doesn't always need to call a tool
</span><span class="n">output</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello world!</span><span class="sh">"</span><span class="p">)</span>
<span class="n">output</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">tool_calls</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">('Hello! How can I assist you today?', [])</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="chat-history">7.A.9. Chat History</h4>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/chat_history/">Chat history</a> is sequence of messages, each of which is associated with a specific role, such as <code>user</code>, <code>assistant</code>, <code>system</code>, or <code>tool</code>, a record of the conversation between the user and the chat model, which is used to maintain context and state throughout the conversation.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://python.langchain.com/assets/images/conversation_patterns-0e4c2311b54fae7412f74b1408615432.png" alt="Conversation patterns" width="45%" height="45%">
</div>
</div>
<div class="paragraph">
<p>A full conversation often starts with a system message that sets the context for the conversation, and follows a combination of two alternating message patterns: user and assistant, representing a back-and-forth conversation, or assistant and tool, representing an "agentic" workflow where the assistant invokes tools for specific tasks.</p>
</div>
<div class="paragraph">
<p>All models have finite context windows, and <code>trim_messages</code> can be used to reduce the size of a chat history to a specified token count or specified message count.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.messages</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AIMessage</span><span class="p">,</span>
    <span class="n">HumanMessage</span><span class="p">,</span>
    <span class="n">SystemMessage</span><span class="p">,</span>
    <span class="n">trim_messages</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">SystemMessage</span><span class="p">(</span><span class="sh">"</span><span class="s">you</span><span class="sh">'</span><span class="s">re a good assistant, you always respond with a joke.</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">HumanMessage</span><span class="p">(</span><span class="sh">"</span><span class="s">i wonder why it</span><span class="sh">'</span><span class="s">s called langchain</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">AIMessage</span><span class="p">(</span>
        <span class="sh">'</span><span class="s">Well, I guess they thought </span><span class="sh">"</span><span class="s">WordRope</span><span class="sh">"</span><span class="s"> and </span><span class="sh">"</span><span class="s">SentenceString</span><span class="sh">"</span><span class="s"> just didn</span><span class="se">\'</span><span class="s">t have the same ring to it!</span><span class="sh">'</span>
    <span class="p">),</span>
    <span class="nc">HumanMessage</span><span class="p">(</span><span class="sh">"</span><span class="s">and who is harrison chasing anyways</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">AIMessage</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">Hmmm let me think.</span><span class="se">\n\n</span><span class="s">Why, he</span><span class="sh">'</span><span class="s">s probably chasing after the last cup of coffee in the office!</span><span class="sh">"</span>
    <span class="p">),</span>
    <span class="nc">HumanMessage</span><span class="p">(</span><span class="sh">"</span><span class="s">what do you call a speechless parrot</span><span class="sh">"</span><span class="p">),</span>
<span class="p">]</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># trimming based on token count
</span><span class="kn">from</span> <span class="n">langchain_core.messages.utils</span> <span class="kn">import</span> <span class="n">count_tokens_approximately</span>

<span class="nf">trim_messages</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">last</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">token_counter</span><span class="o">=</span><span class="n">count_tokens_approximately</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
    <span class="n">start_on</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">end_on</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tool</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">include_system</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">allow_partial</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># trimming based on message count
</span><span class="nf">trim_messages</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">last</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">token_counter</span><span class="o">=</span><span class="nb">len</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># message count
</span>    <span class="n">start_on</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">end_on</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tool</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">include_system</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),
 AIMessage(content="Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># using a chat model as a token counter
</span><span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="nf">trim_messages</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">first</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">token_counter</span><span class="o">=</span><span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o</span><span class="sh">"</span><span class="p">),</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># chaining
</span><span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o</span><span class="sh">"</span><span class="p">)</span>

<span class="n">trimmer</span> <span class="o">=</span> <span class="nf">trim_messages</span><span class="p">(</span>
    <span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">last</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
    <span class="n">start_on</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">end_on</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tool</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">include_system</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">trimmer</span> <span class="o">|</span> <span class="n">llm</span>
<span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.chat_history</span> <span class="kn">import</span> <span class="n">InMemoryChatMessageHistory</span>
<span class="kn">from</span> <span class="n">langchain_core.runnables.history</span> <span class="kn">import</span> <span class="n">RunnableWithMessageHistory</span>

<span class="n">chat_history</span> <span class="o">=</span> <span class="nc">InMemoryChatMessageHistory</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">dummy_get_session_history</span><span class="p">(</span><span class="n">session_id</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">session_id</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">return</span> <span class="nc">InMemoryChatMessageHistory</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">chat_history</span>


<span class="n">trimmer</span> <span class="o">=</span> <span class="nf">trim_messages</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">last</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">include_system</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">start_on</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">trimmer</span> <span class="o">|</span> <span class="n">llm</span>
<span class="n">chain_with_history</span> <span class="o">=</span> <span class="nc">RunnableWithMessageHistory</span><span class="p">(</span>
    <span class="n">chain</span><span class="p">,</span> <span class="n">dummy_get_session_history</span>
<span class="p">)</span>
<span class="n">chain_with_history</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span>
    <span class="p">[</span><span class="nc">HumanMessage</span><span class="p">(</span><span class="sh">"</span><span class="s">what do you call a speechless parrot</span><span class="sh">"</span><span class="p">)],</span>
    <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">configurable</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">session_id</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">}},</span>
<span class="p">)</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="memory">7.A.10. Memory</h4>
<div class="paragraph">
<p><a href="https://python.langchain.com/docs/concepts/memory/">Memory</a> is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. <em>Short-term memory</em>, or <em>thread-scoped memory</em>, can be recalled at any time from within a single conversational thread with a user. <em>Long-term memory</em> is shared across conversational threads, and can be recalled at any time and in any thread.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://langchain-ai.github.io/langgraph/concepts/img/memory/short-vs-long.png" alt="Memory in LLM" width="45%" height="45%">
</div>
</div>
</div>
<div class="sect3">
<h4 id="langchain-expression-language-lcel">7.A.11. LangChain Expression Language (LCEL)</h4>
<div class="paragraph">
<p>The <strong>L</strong>ang<strong>C</strong>hain <strong>E</strong>xpression Language (LCEL) uses a declarative approach, similar to a Unix pipe, to build new <code>Runnable</code> components from existing ones, where a <code>Runnable</code> created with LCEL is often referred to as a "chain" and fully implements the <code>Runnable</code> interface.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.vectorstores</span> <span class="kn">import</span> <span class="n">InMemoryVectorStore</span>
<span class="kn">from</span> <span class="n">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">InMemoryVectorStore</span><span class="p">.</span><span class="nf">from_texts</span><span class="p">(</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">harrison worked at kensho</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">()</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the question based only on the following context:
{context}

Question: {question}
</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="n">prompt_chain</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retriever</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">(),</span>
<span class="p">}</span> <span class="o">|</span> <span class="n">prompt</span>
<span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">where did harrison work?</span><span class="sh">"</span><span class="p">).</span><span class="nf">to_string</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Human: Answer the question based only on the following context:
[Document(id='d03a67c7-a031-43aa-a27c-6411f9dd0dba', metadata={}, page_content='harrison worked at kensho')]

Question: where did harrison work?</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">()</span>

<span class="n">retrieval_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retriever</span><span class="p">,</span> <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">where did harrison work?</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Harrison worked at Kensho.</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>In LCEL chains, the two main composition primitives are <code>RunnableSequence</code> and <code>RunnableParallel</code>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>RunnableSequence</code> is a composition primitive to chain multiple runnables sequentially, with the output of one runnable serving as the input to the next.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableSequence</span>
<span class="n">chain</span> <span class="o">=</span> <span class="nc">RunnableSequence</span><span class="p">([</span><span class="n">runnable1</span><span class="p">,</span> <span class="n">runnable2</span><span class="p">])</span>
<span class="n">final_output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">some_input</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>corresponds to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">output1</span> <span class="o">=</span> <span class="n">runnable1</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">some_input</span><span class="p">)</span>
<span class="n">final_output</span> <span class="o">=</span> <span class="n">runnable2</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">output1</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p><code>RunnableParallel</code> is a composition primitive to run multiple runnables concurrently, with the same input provided to each.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableParallel</span>
<span class="n">chain</span> <span class="o">=</span> <span class="nc">RunnableParallel</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">key1</span><span class="sh">"</span><span class="p">:</span> <span class="n">runnable1</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">key2</span><span class="sh">"</span><span class="p">:</span> <span class="n">runnable2</span><span class="p">,</span>
<span class="p">})</span>
<span class="n">final_output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">some_input</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="p">{</span>
    <span class="sh">"</span><span class="s">key1</span><span class="sh">"</span><span class="p">:</span> <span class="n">runnable1</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">some_input</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">key2</span><span class="sh">"</span><span class="p">:</span> <span class="n">runnable2</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">some_input</span><span class="p">),</span>
<span class="p">}</span></code></pre>
</div>
</div>
</li>
<li>
<p>The <code>|</code> (pipe) operator have been overloaded to create a <code>RunnableSequence</code> from two <code>Runnables</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">chain</span> <span class="o">=</span> <span class="n">runnable1</span> <span class="o">|</span> <span class="n">runnable2</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>is Equivalent to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">chain</span> <span class="o">=</span> <span class="nc">RunnableSequence</span><span class="p">([</span><span class="n">runnable1</span><span class="p">,</span> <span class="n">runnable2</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>is Equivalent to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">chain</span> <span class="o">=</span> <span class="n">runnable1</span><span class="p">.</span><span class="nf">pipe</span><span class="p">(</span><span class="n">runnable2</span><span class="p">)</span></code></pre>
</div>
</div>
</li>
<li>
<p>LCEL applies automatic type coercion to make it easier to compose chains.</p>
<div class="ulist">
<ul>
<li>
<p>Inside an LCEL expression, a dictionary is automatically converted to a <code>RunnableParallel</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">key1</span><span class="sh">"</span><span class="p">:</span> <span class="n">runnable1</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">key2</span><span class="sh">"</span><span class="p">:</span> <span class="n">runnable2</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">mapping</span> <span class="o">|</span> <span class="n">runnable3</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>is automatically converted to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">chain</span> <span class="o">=</span> <span class="nc">RunnableSequence</span><span class="p">([</span><span class="nc">RunnableParallel</span><span class="p">(</span><span class="n">mapping</span><span class="p">),</span> <span class="n">runnable3</span><span class="p">])</span></code></pre>
</div>
</div>
</li>
<li>
<p>Inside an LCEL expression, a function is automatically converted to a <code>RunnableLambda</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="k">def</span> <span class="nf">some_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">some_func</span> <span class="o">|</span> <span class="n">runnable1</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>is automatically converted to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">chain</span> <span class="o">=</span> <span class="nc">RunnableSequence</span><span class="p">([</span><span class="nc">RunnableLambda</span><span class="p">(</span><span class="n">some_func</span><span class="p">),</span> <span class="n">runnable1</span><span class="p">])</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>A <code>dict</code> object defines data routing in LCEL by mapping keys to Runnables, functions, or static values, while <code>RunnablePassthrough</code> duplicates data across the pipeline as a data conduit to orchestrate chain flow.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">()}</span>    <span class="c1"># capture initial input
</span>    <span class="o">|</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="n">llm_chain</span><span class="p">,</span>            <span class="c1"># generate LLM output
</span>        <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">()</span>  <span class="c1"># maintain original input
</span>    <span class="p">}</span>
<span class="p">)</span>
<span class="c1"># output: {"output": "LLM's answer", "input": "user's question"}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="semantic-search-and-retrieval-augmented-generation">8. Semantic Search and Retrieval-Augmented Generation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Dense retrieval, reranking, and Retrieval-Augmented Generation (RAG) represent three significant strategies for enhancing search using language models.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>Dense retrieval</em> systems rely on the concept of embeddings, and turn the search problem into retrieving the nearest neighbors of the search query (after both the query and the documents are converted into embeddings).</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/dense-retrieval.png" alt="Dense retrieval" width="35%" height="35%">
</div>
<div class="title">Figure 89. Dense retrieval is one of the key types of semantic search, relying on the similarity of text embeddings to retrieve relevant results.</div>
</div>
</li>
<li>
<p>A <em>reranking</em> language model is one of multiple steps in search system pipelines and is tasked with scoring the relevance of a subset of results against the query; the order of results is then changed based on these scores.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/reranking.png" alt="Rerankers" width="35%" height="35%">
</div>
<div class="title">Figure 90. Rerankers, the second key type of semantic search, take a search query and a collection of results, and reorder them by relevance, often resulting in vastly improved results.</div>
</div>
</li>
<li>
<p>An <em>RAG</em> (Retrieval-Augmented Generation) system is a text generation system that incorporates search capabilities to reduce hallucinations, increase factuality, and/or ground the generation model on a specific dataset.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/rag.png" alt="RAG" width="35%" height="35%">
</div>
<div class="title">Figure 91. A RAG system formulates an answer to a question and (preferably) cites its information sources.</div>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="semantic-search-with-language-models">8.1. Semantic Search with Language Models</h3>
<div class="paragraph">
<p>An embedding is a numeric representation of text, where each text is intuitively represented as a point (or a vector), and texts with similar meaning are close to each other in the high multi-dimensional embedding space.</p>
</div>
<div class="sect3">
<h4 id="dense-retrieval">8.1.1. Dense Retrieval</h4>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/dense-retrieval-query.png" alt="Dense retrieval" width="25%" height="25%">
</div>
<div class="title">Figure 92. Dense retrieval relies on the property that search queries will be close to their relevant results.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># dense retrieval with FAISS
</span><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">import</span> <span class="n">faiss</span>

<span class="n">text</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Artificial intelligence was founded as an academic discipline in 1956.
Alan Turing was the first person to conduct substantial research in AI.
Born in Maida Vale, London, Turing was raised in southern England.
</span><span class="sh">"""</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span> <span class="k">if</span> <span class="n">s</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># embedding the text chunks.
</span><span class="n">xb</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

<span class="c1"># building the search index.
</span><span class="n">d</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="p">.</span><span class="nc">IndexFlatL2</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">index</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>

<span class="c1"># search the index
</span><span class="n">q</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Who is Alan Turing?</span><span class="sh">"</span>
<span class="n">xq</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">q</span><span class="p">])</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Q: </span><span class="si">{</span><span class="n">q</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentences</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Sentence: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Distance: </span><span class="si">{</span><span class="n">distance</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Q: Who is Alan Turing?
  Sentence: Alan Turing was the first person to conduct substantial research in AI
  Distance: 0.4903
  Sentence: Born in Maida Vale, London, Turing was raised in southern England
  Distance: 1.0674
  Sentence: Artificial intelligence was founded as an academic discipline in 1956
  Distance: 1.4276</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># keyword search with BM25
</span><span class="kn">import</span> <span class="n">string</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">rank_bm25</span> <span class="kn">import</span> <span class="n">BM25Okapi</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">_stop_words</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>


<span class="k">def</span> <span class="nf">bm25_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">tokenized_doc</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">().</span><span class="nf">split</span><span class="p">():</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="p">.</span><span class="nf">strip</span><span class="p">(</span><span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">)</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_stop_words</span><span class="p">.</span><span class="n">ENGLISH_STOP_WORDS</span><span class="p">:</span>
            <span class="n">tokenized_doc</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenized_doc</span>


<span class="n">tokenized_corpus</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">text</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Artificial intelligence was founded as an academic discipline in 1956.
Alan Turing was the first person to conduct substantial research in AI.
Born in Maida Vale, London, Turing was raised in southern England.
</span><span class="sh">"""</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">passage</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
    <span class="n">tokenized_corpus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">bm25_tokenizer</span><span class="p">(</span><span class="n">passage</span><span class="p">))</span>

<span class="n">bm25</span> <span class="o">=</span> <span class="nc">BM25Okapi</span><span class="p">(</span><span class="n">tokenized_corpus</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">keyword_search</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input question:</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
    <span class="n">bm25_scores</span> <span class="o">=</span> <span class="n">bm25</span><span class="p">.</span><span class="nf">get_scores</span><span class="p">(</span><span class="nf">bm25_tokenizer</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
    <span class="n">top_n</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argpartition</span><span class="p">(</span><span class="n">bm25_scores</span><span class="p">,</span> <span class="o">-</span><span class="n">n</span><span class="p">)[</span><span class="o">-</span><span class="n">n</span><span class="p">:]</span>
    <span class="n">bm25_hits</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="sh">'</span><span class="s">corpus_id</span><span class="sh">'</span><span class="p">:</span> <span class="n">idx</span><span class="p">,</span> <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">:</span> <span class="n">bm25_scores</span><span class="p">[</span><span class="n">idx</span><span class="p">]}</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">top_n</span>
    <span class="p">]</span>
    <span class="n">bm25_hits</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">bm25_hits</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Top-3 lexical search (BM25) hits</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hit</span> <span class="ow">in</span> <span class="n">bm25_hits</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span>
            <span class="sh">"</span><span class="se">\t</span><span class="s">{:.3f}</span><span class="se">\t</span><span class="s">{}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                <span class="n">hit</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">],</span> <span class="n">texts</span><span class="p">[</span><span class="n">hit</span><span class="p">[</span><span class="sh">'</span><span class="s">corpus_id</span><span class="sh">'</span><span class="p">]].</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>


<span class="n">q</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Who is Alan Turing?</span><span class="sh">"</span>
<span class="nf">keyword_search</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">texts</span><span class="p">))</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Input question: Who is Alan Turing?
Top-3 lexical search (BM25) hits
	0.737	 Alan Turing was the first person to conduct substantial research in AI
	0.000	 Artificial intelligence was founded as an academic discipline in 1956
	0.000	 Born in Maida Vale, London, Turing was raised in southern England</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>It’s useful to be aware of some of the drawbacks of dense retrieval and how to address them.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Lack of Answer in Retrieved Texts</p>
<div class="paragraph">
<p>Dense retrieval always returns results based on semantic similarity, even if none of the texts actually contain the answer to the query. A potential solution is to implement a distance threshold to filter out results that are not sufficiently relevant. User feedback (click-through rates and satisfaction) can also help improve the system over time.</p>
</div>
</li>
<li>
<p>Difficulty with Exact Phrase Matches</p>
<div class="paragraph">
<p>Dense retrieval, relying on semantic similarity, may not perform well when a user is looking for an exact match of a specific phrase. In such cases, traditional keyword matching is more effective, suggesting the use of hybrid search systems that combine both approaches.</p>
</div>
</li>
<li>
<p>Domain Specificity</p>
<div class="paragraph">
<p>Dense retrieval models trained on data from one domain (e.g., internet and Wikipedia) may not generalize well to other, unseen domains (e.g., legal texts) without sufficient training data from that new domain.</p>
</div>
</li>
<li>
<p>Handling Multi-Sentence Answers</p>
<div class="paragraph">
<p>Dense retrieval systems face the challenge of how to best chunk long texts into embeddings. A key design parameter is deciding the optimal way to divide documents, as answers to some questions may span multiple sentences, and models have context size limitations. Chunking strategies include embedding per document (which can lose information) or embedding multiple chunks per document (which offers better coverage). Various chunking methods exist, such as by sentence, paragraph, or overlapping segments to retain context, with the best approach depending on the text and query types.</p>
</div>
</li>
<li>
<p>Scalability and Efficiency</p>
<div class="paragraph">
<p>While simple nearest neighbor search with tools like NumPy works for smaller datasets, for millions of vectors, optimized approximate nearest neighbor (ANN) search libraries like FAISS or Annoy are necessary for efficient retrieval. Vector databases like Weaviate or Pinecone offer additional functionalities like adding/deleting vectors without rebuilding the index and advanced filtering options.</p>
</div>
</li>
<li>
<p>Need for Fine-Tuning</p>
<div class="paragraph">
<p>To optimize dense retrieval for specific tasks, fine-tuning the embedding models with relevant query-result pairs (including negative examples) is crucial. This process aims to bring embeddings of relevant queries and results closer together in the vector space while pushing irrelevant ones further apart.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="reranking">8.1.2. Reranking</h4>
<div class="paragraph">
<p>A reranker takes in the search query and a number of search results, and returns the optimal ordering of these documents so the most relevant ones to the query are higher in ranking.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/search-reranker.png" alt="LLM rerankers operate on shortlisted search results by relevance." width="35%" height="35%">
</div>
<div class="title">Figure 93. LLM rerankers operate as part of a search pipeline with the goal of reordering a number of shortlisted search results by relevance.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/reranker-model-work.png" alt="How reranking models work" width="35%" height="35%">
</div>
<div class="title">Figure 94. A reranker assigns a relevance score to each document by looking at the document and the query at the same time.</div>
</div>
<div class="paragraph">
<p>For the retrieval, either lexical search, e.g. with a vector engine like Elasticsearch, or dense retrieval with a <a href="https://huggingface.co/models?library=sentence-transformers&amp;author=sentence-transformers"><code>SentenceTransformer</code></a> (a.k.a. bi-encoder) can be used. However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, a re-ranker based on a <a href="https://huggingface.co/models?library=sentence-transformers&amp;author=cross-encoder"><code>CrossEncoder</code></a> that scores the relevancy of all shortlisted candidates for the given search query can be used to output a ranked list.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/bi-cross-encoder.png" alt="Bi vs Cross Encoder" width="30%" height="30%">
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">bi_encoder</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">A man is eating food.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">A man is eating a piece of bread.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">The girl is carrying a baby.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">A man is riding a horse.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">A woman is playing violin.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Two men pushed carts through the woods.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">A man is riding a white horse on an enclosed ground.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">A monkey is playing drums.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">A cheetah is running behind its prey.</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">bi_encoder</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">query</span> <span class="o">=</span> <span class="sh">"</span><span class="s">A man is eating pasta.</span><span class="sh">"</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">query_embedding</span> <span class="o">=</span> <span class="n">bi_encoder</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">top_N</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>
<span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">bi_encoder</span><span class="p">.</span><span class="nf">similarity</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">corpus_embeddings</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="kn">import</span> <span class="n">torch</span>

<span class="n">scores</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_N</span><span class="p">)</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">score</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
    <span class="n">document</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">(</span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">document</span><span class="p">)</span>
    <span class="n">documents</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">document</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">(0.7035) A man is eating food.
(0.5272) A man is eating a piece of bread.
(0.1889) A man is riding a horse.
(0.1047) A man is riding a white horse on an enclosed ground.
(0.0980) A cheetah is running behind its prey.
(0.0819) A monkey is playing drums.
(0.0336) A woman is playing violin.
(-0.0594) Two men pushed carts through the woods.
(-0.0898) The girl is carrying a baby.</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">CrossEncoder</span>

<span class="n">cross_encoder</span> <span class="o">=</span> <span class="nc">CrossEncoder</span><span class="p">(</span><span class="sh">"</span><span class="s">cross-encoder/ms-marco-MiniLM-L-6-v2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">top_K</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">top_N</span><span class="p">)</span>
<span class="n">ranking</span> <span class="o">=</span> <span class="n">cross_encoder</span><span class="p">.</span><span class="nf">rank</span><span class="p">(</span>
    <span class="n">query</span><span class="p">,</span>
    <span class="n">documents</span><span class="p">,</span>
    <span class="n">top_k</span><span class="o">=</span><span class="n">top_K</span><span class="p">,</span>
    <span class="n">return_documents</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ranking</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">(</span><span class="si">{</span><span class="n">r</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">(1.9005) A man is eating food.
(1.4804) A man is eating a piece of bread.
(-7.0890) A man is riding a horse.
(-8.9042) A man is riding a white horse on an enclosed ground.
(-10.7628) A monkey is playing drums.</span></code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="retrieval-augmented-generation-rag">8.2. Retrieval-Augmented Generation (RAG)</h3>
<div class="paragraph">
<p>RAG systems incorporate search capabilities in addition to generation capabilities to enhance factuality and reduce hallucinations.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/basic-rag-pipeline.png" alt="A basic RAG pipeline" width="35%" height="35%">
</div>
<div class="title">Figure 95. A basic RAG pipeline is made up of a search step followed by a grounded generation step where the LLM is prompted with the question and the information retrieved from the search step.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/generative-search-rag-pipeline.png" alt="Generative search" width="35%" height="35%">
</div>
<div class="title">Figure 96. Generative search formulates answers and summaries at the end of a search pipeline while citing its sources (returned by the previous steps in the search system).</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/rag-prompt-flow.png" alt="RAG prompt flow" width="35%" height="35%">
</div>
<div class="title">Figure 97. Find the most relevant information to an input prompt by comparing the similarities between embeddings. The most relevant information is added to the prompt before giving it to the LLM.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">mistral:7b-instruct</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="sh">'</span><span class="s">APK-KEY</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="sh">"</span><span class="s">http://localhost:11434/v1</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Ollama
</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">HTMLHeaderTextSplitter</span>

<span class="n">headers_to_split_on</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">h1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Header 1</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">h2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Header 2</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">h3</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Header 3</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">h4</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Header 4</span><span class="sh">"</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">html_splitter</span> <span class="o">=</span> <span class="nc">HTMLHeaderTextSplitter</span><span class="p">(</span><span class="n">headers_to_split_on</span><span class="p">)</span>
<span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://plato.stanford.edu/entries/goedel/</span><span class="sh">"</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">html_splitter</span><span class="p">.</span><span class="nf">split_text_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">from</span> <span class="n">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">FAISS</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Relevant information:
{context}
Provide a concise answer the following question using the relevant information
provided above:
{question}
</span><span class="sh">"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.chains.retrieval_qa.base</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>

<span class="n">rag</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="p">.</span><span class="nf">from_chain_type</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="sh">"</span><span class="s">stuff</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">db</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(),</span>
    <span class="n">chain_type_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">rag</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">Who is Kurt Gödel?</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'query': 'Who is Kurt Gödel?',
 'result': " Kurt Gödel was an Austrian mathematician and logician. He is best known for his work on the incompleteness theorems, which were established in 1930 and prove that any sufficiently rich formal axiomatic system contains either statements that cannot be proven or disproven within the system itself. Some of Gödel's other notable contributions include his proof of the consistency of the continuum hypothesis using large cardinals, and his work on undecidable propositions in number theory, which led to the concept of Gödel numbers for representing mathematical statements in a formal system. Throughout his life, Gödel also explored philosophical questions related to logic, mathematics, and metaphysics, including questions about realism, the foundations of mathematics, set theory, and the nature of time and truth."}</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="multimodal-large-language-models">9. Multimodal Large Language Models</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A multimodal model is a type of artificial intelligence model capable of processing and reasoning across different modalities, where a modality refers to a distinct type of data such as text, images, audio, video, or sensor data.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/multimodal-language-model.png" alt="Multimodal Large Language Models" width="35%" height="35%">
</div>
<div class="title">Figure 98. Models that are able to deal with different types (or modalities) of data, such as images, audio, video, or sensors, are said to be multimodal. It’s possible for a model to accept a modality as input yet not be able to generate in that modality.</div>
</div>
<div class="sect2">
<h3 id="vision-transformer-vit">9.1. Vision Transformer (ViT)</h3>
<div class="paragraph">
<p>Vision Transformer (ViT) is a method that adapts the Transformer architecture to the field of computer vision, particularly for image recognition tasks, by treating an image as a sequence of flattened image patches which are then linearly embedded and processed by the Transformer encoder in a manner similar to textual tokens, allowing it to capture global relationships in the image more directly than the local receptive fields of convolutional neural networks (CNNs).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/vit-image-patch-embedding-encoder.png" alt="The main algorithm behind ViT" width="30%" height="30%">
</div>
<div class="title">Figure 99. The main algorithm behind ViT. After patching the images and linearly projecting them, the patch embeddings are passed to the encoder and treated as if they were textual tokens.</div>
</div>
</div>
<div class="sect2">
<h3 id="multimodal-embedding-models">9.2. Multimodal Embedding Models</h3>
<div class="paragraph">
<p>A multimodal embedding model is a type of model that can create numerical representations (embeddings) for multiple modalities, such as text and imagery, within the same vector space, allowing for direct comparison of representations from different modalities based on their semantic content.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/multimodal-vector-space.png" alt="Multimodal Embedding Models Space" width="30%" height="30%">
</div>
<div class="title">Figure 100. Despite having coming from different modalities, embeddings with similar meaning will be close to each other in vector space.</div>
</div>
<div class="paragraph">
<p>Contrastive Language-Image Pre-training (CLIP) is an embedding model to compute embeddings of both images and texts.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/first-step-of-traning-clip.png" alt="First step of traning CLIP" width="30%" height="30%">
</div>
<div class="title">Figure 101. In the first step of training CLIP, both images and text are embedded using an image and text encoder, respectively.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/second-step-of-traning-clip.png" alt="Second step of training CLIP" width="30%" height="30%">
</div>
<div class="title">Figure 102. In the second step of training CLIP, the similarity between the sentence and image embedding is calculated using cosine similarity.</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/third-step-of-traning-clip.png" alt="Third step of training CLIP" width="30%" height="30%">
</div>
<div class="title">Figure 103. In the third step of training CLIP, the text and image encoders are updated to match what the intended similarity should be (called contrastive learning). This updates the embeddings such that they are closer in vector space if the inputs are similar.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># load an AI-generated image of a puppy playing in the snow from a URL
</span><span class="n">puppy_path</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">https://raw.githubusercontent.com/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">HandsOnLLM/Hands-On-Large-Language-Models/main/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">chapter09/images/puppy.png</span><span class="sh">"</span>
<span class="p">)</span>
<span class="c1"># open the image from the URL and convert it to RGB format
</span><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">puppy_path</span><span class="p">)).</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># define a text caption for the image
</span><span class="n">caption</span> <span class="o">=</span> <span class="sh">"</span><span class="s">a puppy playing in the snow</span><span class="sh">"</span></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/puppy-snow.png" alt="puppy snow" width="20%" height="20%">
</div>
<div class="title">Figure 104. An AI-generated image of a puppy playing in the snow.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">CLIPTokenizer</span><span class="p">,</span> <span class="n">CLIPProcessor</span><span class="p">,</span> <span class="n">CLIPModel</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">openai/clip-vit-base-patch32</span><span class="sh">"</span>

<span class="c1"># load the tokenizer associated with the CLIP model to preprocess text
</span><span class="n">clip_tokenizer</span> <span class="o">=</span> <span class="n">CLIPTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># load the processor associated with the CLIP model to preprocess images and text
</span><span class="n">clip_processor</span> <span class="o">=</span> <span class="n">CLIPProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># load the main CLIP model for generating text and image embeddings
</span><span class="n">model</span> <span class="o">=</span> <span class="n">CLIPModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># tokenize the input caption into numerical representations
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">clip_tokenizer</span><span class="p">(</span><span class="n">caption</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">inputs</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'input_ids': tensor([[49406,   320,  6829,  1629,   530,   518,  2583, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># convert the token IDs back to the corresponding text tokens
</span><span class="n">clip_tokenizer</span><span class="p">.</span><span class="nf">convert_ids_to_tokens</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">['&lt;|startoftext|&gt;</span><span class="s1">',
</span><span class="gp"> 'a&lt;/w&gt;</span><span class="s1">'</span>,
<span class="gp"> 'puppy&lt;/w&gt;</span><span class="s1">',
</span><span class="gp"> 'playing&lt;/w&gt;</span><span class="s1">'</span>,
<span class="gp"> 'in&lt;/w&gt;</span><span class="s1">',
</span><span class="gp"> 'the&lt;/w&gt;</span><span class="s1">'</span>,
<span class="gp"> 'snow&lt;/w&gt;</span><span class="s1">',
</span><span class="gp"> '&lt;|endoftext|&gt;</span><span class="s1">'</span><span class="o">]</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># create a text embedding vector representing the semantic meaning of the caption
</span><span class="n">text_embedding</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_text_features</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">text_embedding</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, embedding_dimension)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">torch.Size([1, 512])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># preprocess the image to match the input requirements of the CLIP model
</span><span class="n">image_inputs</span> <span class="o">=</span> <span class="nf">clip_processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">image_pixel_values</span> <span class="o">=</span> <span class="n">image_inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">pixel_values</span><span class="sh">"</span><span class="p">]</span>
<span class="n">image_pixel_values</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, num_channels, height, width)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">torch.Size([1, 3, 224, 224])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># prepare the preprocessed image tensor for visualization
</span><span class="n">img</span> <span class="o">=</span> <span class="n">image_pixel_values</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># remove the batch dimension
</span><span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># transpose dimensions for correct visualization order (C, H, W -&gt; H, W, C)
</span><span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ijk-&gt;jik</span><span class="sh">"</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>
<span class="c1"># visualize the preprocessed image
</span><span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># turn off axis labels and ticks
</span><span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/puppy-snow-processed-by-clip.png" alt="puppy snow processed by clip" width="20%" height="20%">
</div>
<div class="title">Figure 105. The preprocessed input image by CLIP.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># create the image embedding vector representing the visual content of the image
</span><span class="n">image_embedding</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_image_features</span><span class="p">(</span><span class="n">image_pixel_values</span><span class="p">)</span>
<span class="n">image_embedding</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, embedding_dimension): same as that of the text embedding</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">torch.Size([1, 512])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># normalize the text and image embeddings
</span><span class="n">text_embedding</span> <span class="o">/=</span> <span class="n">text_embedding</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">image_embedding</span> <span class="o">/=</span> <span class="n">image_embedding</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># calculate the cosine similarity score
</span><span class="n">text_embedding</span> <span class="o">=</span> <span class="n">text_embedding</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># move the text embedding to CPU and convert to NumPy array
</span><span class="n">image_embedding</span> <span class="o">=</span> <span class="n">image_embedding</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># move the image embedding to CPU and convert to NumPy array
</span><span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">text_embedding</span><span class="p">,</span> <span class="n">image_embedding</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">score</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">array([[0.33146894]], dtype=float32)</span></code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p><code>sentence-transformers</code> implements a few CLIP-based models that make it much easier to create embeddings. It only takes a few lines of code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">puppy_path</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">https://raw.githubusercontent.com/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">HandsOnLLM/Hands-On-Large-Language-Models/main/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">chapter09/images/puppy.png</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">puppy_path</span><span class="p">)).</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>
<span class="n">caption</span> <span class="o">=</span> <span class="sh">"</span><span class="s">a puppy playing in the snow</span><span class="sh">"</span>

<span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">sentence-transformers/clip-ViT-B-32</span><span class="sh">"</span><span class="p">)</span>

<span class="n">image_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">image</span><span class="p">])</span>
<span class="n">text_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">caption</span><span class="p">])</span>

<span class="n">sim_matrix</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="nf">cos_sim</span><span class="p">(</span><span class="n">image_embeddings</span><span class="p">,</span> <span class="n">text_embeddings</span><span class="p">)</span>
<span class="n">sim_matrix</span>  <span class="c1"># tensor([[0.3315]])</span></code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="multimodal-text-generation-models">9.3. Multimodal Text Generation Models</h3>
<div class="paragraph">
<p>BLIP-2 (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 2) is a multimodal text generation model designed to introduce vision capabilities to existing, pre-trained language models (LLMs) without requiring end-to-end training from scratch.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/q-former-vit-llm-bridge.png" alt="The Querying Transformer is the bridge between vision (ViT) and text (LLM)" width="35%" height="35%">
</div>
<div class="title">Figure 106. The Querying Transformer is the bridge between vision (ViT) and text (LLM) that is the only trainable component of the pipeline.</div>
</div>
<div class="sect3">
<h4 id="blip-2-bridging-the-modality-gap">9.3.1. BLIP-2: Bridging the Modality Gap</h4>
<div class="paragraph">
<p>BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), connecting a frozen (non-trainable) pre-trained image encoder like a Vision Transformer and a frozen pre-trained LLM.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Q-Former is trained in two stages, one for each modality to make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM to give information about the image in a similar manner to the context providing an LLM when prompting.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/the-full-blip-2.png" alt="The full BLIP-2 procedure." width="35%" height="35%">
</div>
<div class="title">Figure 107. In step 1, representation learning is applied to learn representations for vision and language simultaneously. In step 2, these representations are converted to soft visual prompts to feed the LLM.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>In step 1, image-document pairs are used to train the Q-Former to represent both images and text, which are generally captions of images similar tranning CLIP.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/q-former-1-step-traning.png" alt="Q-Former three contrastive-like tasks to learn visual-text representations." width="35%" height="35%">
</div>
<div class="title">Figure 108. In step 1, the output of the frozen ViT is used together with its caption and trained on three contrastive-like tasks to learn visual-text representations.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The images are fed to the frozen ViT to extract vision embeddings, which are used as the input of Q-Former’s ViT, and the captions are used as the input of Q-Former’s Text Transformer.</p>
</li>
<li>
<p>The Q-Former is then trained on three tasks: image-text contrastive learning that attempts to align pairs of image and text embeddings such that they maximize their mutual information, image-text matching that predicts whether an image and text pair is positive (matched) or negative (unmatched), and image-grounded text generation that generates text based on information extracted from the input image.</p>
</li>
</ul>
</div>
</li>
<li>
<p>In step 2, the learnable embeddings containing aligned visual and textual information in the same dimensional space from the Q-Former are projected to match the LLM&#8217;s input format and then serve as soft visual prompts, conditioning the LLM on the visual representations.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/q-former-2-step-traning.png" alt="In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer" width="35%" height="35%">
</div>
<div class="title">Figure 109. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer. The projected embeddings serve as a soft visual prompt.</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="preprocessing-multimodal-inputs">9.3.2. Preprocessing Multimodal Inputs</h4>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>

<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># load image of a supercar
</span><span class="n">car_path</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">https://raw.githubusercontent.com/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">HandsOnLLM/Hands-On-Large-Language-Models/main/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">chapter09/images/car.png</span><span class="sh">"</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">car_path</span><span class="p">))</span> <span class="k">as</span> <span class="n">i</span><span class="p">:</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/orange-supercar.png" alt="An orange supercar driving on the road at sunset." width="10%" height="10%">
</div>
<div class="title">Figure 110. An orange supercar driving on the road at sunset.</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">Blip2ForConditionalGeneration</span>

<span class="c1"># load processor and main model
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Salesforce/blip2-opt-2.7b</span><span class="sh">"</span>
<span class="n">blip_processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Blip2ForConditionalGeneration</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
<span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">model</span><span class="p">.</span><span class="n">vision_model</span>  <span class="c1"># vision transformer in the loaded BLIP-2 model.</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Blip2VisionModel(
  (embeddings): Blip2VisionEmbeddings(
    (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))
  )
  (encoder): Blip2Encoder(
    (layers): ModuleList(
      (0-38): 39 x Blip2EncoderLayer(
        (self_attn): Blip2Attention(
          (dropout): Dropout(p=0.0, inplace=False)
          (qkv): Linear(in_features=1408, out_features=4224, bias=True)
          (projection): Linear(in_features=1408, out_features=1408, bias=True)
        )
        (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Blip2MLP(
          (activation_fn): GELUActivation()
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
        )
        (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">model</span><span class="p">.</span><span class="n">language_model</span>  <span class="c1"># text generative model in the loaded BLIP-2 model.</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50304, 2560, padding_idx=1)
      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)
      (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0-31): 32 x OPTDecoderLayer(
          (self_attn): OPTSdpaAttention(
            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=2560, out_features=50304, bias=False)
)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># preprocess the image
</span><span class="n">image_inputs</span> <span class="o">=</span> <span class="nf">blip_processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">image_pixel_values</span> <span class="o">=</span> <span class="n">image_inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">pixel_values</span><span class="sh">"</span><span class="p">]</span>
<span class="n">image_pixel_values</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># a 224 × 224-sized image</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">torch.Size([1, 3, 224, 224])</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># tokenizer used to tokenize the input text
</span><span class="n">blip_processor</span><span class="p">.</span><span class="n">tokenizer</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">GPT2TokenizerFast(name_or_path='Salesforce/blip2-opt-2.7b', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;/s&gt;</span><span class="s1">', '</span>eos_token<span class="s1">': '</span>&lt;/s&gt;<span class="s1">', '</span>unk_token<span class="s1">': '</span>&lt;/s&gt;<span class="s1">', '</span>pad_token<span class="s1">': '</span>&lt;pad&gt;<span class="s1">'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
</span><span class="gp">	1: AddedToken("&lt;pad&gt;</span><span class="s1">", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
</span><span class="gp">	2: AddedToken("&lt;/s&gt;</span><span class="s1">", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
</span><span class="gp">	50265: AddedToken("&lt;image&gt;</span><span class="s1">", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
</span><span class="go">}
)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># preprocess the text
</span><span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Her vocalization was remarkably melodic</span><span class="sh">"</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="nf">blip_processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">token_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># convert input ids back to tokens
</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">blip_processor</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_ids_to_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
<span class="n">tokens</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">['&lt;/s&gt;</span><span class="s1">', '</span>Her<span class="s1">', '</span>Ġvocal<span class="s1">', '</span>ization<span class="s1">', '</span>Ġwas<span class="s1">', '</span>Ġremarkably<span class="s1">', '</span>Ġmel<span class="s1">', '</span>odic<span class="s1">']</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># replace the space token with an underscore
</span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">Ġ</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="n">tokens</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">['&lt;/s&gt;</span><span class="s1">', '</span>Her<span class="s1">', '</span>_vocal<span class="s1">', '</span>ization<span class="s1">', '</span>_was<span class="s1">', '</span>_remarkably<span class="s1">', '</span>_mel<span class="s1">', '</span>odic<span class="s1">']</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="use-case-1-image-captioning">9.3.3. Use Case 1: Image Captioning</h4>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">Blip2ForConditionalGeneration</span>

<span class="c1"># load processor and main model
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Salesforce/blip2-opt-2.7b</span><span class="sh">"</span>
<span class="n">blip_processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Blip2ForConditionalGeneration</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># load an AI-generated image of a supercar
</span><span class="n">car_path</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">https://raw.githubusercontent.com/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">HandsOnLLM/Hands-On-Large-Language-Models/main/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">chapter09/images/car.png</span><span class="sh">"</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">car_path</span><span class="p">))</span> <span class="k">as</span> <span class="n">i</span><span class="p">:</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># convert an image into inputs and preprocess it
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">blip_processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
<span class="c1"># {'pixel_values': tensor([[[[-1.0039, -1.0039, -0.9893,  ..., -0.0842, -0.0988, -0.0842],
</span>
<span class="c1"># generate image ids to be passed to the decoder (LLM)
</span><span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># generate text from the image ids
</span><span class="n">generated_text</span> <span class="o">=</span> <span class="n">blip_processor</span><span class="p">.</span><span class="nf">batch_decode</span><span class="p">(</span>
    <span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">generated_text</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">strip</span><span class="p">()</span>
<span class="n">generated_text</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">an orange supercar driving on the road at sunset</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="use-case-2-multimodal-chat-based-prompting">9.3.4. Use Case 2: Multimodal Chat-Based Prompting</h4>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">Blip2ForConditionalGeneration</span>

<span class="c1"># load processor and main model
</span><span class="n">dev</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Salesforce/blip2-opt-2.7b</span><span class="sh">"</span>
<span class="n">blip_processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Blip2ForConditionalGeneration</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">dev</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># load an AI-generated image of a supercar
</span><span class="n">car_path</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">https://raw.githubusercontent.com/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">HandsOnLLM/Hands-On-Large-Language-Models/main/</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">chapter09/images/car.png</span><span class="sh">"</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">car_path</span><span class="p">))</span> <span class="k">as</span> <span class="n">i</span><span class="p">:</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># visual question answering
</span><span class="n">prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Question: Write down what you see in this picture. Answer:</span><span class="sh">"</span>

<span class="c1"># process both the image and the prompt
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">blip_processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

<span class="c1"># generate text
</span><span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">blip_processor</span><span class="p">.</span><span class="nf">batch_decode</span><span class="p">(</span>
    <span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">generated_text</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">strip</span><span class="p">()</span>
<span class="n">generated_text</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># chat-like prompting: a follow-up question
</span><span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">Question: Write down what you see in this picture. Answer: A sports </span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">car driving on the road at sunset. Question: What would it cost me to </span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">drive that car? Answer:</span><span class="sh">"</span>
<span class="p">)</span>
<span class="c1"># Generate output
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">blip_processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">blip_processor</span><span class="p">.</span><span class="nf">batch_decode</span><span class="p">(</span>
    <span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">generated_text</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">strip</span><span class="p">()</span>
<span class="n">generated_text</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer: $</span>1,000,000</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="creating-and-fine-tuning-text-embedding-models">10. Creating and Fine-Tuning Text Embedding Models</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Embedding models are Large Language Models (LLMs) used to convert unstructured textual data (like documents, sentences, or phrases) into dense numerical representations called embeddings.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The primary goal of these models is to accurately capture the semantic meaning of the text, such that texts with similar meanings have embeddings that are close to each other in a high-dimensional vector space, while texts with different meanings have dissimilar embeddings.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/embedding-semantic-similarity-n-d-space.png" alt="Embedding model on semantic similarity" width="35%" height="35%">
</div>
<div class="title">Figure 111. The idea of semantic similarity is that we expect textual data with similar meanings to be closer to each other in n-dimensional space (two dimensions are illustra‐ ted here).</div>
</div>
</li>
<li>
<p>Embedding models can also be trained or fine-tuned for other purposes, such as capturing sentiment similarity, by guiding the model with appropriate training examples.</p>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/embedding-sentiment-similarity-n-d-space.png" alt="Embedding model on sentiment similarity" width="35%" height="35%">
</div>
<div class="title">Figure 112. In addition to semantic similarity, an embedding model can be trained to focus on sentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to positive reviews (green).</div>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="contrastive-learning">10.1. Contrastive Learning</h3>
<div class="paragraph">
<p>Contrastive learning is a self-supervised or supervised machine learning technique that aims to learn representations of data by contrasting similar ("positive") and dissimilar ("negative") examples (Why P and not Q?) to create an embedding space where similar data points are located close to each other, while dissimilar data points are far apart, which is effective in various domains, including computer vision and natural language processing, for tasks like representation learning, similarity search, and few-shot learning.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">Reporter: “Why did you rob a bank?”

Robber: “Because that is where the money is.”

Reporter (alternatively): “Why did you rob a bank (P) instead of obeying the law (Q)?”</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="sentence-transformers-sbert">10.2. Sentence Transformers (SBERT)</h3>
<div class="paragraph">
<p>A cross-encoder is a Transformer-based model that processes two sentences together to directly predict their similarity score via a classification head, but it&#8217;s computationally expensive for large-scale pairwise comparisons and doesn&#8217;t typically generate individual sentence embeddings.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/cross-encoder-architecture.png" alt="The architecture of a cross-encoder" width="30%" height="30%">
</div>
<div class="title">Figure 113. The architecture of a cross-encoder. Both sentences are concatenated, separated with a &lt;SEP&gt; token, and fed to the model simultaneously.</div>
</div>
<div class="paragraph">
<p>The authors of <a href="https://github.com/UKPLab/sentence-transformers">sentence-transformers</a> addressed the limitations of cross-encoders (slow speed, no embeddings) by developing a fast alternative that generates semantically comparable, fixed-size embeddings by using a Siamese architecture, also known as a bi-encoder or SBERT, with two identical BERT models (sharing weights) that process sentences independently and then apply mean pooling to the final layer.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/sbert-bi-encoder.png" alt="The architecture of the original sentence-transformers model." width="20%" height="20%">
</div>
<div class="title">Figure 114. The architecture of the original sentence-transformers model, which leverages a Siamese network, also called a bi-encoder.</div>
</div>
</div>
<div class="sect2">
<h3 id="creating-an-embedding-model">10.3. Creating an Embedding Model</h3>
<div class="paragraph">
<p>Natural Language Inference (NLI) datasets, used in pretraining embedding models, classify premise-hypothesis pairs as entailment (similar meaning), contradiction (opposite meaning), or neutral.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ai/llms/nli-datasets-contrastive-examples.png" alt="NLI datasets for contrastive learning" width="35%" height="35%">
</div>
<div class="title">Figure 115. We can leverage the structure of NLI datasets to generate negative examples (contradiction) and positive examples (entailments) for contrastive learning.</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Entailments serve as positive examples for contrastive learning (similar pairs), while contradictions serve as negative examples (dissimilar pairs).</p>
</li>
<li>
<p>The Multi-Genre Natural Language Inference (MNLI) corpus from the <a href="https://gluebenchmark.com/">General Language Understanding Evaluation (GLUE)</a> benchmark contains annotated sentence pairs with these relationships, and is a common source for generating such contrastive training data.</p>
</li>
<li>
<p>A subset of MNLI is often used for faster experimentation, though larger, quality datasets are generally preferred for stable training.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># Load MNLI dataset from GLUE
# 0 = entailment, 1 = neutral, 2 = contradiction
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">glue</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># load a dataset from the GLUE benchmark
</span>    <span class="sh">"</span><span class="s">mnli</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># load the MNLI dataset
</span>    <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># load the training split
</span><span class="p">).</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">50_000</span><span class="p">))</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="nf">remove_columns</span><span class="p">(</span><span class="sh">"</span><span class="s">idx</span><span class="sh">"</span><span class="p">)</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="n">train_dataset</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">{'premise': 'One of our number will carry out your instructions minutely.',
 'hypothesis': 'A member of my team will execute your orders with immense precision.',
 'label': 0}</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="py"><span class="c1"># train model
</span><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># use a base model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">google-bert/bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">losses</span>

<span class="c1"># define the softmax loss function.
</span><span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="p">.</span><span class="nc">SoftmaxLoss</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">sentence_embedding_dimension</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="nf">get_sentence_embedding_dimension</span><span class="p">(),</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">sentence_transformers.evaluation</span> <span class="kn">import</span> <span class="n">EmbeddingSimilarityEvaluator</span>

<span class="c1"># create an embedding similarity evaluator for STSB
</span><span class="n">val_sts</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">glue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">stsb</span><span class="sh">"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">)</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="nc">EmbeddingSimilarityEvaluator</span><span class="p">(</span>
    <span class="n">sentences1</span><span class="o">=</span><span class="n">val_sts</span><span class="p">[</span><span class="sh">"</span><span class="s">sentence1</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">sentences2</span><span class="o">=</span><span class="n">val_sts</span><span class="p">[</span><span class="sh">"</span><span class="s">sentence2</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">scores</span><span class="o">=</span><span class="p">[</span><span class="n">score</span> <span class="o">/</span> <span class="mi">5</span> <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">val_sts</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">]],</span>
    <span class="n">main_similarity</span><span class="o">=</span><span class="sh">"</span><span class="s">cosine</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">sentence_transformers.training_args</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SentenceTransformerTrainingArguments</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="nc">SentenceTransformerTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">base_embedding_model</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">sentence_transformers.trainer</span> <span class="kn">import</span> <span class="n">SentenceTransformerTrainer</span>

<span class="c1"># train embedding model
</span><span class="n">trainer</span> <span class="o">=</span> <span class="nc">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">train_loss</span><span class="p">,</span>
    <span class="n">evaluator</span><span class="o">=</span><span class="n">evaluator</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="c1"># evaluate the trained model
</span><span class="nf">evaluator</span><span class="p">(</span><span class="n">model</span><span class="p">)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="references">References</h2>
<div class="sectionbody">
<div class="ulist bibliography">
<ul class="bibliography">
<li>
<p><a id="hands-on-llm"></a>[1] Jay Alammar, Maarten Grootendorst <em>Hands-On Large Language Models: Language Understanding and Generation</em>. O&#8217;Reilly Media; 1st edition (October 15, 2024)</p>
</li>
</ul>
</div>
</div>
</div>
<style>
  .utterances {
      max-width: 100%;
  }
</style>
<script src="https://utteranc.es/client.js"
        repo="looogos/utterances"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

</div>
</article>
    </main>
    <footer class="c-footer">
  <div class="c-footer-license">
    <span>Article licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span>
  </div>
  
  <details class="c-footer-extralinks" open>
    <summary class="c-footer-extralinks-summary">Extral Links</summary>
    <div class="c-footer-extralinks-content">
      
      <a href="https://jekyllrb.com/">Jekyll</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://shopify.github.io/liquid/">Liquid</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://docs.asciidoctor.org/">Asciidoctor</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://github.com/qqbuby/">GitHub</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="/feed.xml">RSS</a>
      
      
    </div>
  </details>
  
</footer>

    <script src="/assets/js/nav.js" defer></script>
    <script src="/assets/js/heading-anchors.js" defer></script>
    <!-- https://cdn.jsdelivr.net/gh/lurongkai/anti-baidu/js/anti-baidu-latest.min.js -->    
    <script type="text/javascript" src="/js/anti-baidu.min.js" charset="UTF-8"></script>
  </body>
</html>
