<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Linux CGroups and Containers | CODE FARM</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Linux CGroups and Containers" />
<meta property="og:locale" content="en" />
<meta name="description" content="1. What are Control Groups 1.1. What are Resource Controllers 1.2. How Control Groups are Organized 1.3. Systemd 1.4. libcgroup 1.5. nsenter 1.6. free 2. What are Containers? 2.1. Containers vs. VMs 2.2. Open Container Initiative 2.3. What is Docker? 2.4. What is Kubernetes? References 1. What are Control Groups Linux Control Groups (cgroups) enable limits on the use of system hardware, ensuring that an individual process running inside a cgroup only utilizes as much as has been allowed in the cgroups configuration. [1] Control Groups restrict the volume of usage on a resource that has been enabled by a namespace. For example, the network namespace allows a process to access a particular network card, the cgroup ensures that the process does not exceed 50% usage of that card, ensuring bandwidth is available for other processes. Control Group Namespaces provide a virtualized view of individual cgroups through the /proc/self/ns/cgroup interface. Namespaces are a kernel feature that allow a virtual view of isolated system resources. By isolating a process from system resources, you can specify and control what a process is able to interact with. Namespaces are an essential part of Control Groups, and a fundamental aspect of containers in Linux. [2][19] Mount The mount namespace isolates file system mount points, enabling each process to have a distinct filesystem space within wich to operate. UTS UTS (UNIX Time-Sharing) namespaces allow a single system to appear to have different host and domain names to different processes. IPC System V IPC, POSIX message queues PID Process IDs Network Network namespaces virtualize the network stack. On creation, a network namespace contains only a loopback interface. Each network interface (physical or virtual) is present in exactly 1 namespace and can be moved between namespaces. Each namespace will have a private set of IP addresses, its own routing table, socket listing, connection tracking table, firewall, and other network-related resources. Destroying a network namespace destroys any virtual interfaces within it and moves any physical interfaces within it back to the initial network namespace. User User and group IDs Control Groups Isolates cgroups 1.1. What are Resource Controllers A resource controller, also called a cgroup subsystem, represents a single resource, such as CPU time or memory. The Linux kernel provides a range of resource controllers, that are mounted automatically by systemd. [3] Find the list of currently mounted resource controllers in /proc/cgroups. blkio — sets limits on input/output access to and from block devices; cpu — uses the CPU scheduler to provide cgroup tasks access to the CPU. It is mounted together with the cpuacct controller on the same mount; cpuacct — creates automatic reports on CPU resources used by tasks in a cgroup. It is mounted together with the cpu controller on the same mount; cpuset — assigns individual CPUs (on a multicore system) and memory nodes to tasks in a cgroup; devices — allows or denies access to devices for tasks in a cgroup; freezer — suspends or resumes tasks in a cgroup; memory — sets limits on memory use by tasks in a cgroup and generates automatic reports on memory resources used by those tasks; net_cls — tags network packets with a class identifier (classid) that allows the Linux traffic controller (the tc command) to identify packets originating from a particular cgroup task. A subsystem of net_cls, the net_filter (iptables) can also use this tag to perform actions on such packets. The net_filter tags network sockets with a firewall identifier (fwid) that allows the Linux firewall (the iptables command) to identify packets (skb&#8594;sk) originating from a particular cgroup task; perf_event — enables monitoring cgroups with the perf tool; hugetlb — allows to use virtual memory pages of large sizes and to enforce resource limits on these pages. 1.2. How Control Groups are Organized Cgroups are organized hierarchically, like processes, and child cgroups inherit some of the attributes of their parents. However, there are differences between the two models. [5] 1.2.1. The Linux Process Model All processes on a Linux system are child processes of a common parent: the init process, which is executed by the kernel at boot time and starts other processes (which may in turn start child processes of their own). Because all processes descend from a single parent, the Linux process model is a single hierarchy, or tree. Additionally, every Linux process except init inherits the environment (such as the PATH variable) and certain other attributes (such as open file descriptors) of its parent process. 1.2.2. The Cgroup Model Cgroups are similar to processes in that: they are hierarchical, and child cgroups inherit certain attributes from their parent cgroup. The fundamental difference is that many different hierarchies of cgroups can exist simultaneously on a system. If the Linux process model is a single tree of processes, then the cgroup model is one or more separate, unconnected trees of tasks (i.e. processes). Multiple separate hierarchies of cgroups are necessary because each hierarchy is attached to one or more subsystems. Remember that system processes are called tasks in cgroup terminology. Here are a few simple rules governing the relationships between subsystems, hierarchies of cgroups, and tasks, along with explanations of the consequences of those rules. Rule 1 A single hierarchy can have one or more subsystems attached to it. As a consequence, the cpu and memory subsystems (or any number of subsystems) can be attached to a single hierarchy, as long as each one is not attached to any other hierarchy which has any other subsystems attached to it already (see Rule 2). Rule 2 Any single subsystem (such as cpu) cannot be attached to more than one hierarchy if one of those hierarchies has a different subsystem attached to it already. As a consequence, the cpu subsystem can never be attached to two different hierarchies if one of those hierarchies already has the memory subsystem attached to it. However, a single subsystem can be attached to two hierarchies if both of those hierarchies have only that subsystem attached. Rule 3 Each time a new hierarchy is created on the systems, all tasks on the system are initially members of the default cgroup of that hierarchy, which is known as the root cgroup. For any single hierarchy you create, each task on the system can be a member of exactly one cgroup in that hierarchy. A single task may be in multiple cgroups, as long as each of those cgroups is in a different hierarchy. As soon as a task becomes a member of a second cgroup in the same hierarchy, it is removed from the first cgroup in that hierarchy. At no time is a task ever in two different cgroups in the same hierarchy. As a consequence, if the cpu and memory subsystems are attached to a hierarchy named cpu_mem_cg, and the net_cls subsystem is attached to a hierarchy named net, then a running httpd process could be a member of any one cgroup in cpu_mem_cg, and any one cgroup in net. The cgroup in cpu_mem_cg that the httpd process is a member of might restrict its CPU time to half of that allotted to other processes, and limit its memory usage to a maximum of 1024 MB. Additionally, the cgroup in net that the httpd process is a member of might limit its transmission rate to 30 MB/s (megabytes per second). When the first hierarchy is created, every task on the system is a member of at least one cgroup: the root cgroup. When using cgroups, therefore, every system task is always in at least one cgroup. Rule 4 Any process (task) on the system which forks itself creates a child task. A child task automatically inherits the cgroup membership of its parent but can be moved to different cgroups as needed. Once forked, the parent and child processes are completely independent. As a consequence, consider the httpd task that is a member of the cgroup named half_cpu_1gb_max in the cpu_and_mem hierarchy, and a member of the cgroup trans_rate_30 in the net hierarchy. When that httpd process forks itself, its child process automatically becomes a member of the half_cpu_1gb_max cgroup, and the trans_rate_30 cgroup. It inherits the exact same cgroups its parent task belongs to. From that point forward, the parent and child tasks are completely independent of each other: changing the cgroups that one task belongs to does not affect the other. Neither will changing cgroups of a parent task affect any of its grandchildren in any way. To summarize: any child task always initially inherits memberships to the exact same cgroups as their parent task, but those memberships can be changed or removed later. 1.3. Systemd Systemd is a system and service manager for Linux operating systems. It is designed to be backwards compatible with SysV init scripts, and provides a number of features such as parallel startup of system services at boot time, on-demand activation of daemons, or dependency-based service control logic. [4] Systemd introduces the concept of systemd units, represented by unit configuration files. Table 1. Available systemd Unit Types Unit Type File Extension Description Service unit .service A system service. Target unit .target A group of systemd units. Automount unit .automount A file system automount point. Device unit .device A device file recognized by the kernel. Mount unit .mount A file system mount point. Path unit .path A file or directory in a file system. Scope unit .scope An externally created process. Slice unit .slice A group of hierarchically organized units that manage system processes. Snapshot unit .snapshot A saved state of the systemd manager. Socket unit .socket An inter-process communication socket. Swap unit .swap A swap device or a swap file. Timer unit .timer A systemd timer. Table 2. Systemd Unit Files Locations Directory Description /usr/lib/systemd/system/ Systemd unit files distributed with installed RPM packages. /run/systemd/system/ Systemd unit files created at run time. This directory takes precedence over the directory with installed service unit files. /etc/systemd/system/ Systemd unit files created by systemctl enable as well as unit files added for extending a service. This directory takes precedence over the directory with runtime unit files. By default, systemd automatically creates a hierarchy of slice, scope and service units to provide a unified structure for the cgroup tree. Also, systemd automatically mounts hierarchies for important kernel resource controllers in the /sys/fs/cgroup/ directory. [3] All processes running on the system are child processes of the systemd init process. Systemd provides three unit types that are used for the purpose of resource control: Service — A process or a group of processes, which systemd started based on a unit configuration file. Services encapsulate the specified processes so that they can be started and stopped as one set. Scope — A group of externally created processes. Scopes encapsulate processes that are started and stopped by arbitrary processes through the fork() function and then registered by systemd at runtime. For instance, user sessions, containers, and virtual machines are treated as scopes. Slice — A group of hierarchically organized units. Slices do not contain processes, they organize a hierarchy in which scopes and services are placed. The actual processes are contained in scopes or in services. In this hierarchical tree, every name of a slice unit corresponds to the path to a location in the hierarchy. The dash (&#8220;-&#8221;) character acts as a separator of the path components. Services, scopes, and slices are created manually by the system administrator or dynamically by programs. By default, the operating system defines a number of built-in services that are necessary to run the system. Use the systemctl command to list system units and to view their status. Also, the systemd-cgls command is provided to view the hierarchy of control groups and systemd-cgtop to monitor their resource consumption in real time. Use the following command to list all active units on the system: systemctl list-units The list-units option is executed by default, which means that you will receive the same output when you omit this option. To list all unit files installed on your system and their status, type: systemctl list-unit-files To view a list of all slices used on the system, type: systemctl -t slice UNIT LOAD ACTIVE SUB DESCRIPTION -.slice loaded active active Root Slice system-getty.slice loaded active active system-getty.slice system-modprobe.slice loaded active active system-modprobe.slice system.slice loaded active active System Slice user-1000.slice loaded active active User Slice of UID 1000 user.slice loaded active active User and Session Slice LOAD = Reflects whether the unit definition was properly loaded. ACTIVE = The high-level unit activation state, i.e. generalization of SUB. SUB = The low-level unit activation state, values depend on unit type. 6 loaded units listed. Pass --all to see loaded but inactive units, too. To show all installed unit files use &#39;systemctl list-unit-files&#39;. To display detailed information about a service unit that corresponds to a system service, type: systemctl status ssh.service ● ssh.service - OpenBSD Secure Shell server Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-11-23 15:07:53 CST; 49min ago Docs: man:sshd(8) man:sshd_config(5) Process: 350 ExecStartPre=/usr/sbin/sshd -t (code=exited, status=0/SUCCESS) Main PID: 367 (sshd) Tasks: 1 (limit: 4641) Memory: 8.0M CPU: 265ms CGroup: /system.slice/ssh.service └─367 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups To display the whole cgroup hierarchy on your system, type: systemd-cgls When systemd-cgls is issued without parameters, it returns the entire cgroup hierarchy. To view the information that stored in dedicated process files, type as root: cat proc/PID/cgroup Where PID stands for the ID of the process you wish to examine. The systemd-cgls command provides a static snapshot of the cgroup hierarchy. To see a dynamic account of currently running cgroups ordered by their resource usage (CPU, Memory, and IO), use: systemd-cgtop 1.4. libcgroup The libcgroup package, which was the main tool for cgroup management in previous versions of Red Hat Enterprise Linux, is now deprecated. In order to use libcgroup tools, first ensure the cgroup-tools packages are installed on your system. sudo apt-get install cgroup-tools -y # sudo yum install libcgroup-tools -y The cgroup-tools with version 0.41-21.el7 does not work with Cgroup v2. $ sudo yum --showduplicates list libcgroup-tools libcgroup-tools.x86_64 0.41-21.el7 @base The cgroup version depends on the Linux distribution being used and the default cgroup version configured on the OS. To check which cgroup version your distribution uses, run the stat -fc %T /sys/fs/cgroup/ command on the node: [6] stat -fc %T /sys/fs/cgroup/ # For cgroup v2, the output is cgroup2fs. # For cgroup v1, the output is tmpfs. Finding a Process To find the cgroup to which a process belongs, run: ps -O cgroup $ ps -o pid,cgroup:60,command PID CGROUP COMMAND 5345 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev -bash 5441 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev ps -o pid,cgroup:60,command Or, if you know the PID for the process, run: cat /proc/PID/cgroup where PID stands for a ID of the inspected process. $ cat /proc/5345/cgroup 11:cpuset:/ 10:memory:/user.slice/user-1001.slice/session-21.scope 9:devices:/user.slice 8:perf_event:/ 7:blkio:/user.slice 6:rdma:/ 5:pids:/user.slice/user-1001.slice/session-21.scope 4:freezer:/ 3:net_cls,net_prio:/ 2:cpu,cpuacct:/user.slice 1:name=systemd:/user.slice/user-1001.slice/session-21.scope 0::/user.slice/user-1001.slice/session-21.scope Listing Controllers To find the controllers that are available in your kernel and information on how they are mounted together to hierarchies, execute: cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 4 68 1 cpu 3 277 1 cpuacct 3 277 1 memory 6 277 1 devices 8 277 1 freezer 11 68 1 net_cls 10 68 1 blkio 5 277 1 perf_event 2 68 1 hugetlb 7 68 1 pids 9 277 1 net_prio 10 68 1 Alternatively, to find the mount points of particular subsystems, execute the following command: $ lssubsys -m [controllers] Here controllers stands for a list of the subsystems seperated with space in which you are interested. $ lssubsys -m cpuset /sys/fs/cgroup/cpuset cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct memory /sys/fs/cgroup/memory devices /sys/fs/cgroup/devices freezer /sys/fs/cgroup/freezer net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio blkio /sys/fs/cgroup/blkio perf_event /sys/fs/cgroup/perf_event hugetlb /sys/fs/cgroup/hugetlb pids /sys/fs/cgroup/pids $ lssubsys -m cpu memory cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct memory /sys/fs/cgroup/memory Finding Hierarchies It is recommended that you mount hierarchies under the /sys/fs/cgroup/ directory. Assuming this is the case on your system, list or browse the contents of that directory to obtain a list of hierarchies. If the tree utility is installed on your system, run it to obtain an overview of all hierarchies and the cgroups within them: tree /sys/fs/cgroup $ tree -L 1 /sys/fs/cgroup/ /sys/fs/cgroup/ ├── blkio ├── cpu -&gt; cpu,cpuacct ├── cpuacct -&gt; cpu,cpuacct ├── cpu,cpuacct ├── cpuset ├── devices ├── freezer ├── hugetlb ├── memory ├── net_cls -&gt; net_cls,net_prio ├── net_cls,net_prio ├── net_prio -&gt; net_cls,net_prio ├── perf_event ├── pids └── systemd Finding Control Groups To list the cgroups on a system, execute as root: lscgroup To restrict the output to a specific hierarchy, specify a controller and a path in the format controller:path. For example: lscgroup cpuset:adminusers The above command lists only subgroups of the adminusers cgroup in the hierarchy to which the cpuset controller is attached. Displaying Parameters of Control Groups To display the parameters of specific cgroups, run: cgget -r parameter list_of_cgroups where parameter is a pseudofile that contains values for a controller, and list_of_cgroups is a list of cgroups separated with spaces. For example: cgget -r cpuset.cpus -r memory.limit_in_bytes group1 group2 displays the values of cpuset.cpus and memory.limit_in_bytes for cgroups group1 and group2. If you do not know the names of the parameters themselves, use a command like: cgget -g cpuset / 1.5. nsenter The kernel allocates and restricts the resources for individual processes running on the Linux operating system. The namespaces within the kernel partition these resources. Namespaces allocate the resources to a process such that the process only sees those specific resources, widely-used in container runtimes to provide a layer of isolation among containers that run on the same host. [7] Use Docker to create a container from the debian:bullseye image and install procps package inside the container, which provides top and ps commands. $ docker run --rm -it --name namespace-demo -it debian:bullseye /bin/bash root@5153317a7aa2:/# apt-get update &amp;&amp; apt-get install procps -y ... root@5153317a7aa2:/# ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 481 pts/0 00:00:00 ps In another terminal, use the docker inspect command to determine the process id associated with the new container. $ docker inspect namespace-demo -f &quot;{{.State.Pid}}&quot; 415111 The process id is 415111. Each process has a /proc/[pid]/ns/ subdirectory containing one entry for each namespace that supports being manipulated by setns. Use the ls or lsns command to list the namespaces associated with a given process. $ sudo ls -l /proc/415111/ns/ total 0 lrwxrwxrwx 1 root root 0 Nov 24 12:34 cgroup -&gt; &#39;cgroup:[4026533307]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 ipc -&gt; &#39;ipc:[4026533238]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 mnt -&gt; &#39;mnt:[4026533236]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 net -&gt; &#39;net:[4026533241]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid -&gt; &#39;pid:[4026533239]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid_for_children -&gt; &#39;pid:[4026533239]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 time -&gt; &#39;time:[4026531834]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 time_for_children -&gt; &#39;time:[4026531834]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 user -&gt; &#39;user:[4026531837]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 uts -&gt; &#39;uts:[4026533237]&#39; $ sudo lsns -p 415111 NS TYPE NPROCS PID USER COMMAND 4026531834 time 208 1 root /sbin/init 4026531837 user 208 1 root /sbin/init 4026533236 mnt 1 415111 root /bin/bash 4026533237 uts 1 415111 root /bin/bash 4026533238 ipc 1 415111 root /bin/bash 4026533239 pid 1 415111 root /bin/bash 4026533241 net 1 415111 root /bin/bash 4026533307 cgroup 1 415111 root /bin/bash The nsenter command expands to namespace enter. It accepts different options to only enter the specified namespace. Let&#8217;s enter the network namespace (-n) to check the IP address and route table. $ nsenter -t 415111 -n ip a s nsenter: cannot open /proc/415111/ns/net: Permission denied $ sudo nsenter -t 415111 -n ip a s 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 19: eth0@if20: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever Here, -t is the target process id, and -n refers to the network namespace. $ sudo nsenter -t 415111 -n ip route default via 172.17.0.1 dev eth0 172.17.0.0/16 dev eth0 proto kernel scope link src 172.17.0.2 Next, enter the process namespace to check the process details. $ sudo nsenter -t 415111 -p -r ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 04:00 pts/0 00:00:00 /bin/bash root 489 0 0 04:44 ? 00:00:00 ps -ef The -r option sets the root directory to the top-level directory within the namespace so that the commands run in the context of the namespace. $ sudo nsenter -t 415111 -p -r top Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu(s): 7.1 us, 14.3 sy, 0.0 ni, 78.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 3900.2 total, 129.4 free, 2200.9 used, 1569.8 buff/cache MiB Swap: 0.0 total, 0.0 free, 0.0 used. 1459.5 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4092 3492 2984 S 0.0 0.1 0:00.06 bash 490 root 20 0 6936 3192 2708 R 0.0 0.1 0:00.01 top The bash command, which executes during docker run, is the first process inside the namespace. Enter the UTC namespace to check the hostname. $ sudo nsenter -t 415111 -u hostname 5153317a7aa2 Modify the hostname within the namespace and verify the new name. $ sudo nsenter -t 415111 -u hostname foo.bar.buzz $ sudo nsenter -t 415111 -u hostname foo.bar.buzz Finally, enter all namespaces by using the -a option. $ sudo nsenter -t 415111 -a lsns NS TYPE NPROCS PID USER COMMAND 4026531834 time 2 1 root /bin/bash 4026531837 user 2 1 root /bin/bash 4026533236 mnt 2 1 root /bin/bash 4026533237 uts 2 1 root /bin/bash 4026533238 ipc 2 1 root /bin/bash 4026533239 pid 2 1 root /bin/bash 4026533241 net 2 1 root /bin/bash 4026533307 cgroup 2 1 root /bin/bash 1.6. free free is a popular command used by system administrators on Unix/Linux platforms. It&#8217;s a powerful tool that gives insight into the memory usage in human-readable format. The man page for this command states that free displays the total amount of free and used memory on the system, including physical and swap space, as well as the buffers and caches used by the kernel. The information is gathered by parsing /proc/meminfo. $ docker version -f &#39;{{.Server.Version}}&#39; 24.0.7 $ docker info | grep Cgroup Cgroup Driver: systemd Cgroup Version: 2 $ docker run --rm -it busybox free total used free shared buff/cache available Mem: 3993764 2410712 190544 2224 1392508 1344364 Swap: 0 0 0 $ docker run --rm -it --memory 100m busybox free total used free shared buff/cache available Mem: 3993764 2421988 178952 2180 1392824 1333032 Swap: 0 0 0 $ docker run --rm -it --memory 100m busybox sh -c &#39;echo $(($(cat /sys/fs/cgroup/memory.max) / 1024 / 1024))&#39; 100 $ docker version -f &#39;{{.Server.Version}}&#39; 24.0.7 $ docker info | grep Cgroup Cgroup Driver: cgroupfs Cgroup Version: 1 $ docker run --rm -d --memory 100m busybox sleep 10m 6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 $ lscgroup | grep 6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 pids:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 devices:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 cpu,cpuacct:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 memory:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 blkio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 cpuset:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 freezer:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 net_cls,net_prio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 hugetlb:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 perf_event:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 $ cgget -r memory.limit_in_bytes /docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 /docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540: memory.limit_in_bytes: 104857600 $ docker run --rm -it --memory 100m busybox cat /sys/fs/cgroup/memory/memory.limit_in_bytes 104857600 $ sudo crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 7fe801499210a 1ec24650902b1 2 hours ago Running kube-swagger-ui 0 bee47f1dec5bc $ lscgroup | grep 7fe801499210a cpu,cpuacct:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope net_cls,net_prio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope freezer:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope pids:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope blkio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope perf_event:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope devices:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope memory:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope cpuset:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope 2. What are Containers? Containers are lightweight packages of your application code together with dependencies such as specific versions of programming language runtimes and libraries required to run your software services. [8][9] Containers make it easy to share CPU, memory, storage, and network resources at the operating systems level and offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. 2.1. Containers vs. VMs You might already be familiar with VMs: a guest operating system such as Linux or Windows runs on top of a host operating system with access to the underlying hardware. Containers are often compared to virtual machines (VMs). Like virtual machines, containers allow you to package your application together with libraries and other dependencies, providing isolated environments for running your software services. Containers are much more lightweight than VMs Containers virtualize at the OS level while VMs virtualize at the hardware level Containers share the OS kernel and use a fraction of the memory VMs require 2.2. Open Container Initiative The Open Container Initiative (OCI) is an open governance structure for the express purpose of creating open industry standards around container formats and runtimes. Established in June 2015 by Docker and other leaders in the container industry, the OCI currently contains three specifications: the Runtime Specification (runtime-spec), the Image Specification (image-spec) and the Distribution Specification (distribution-spec). Docker is donating its container format and runtime, runC, to the OCI to serve as the cornerstone of this new effort. [10] runc runc is a CLI tool for spawning and running containers on Linux according to the OCI specification. containerd containerd is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. Containerd is built on top of the Open Container Initiative’s runC and specification. Containerd is a daemon providing a GRPC API to manage containers on the local system. Containerd leverages runC to provide advanced functionality like checkpoint and restore, seccomp, and user namespace support which will open the door for these features into Docker. [11] Containerd is designed to be embedded into a larger system, rather than being used directly by developers or end-users. Containerd was designed to be used by Docker and Kubernetes as well as any other container platform that wants to abstract away syscalls or OS specific functionality to run containers on linux, windows, solaris, or other OSes. [12] 2.3. What is Docker? The word &quot;Docker&quot; refers to several things, including an open source community project; tools from the open source project; Docker Inc., the company that primarily supports that project; and the tools that company formally supports. [13] With Docker, you can treat containers like extremely lightweight, modular virtual machines. [14] The Docker technology uses the Linux kernel and features of the kernel, like Cgroups and namespaces, to segregate processes so they can run independently. This independence is the intention of containers—the ability to run multiple processes and apps separately from one another to make better use of your infrastructure while retaining the security you would have with separate systems. 2.4. What is Kubernetes? Kubernetes (also known as k8s or “kube”) is an open source container orchestration platform that automates many of the manual processes involved in deploying, managing, and scaling containerized applications. [15] With Kubernetes you can: Orchestrate containers across multiple hosts. Make better use of hardware to maximize resources needed to run your enterprise apps. Control and automate application deployments and updates. Mount and add storage to run stateful apps. Scale containerized applications and their resources on the fly. Declaratively manage services, which guarantees the deployed applications are always running the way you intended them to run. Health-check and self-heal your apps with autoplacement, autorestart, autoreplication, and autoscaling. 2.4.1. Container Runtime Interface (CRI) At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. [16] CRI (Container Runtime Interface) consists of a specifications/requirements (to-be-added), protobuf API, and libraries for container runtimes to integrate with kubelet on a node. Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server. Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface). 2.4.2. What is dockershim Just as Kubernetes started out with built-in support for Docker Engine, it also included built-in support for various storage volume solutions, network solutions, and even cloud providers. But maintaining these things on an ongoing basis became too cumbersome, so the community decided to strip all third party solutions out of the core, creating the relevant interfaces, such as: [17] Container Runtime Interface (CRI) Container Network Interface (CNI) Container Storage Interface (CSI) The idea was that any vendor could create a product that automatically interfaces with Kubernetes, as long as it is compliant with these interfaces. That doesn’t mean that non-compliant components can’t be used with Kubernetes; Kubernetes can do anything with the right components. It just means that non-compliant components need a “shim”, which translates between the component and the relevant Kubernetes interface. For example, dockershim takes CRI commands and translates them into something Docker Engine understands, and vice versa. But with the drive to take third-party components like this out of the Kubernetes core, dockershim had to be removed. The Kubernetes project plans to deprecate Docker Engine support in the kubelet and support for dockershim will be removed in a future release announced as a part of the Kubernetes v1.20 release. https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/ https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/ https://kubernetes.io/blog/2020/12/02/dockershim-faq/ https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation https://www.docker.com/blog/what-developers-need-to-know-about-docker-docker-engine-and-kubernetes-v1-20/ https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/ 2.4.3. Debugging Kubernetes nodes with crictl crictl is a command-line interface for CRI-compatible container runtimes. You can use it to inspect and debug container runtimes and applications on a Kubernetes node. crictl and its source are hosted in the cri-tools repository. [18] crictl connects to unix:///var/run/dockershim.sock by default. For other runtimes, you can set the endpoint in multiple different ways: By setting flags --runtime-endpoint and --image-endpoint By setting environment variables CONTAINER_RUNTIME_ENDPOINT and IMAGE_SERVICE_ENDPOINT By setting the endpoint in the config file --config=/etc/crictl.yaml You can also specify timeout values when connecting to the server and enable or disable debugging, by specifying timeout or debug values in the configuration file or using the --timeout and --debug command-line flags. To view or edit the current configuration, view or edit the contents of /etc/crictl.yaml. runtime-endpoint: unix:///var/run/dockershim.sock image-endpoint: unix:///var/run/dockershim.sock timeout: 10 debug: true If you use crictl to create pod sandboxes or containers on a running Kubernetes cluster, the Kubelet will eventually delete them. crictl is not a general purpose workflow tool, but a tool that is useful for debugging. List pods $ sudo crictl pods POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) d6f1ec09d33d0 20 hours ago Ready kube-swagger-ui-69b565bcb9-kzv4d default 3 (default) 8b8633b6940b7 20 hours ago Ready metrics-server-559f9dc594-mmkch kube-system 3 (default) b9c3ee6965fb8 20 hours ago Ready overprovisioning-5767847cf9-47zh2 default 3 (default) ... $ sudo crictl pods --name echoserver-66dcc9bcc6-5hwd9 POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) $ sudo crictl pods --label app=echoserver POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) List images $ sudo crictl images IMAGE TAG IMAGE ID SIZE k8s.gcr.io/coredns/coredns v1.8.4 8d147537fb7d1 47.6MB k8s.gcr.io/echoserver 1.4 a90209bb39e3d 140MB k8s.gcr.io/etcd 3.5.0-0 0048118155842 295MB k8s.gcr.io/kube-apiserver v1.22.3 53224b502ea4d 128MB ... $ sudo crictl images k8s.gcr.io/kube-apiserver IMAGE TAG IMAGE ID SIZE k8s.gcr.io/kube-apiserver v1.16.10 d925057c2fa51 170MB k8s.gcr.io/kube-apiserver v1.18.3 7e28efa976bd1 173MB k8s.gcr.io/kube-apiserver v1.22.3 53224b502ea4d 128MB List containers $ sudo crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 36720d10e6ac8 8522d622299ca 3 hours ago Running kube-flannel 0 e33d2645a22ab a93b979cf6c5a 6120bd723dced 3 hours ago Running kube-proxy 0 affa1ca7f0a2b Execute a command in a running container $ sudo crictl exec -i -t a93b979cf6c5a ls bin dev home lib64 mnt proc run srv tmp var boot etc lib media opt root sbin sys usr Get a container&#8217;s logs $ sudo crictl logs --tail 10 36720d10e6ac8 I1124 03:53:12.446713 1 iptables.go:172] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully I1124 03:53:12.447346 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I1124 03:53:12.448650 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE --random-fully I1124 03:53:12.449900 1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.1.0/24 -j RETURN I1124 03:53:12.451037 1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully I1124 03:53:12.543281 1 iptables.go:148] Some iptables rules are missing; deleting and recreating rules I1124 03:53:12.543375 1 iptables.go:172] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT I1124 03:53:12.544132 1 iptables.go:172] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT I1124 03:53:12.544818 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT I1124 03:53:12.546086 1 iptables.go:160] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT Run a pod sandbox Using crictl to run a pod sandbox is useful for debugging container runtimes. On a running Kubernetes cluster, the sandbox will eventually be stopped and deleted by the Kubelet. Create a JSON file like the following: pod-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;nginx-sandbox&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;attempt&quot;: 1, &quot;uid&quot;: &quot;hdishd83djaidwnduwk28bcsb&quot; }, &quot;log_directory&quot;: &quot;/tmp&quot;, &quot;linux&quot;: { } } Use the crictl runp command to apply the JSON and run the sandbox. $ sudo crictl runp pod-config.json 7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada The ID of the sandbox is returned. Create a container Pull a busybox image $ sudo crictl pull busybox Image is up to date for busybox@sha256:e7157b6d7ebbe2cce5eaa8cfe8aa4fa82d173999b9f90a9ec42e57323546c353 Create configs for the pod and the container: pod-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;nginx-sandbox&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;attempt&quot;: 1, &quot;uid&quot;: &quot;hdishd83djaidwnduwk28bcsb&quot; }, &quot;log_directory&quot;: &quot;/tmp&quot;, &quot;linux&quot;: { } } container-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;busybox&quot; }, &quot;image&quot;:{ &quot;image&quot;: &quot;busybox&quot; }, &quot;command&quot;: [ &quot;top&quot; ], &quot;log_path&quot;:&quot;busybox.log&quot;, &quot;linux&quot;: { } } Create the container, passing the ID of the previously-created pod, the container config file, and the pod config file. The ID of the container is returned. $ sudo crictl create 7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada container-config.json pod-config.json 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 You should set the CLI argument container-config.json before pod-config.json. Start a container To start a container, pass its ID to crictl start: $ sudo crictl start 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 Create and start a container within one command $ sudo crictl run container-config.json pod-config.json 7dc686635ed282a71b4b46210fc061847ea19f001e10f5860c335aa4375c713e $ sudo crictl ps -a CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 7dc686635ed28 busybox 20 seconds ago Exited busybox 0 32623026e0ec8 References [1] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_features [2] https://en.wikipedia.org/wiki/Linux_namespaces [3] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index [4] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd [5] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/index [6] https://kubernetes.io/docs/concepts/architecture/cgroups/ [7] https://www.redhat.com/sysadmin/container-namespaces-nsenter [8] https://cloud.google.com/learn/what-are-containers [9] https://www.redhat.com/en/topics/containers/whats-a-linux-container [10] https://opencontainers.org/about/overview/ [11] https://www.docker.com/blog/containerd-daemon-to-control-runc/ [12] https://www.docker.com/blog/what-is-containerd-runtime/ [13] https://www.redhat.com/en/topics/containers/what-is-docker [14] https://www.docker.com/resources/what-container [15] https://www.redhat.com/en/topics/containers/what-is-kubernetes [16] https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/ [17] https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/ [18] https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/ [19] https://www.suse.com/c/demystifying-containers-part-i-kernel-space/" />
<meta property="og:description" content="1. What are Control Groups 1.1. What are Resource Controllers 1.2. How Control Groups are Organized 1.3. Systemd 1.4. libcgroup 1.5. nsenter 1.6. free 2. What are Containers? 2.1. Containers vs. VMs 2.2. Open Container Initiative 2.3. What is Docker? 2.4. What is Kubernetes? References 1. What are Control Groups Linux Control Groups (cgroups) enable limits on the use of system hardware, ensuring that an individual process running inside a cgroup only utilizes as much as has been allowed in the cgroups configuration. [1] Control Groups restrict the volume of usage on a resource that has been enabled by a namespace. For example, the network namespace allows a process to access a particular network card, the cgroup ensures that the process does not exceed 50% usage of that card, ensuring bandwidth is available for other processes. Control Group Namespaces provide a virtualized view of individual cgroups through the /proc/self/ns/cgroup interface. Namespaces are a kernel feature that allow a virtual view of isolated system resources. By isolating a process from system resources, you can specify and control what a process is able to interact with. Namespaces are an essential part of Control Groups, and a fundamental aspect of containers in Linux. [2][19] Mount The mount namespace isolates file system mount points, enabling each process to have a distinct filesystem space within wich to operate. UTS UTS (UNIX Time-Sharing) namespaces allow a single system to appear to have different host and domain names to different processes. IPC System V IPC, POSIX message queues PID Process IDs Network Network namespaces virtualize the network stack. On creation, a network namespace contains only a loopback interface. Each network interface (physical or virtual) is present in exactly 1 namespace and can be moved between namespaces. Each namespace will have a private set of IP addresses, its own routing table, socket listing, connection tracking table, firewall, and other network-related resources. Destroying a network namespace destroys any virtual interfaces within it and moves any physical interfaces within it back to the initial network namespace. User User and group IDs Control Groups Isolates cgroups 1.1. What are Resource Controllers A resource controller, also called a cgroup subsystem, represents a single resource, such as CPU time or memory. The Linux kernel provides a range of resource controllers, that are mounted automatically by systemd. [3] Find the list of currently mounted resource controllers in /proc/cgroups. blkio — sets limits on input/output access to and from block devices; cpu — uses the CPU scheduler to provide cgroup tasks access to the CPU. It is mounted together with the cpuacct controller on the same mount; cpuacct — creates automatic reports on CPU resources used by tasks in a cgroup. It is mounted together with the cpu controller on the same mount; cpuset — assigns individual CPUs (on a multicore system) and memory nodes to tasks in a cgroup; devices — allows or denies access to devices for tasks in a cgroup; freezer — suspends or resumes tasks in a cgroup; memory — sets limits on memory use by tasks in a cgroup and generates automatic reports on memory resources used by those tasks; net_cls — tags network packets with a class identifier (classid) that allows the Linux traffic controller (the tc command) to identify packets originating from a particular cgroup task. A subsystem of net_cls, the net_filter (iptables) can also use this tag to perform actions on such packets. The net_filter tags network sockets with a firewall identifier (fwid) that allows the Linux firewall (the iptables command) to identify packets (skb&#8594;sk) originating from a particular cgroup task; perf_event — enables monitoring cgroups with the perf tool; hugetlb — allows to use virtual memory pages of large sizes and to enforce resource limits on these pages. 1.2. How Control Groups are Organized Cgroups are organized hierarchically, like processes, and child cgroups inherit some of the attributes of their parents. However, there are differences between the two models. [5] 1.2.1. The Linux Process Model All processes on a Linux system are child processes of a common parent: the init process, which is executed by the kernel at boot time and starts other processes (which may in turn start child processes of their own). Because all processes descend from a single parent, the Linux process model is a single hierarchy, or tree. Additionally, every Linux process except init inherits the environment (such as the PATH variable) and certain other attributes (such as open file descriptors) of its parent process. 1.2.2. The Cgroup Model Cgroups are similar to processes in that: they are hierarchical, and child cgroups inherit certain attributes from their parent cgroup. The fundamental difference is that many different hierarchies of cgroups can exist simultaneously on a system. If the Linux process model is a single tree of processes, then the cgroup model is one or more separate, unconnected trees of tasks (i.e. processes). Multiple separate hierarchies of cgroups are necessary because each hierarchy is attached to one or more subsystems. Remember that system processes are called tasks in cgroup terminology. Here are a few simple rules governing the relationships between subsystems, hierarchies of cgroups, and tasks, along with explanations of the consequences of those rules. Rule 1 A single hierarchy can have one or more subsystems attached to it. As a consequence, the cpu and memory subsystems (or any number of subsystems) can be attached to a single hierarchy, as long as each one is not attached to any other hierarchy which has any other subsystems attached to it already (see Rule 2). Rule 2 Any single subsystem (such as cpu) cannot be attached to more than one hierarchy if one of those hierarchies has a different subsystem attached to it already. As a consequence, the cpu subsystem can never be attached to two different hierarchies if one of those hierarchies already has the memory subsystem attached to it. However, a single subsystem can be attached to two hierarchies if both of those hierarchies have only that subsystem attached. Rule 3 Each time a new hierarchy is created on the systems, all tasks on the system are initially members of the default cgroup of that hierarchy, which is known as the root cgroup. For any single hierarchy you create, each task on the system can be a member of exactly one cgroup in that hierarchy. A single task may be in multiple cgroups, as long as each of those cgroups is in a different hierarchy. As soon as a task becomes a member of a second cgroup in the same hierarchy, it is removed from the first cgroup in that hierarchy. At no time is a task ever in two different cgroups in the same hierarchy. As a consequence, if the cpu and memory subsystems are attached to a hierarchy named cpu_mem_cg, and the net_cls subsystem is attached to a hierarchy named net, then a running httpd process could be a member of any one cgroup in cpu_mem_cg, and any one cgroup in net. The cgroup in cpu_mem_cg that the httpd process is a member of might restrict its CPU time to half of that allotted to other processes, and limit its memory usage to a maximum of 1024 MB. Additionally, the cgroup in net that the httpd process is a member of might limit its transmission rate to 30 MB/s (megabytes per second). When the first hierarchy is created, every task on the system is a member of at least one cgroup: the root cgroup. When using cgroups, therefore, every system task is always in at least one cgroup. Rule 4 Any process (task) on the system which forks itself creates a child task. A child task automatically inherits the cgroup membership of its parent but can be moved to different cgroups as needed. Once forked, the parent and child processes are completely independent. As a consequence, consider the httpd task that is a member of the cgroup named half_cpu_1gb_max in the cpu_and_mem hierarchy, and a member of the cgroup trans_rate_30 in the net hierarchy. When that httpd process forks itself, its child process automatically becomes a member of the half_cpu_1gb_max cgroup, and the trans_rate_30 cgroup. It inherits the exact same cgroups its parent task belongs to. From that point forward, the parent and child tasks are completely independent of each other: changing the cgroups that one task belongs to does not affect the other. Neither will changing cgroups of a parent task affect any of its grandchildren in any way. To summarize: any child task always initially inherits memberships to the exact same cgroups as their parent task, but those memberships can be changed or removed later. 1.3. Systemd Systemd is a system and service manager for Linux operating systems. It is designed to be backwards compatible with SysV init scripts, and provides a number of features such as parallel startup of system services at boot time, on-demand activation of daemons, or dependency-based service control logic. [4] Systemd introduces the concept of systemd units, represented by unit configuration files. Table 1. Available systemd Unit Types Unit Type File Extension Description Service unit .service A system service. Target unit .target A group of systemd units. Automount unit .automount A file system automount point. Device unit .device A device file recognized by the kernel. Mount unit .mount A file system mount point. Path unit .path A file or directory in a file system. Scope unit .scope An externally created process. Slice unit .slice A group of hierarchically organized units that manage system processes. Snapshot unit .snapshot A saved state of the systemd manager. Socket unit .socket An inter-process communication socket. Swap unit .swap A swap device or a swap file. Timer unit .timer A systemd timer. Table 2. Systemd Unit Files Locations Directory Description /usr/lib/systemd/system/ Systemd unit files distributed with installed RPM packages. /run/systemd/system/ Systemd unit files created at run time. This directory takes precedence over the directory with installed service unit files. /etc/systemd/system/ Systemd unit files created by systemctl enable as well as unit files added for extending a service. This directory takes precedence over the directory with runtime unit files. By default, systemd automatically creates a hierarchy of slice, scope and service units to provide a unified structure for the cgroup tree. Also, systemd automatically mounts hierarchies for important kernel resource controllers in the /sys/fs/cgroup/ directory. [3] All processes running on the system are child processes of the systemd init process. Systemd provides three unit types that are used for the purpose of resource control: Service — A process or a group of processes, which systemd started based on a unit configuration file. Services encapsulate the specified processes so that they can be started and stopped as one set. Scope — A group of externally created processes. Scopes encapsulate processes that are started and stopped by arbitrary processes through the fork() function and then registered by systemd at runtime. For instance, user sessions, containers, and virtual machines are treated as scopes. Slice — A group of hierarchically organized units. Slices do not contain processes, they organize a hierarchy in which scopes and services are placed. The actual processes are contained in scopes or in services. In this hierarchical tree, every name of a slice unit corresponds to the path to a location in the hierarchy. The dash (&#8220;-&#8221;) character acts as a separator of the path components. Services, scopes, and slices are created manually by the system administrator or dynamically by programs. By default, the operating system defines a number of built-in services that are necessary to run the system. Use the systemctl command to list system units and to view their status. Also, the systemd-cgls command is provided to view the hierarchy of control groups and systemd-cgtop to monitor their resource consumption in real time. Use the following command to list all active units on the system: systemctl list-units The list-units option is executed by default, which means that you will receive the same output when you omit this option. To list all unit files installed on your system and their status, type: systemctl list-unit-files To view a list of all slices used on the system, type: systemctl -t slice UNIT LOAD ACTIVE SUB DESCRIPTION -.slice loaded active active Root Slice system-getty.slice loaded active active system-getty.slice system-modprobe.slice loaded active active system-modprobe.slice system.slice loaded active active System Slice user-1000.slice loaded active active User Slice of UID 1000 user.slice loaded active active User and Session Slice LOAD = Reflects whether the unit definition was properly loaded. ACTIVE = The high-level unit activation state, i.e. generalization of SUB. SUB = The low-level unit activation state, values depend on unit type. 6 loaded units listed. Pass --all to see loaded but inactive units, too. To show all installed unit files use &#39;systemctl list-unit-files&#39;. To display detailed information about a service unit that corresponds to a system service, type: systemctl status ssh.service ● ssh.service - OpenBSD Secure Shell server Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-11-23 15:07:53 CST; 49min ago Docs: man:sshd(8) man:sshd_config(5) Process: 350 ExecStartPre=/usr/sbin/sshd -t (code=exited, status=0/SUCCESS) Main PID: 367 (sshd) Tasks: 1 (limit: 4641) Memory: 8.0M CPU: 265ms CGroup: /system.slice/ssh.service └─367 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups To display the whole cgroup hierarchy on your system, type: systemd-cgls When systemd-cgls is issued without parameters, it returns the entire cgroup hierarchy. To view the information that stored in dedicated process files, type as root: cat proc/PID/cgroup Where PID stands for the ID of the process you wish to examine. The systemd-cgls command provides a static snapshot of the cgroup hierarchy. To see a dynamic account of currently running cgroups ordered by their resource usage (CPU, Memory, and IO), use: systemd-cgtop 1.4. libcgroup The libcgroup package, which was the main tool for cgroup management in previous versions of Red Hat Enterprise Linux, is now deprecated. In order to use libcgroup tools, first ensure the cgroup-tools packages are installed on your system. sudo apt-get install cgroup-tools -y # sudo yum install libcgroup-tools -y The cgroup-tools with version 0.41-21.el7 does not work with Cgroup v2. $ sudo yum --showduplicates list libcgroup-tools libcgroup-tools.x86_64 0.41-21.el7 @base The cgroup version depends on the Linux distribution being used and the default cgroup version configured on the OS. To check which cgroup version your distribution uses, run the stat -fc %T /sys/fs/cgroup/ command on the node: [6] stat -fc %T /sys/fs/cgroup/ # For cgroup v2, the output is cgroup2fs. # For cgroup v1, the output is tmpfs. Finding a Process To find the cgroup to which a process belongs, run: ps -O cgroup $ ps -o pid,cgroup:60,command PID CGROUP COMMAND 5345 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev -bash 5441 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev ps -o pid,cgroup:60,command Or, if you know the PID for the process, run: cat /proc/PID/cgroup where PID stands for a ID of the inspected process. $ cat /proc/5345/cgroup 11:cpuset:/ 10:memory:/user.slice/user-1001.slice/session-21.scope 9:devices:/user.slice 8:perf_event:/ 7:blkio:/user.slice 6:rdma:/ 5:pids:/user.slice/user-1001.slice/session-21.scope 4:freezer:/ 3:net_cls,net_prio:/ 2:cpu,cpuacct:/user.slice 1:name=systemd:/user.slice/user-1001.slice/session-21.scope 0::/user.slice/user-1001.slice/session-21.scope Listing Controllers To find the controllers that are available in your kernel and information on how they are mounted together to hierarchies, execute: cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 4 68 1 cpu 3 277 1 cpuacct 3 277 1 memory 6 277 1 devices 8 277 1 freezer 11 68 1 net_cls 10 68 1 blkio 5 277 1 perf_event 2 68 1 hugetlb 7 68 1 pids 9 277 1 net_prio 10 68 1 Alternatively, to find the mount points of particular subsystems, execute the following command: $ lssubsys -m [controllers] Here controllers stands for a list of the subsystems seperated with space in which you are interested. $ lssubsys -m cpuset /sys/fs/cgroup/cpuset cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct memory /sys/fs/cgroup/memory devices /sys/fs/cgroup/devices freezer /sys/fs/cgroup/freezer net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio blkio /sys/fs/cgroup/blkio perf_event /sys/fs/cgroup/perf_event hugetlb /sys/fs/cgroup/hugetlb pids /sys/fs/cgroup/pids $ lssubsys -m cpu memory cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct memory /sys/fs/cgroup/memory Finding Hierarchies It is recommended that you mount hierarchies under the /sys/fs/cgroup/ directory. Assuming this is the case on your system, list or browse the contents of that directory to obtain a list of hierarchies. If the tree utility is installed on your system, run it to obtain an overview of all hierarchies and the cgroups within them: tree /sys/fs/cgroup $ tree -L 1 /sys/fs/cgroup/ /sys/fs/cgroup/ ├── blkio ├── cpu -&gt; cpu,cpuacct ├── cpuacct -&gt; cpu,cpuacct ├── cpu,cpuacct ├── cpuset ├── devices ├── freezer ├── hugetlb ├── memory ├── net_cls -&gt; net_cls,net_prio ├── net_cls,net_prio ├── net_prio -&gt; net_cls,net_prio ├── perf_event ├── pids └── systemd Finding Control Groups To list the cgroups on a system, execute as root: lscgroup To restrict the output to a specific hierarchy, specify a controller and a path in the format controller:path. For example: lscgroup cpuset:adminusers The above command lists only subgroups of the adminusers cgroup in the hierarchy to which the cpuset controller is attached. Displaying Parameters of Control Groups To display the parameters of specific cgroups, run: cgget -r parameter list_of_cgroups where parameter is a pseudofile that contains values for a controller, and list_of_cgroups is a list of cgroups separated with spaces. For example: cgget -r cpuset.cpus -r memory.limit_in_bytes group1 group2 displays the values of cpuset.cpus and memory.limit_in_bytes for cgroups group1 and group2. If you do not know the names of the parameters themselves, use a command like: cgget -g cpuset / 1.5. nsenter The kernel allocates and restricts the resources for individual processes running on the Linux operating system. The namespaces within the kernel partition these resources. Namespaces allocate the resources to a process such that the process only sees those specific resources, widely-used in container runtimes to provide a layer of isolation among containers that run on the same host. [7] Use Docker to create a container from the debian:bullseye image and install procps package inside the container, which provides top and ps commands. $ docker run --rm -it --name namespace-demo -it debian:bullseye /bin/bash root@5153317a7aa2:/# apt-get update &amp;&amp; apt-get install procps -y ... root@5153317a7aa2:/# ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 481 pts/0 00:00:00 ps In another terminal, use the docker inspect command to determine the process id associated with the new container. $ docker inspect namespace-demo -f &quot;{{.State.Pid}}&quot; 415111 The process id is 415111. Each process has a /proc/[pid]/ns/ subdirectory containing one entry for each namespace that supports being manipulated by setns. Use the ls or lsns command to list the namespaces associated with a given process. $ sudo ls -l /proc/415111/ns/ total 0 lrwxrwxrwx 1 root root 0 Nov 24 12:34 cgroup -&gt; &#39;cgroup:[4026533307]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 ipc -&gt; &#39;ipc:[4026533238]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 mnt -&gt; &#39;mnt:[4026533236]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 net -&gt; &#39;net:[4026533241]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid -&gt; &#39;pid:[4026533239]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid_for_children -&gt; &#39;pid:[4026533239]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 time -&gt; &#39;time:[4026531834]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 time_for_children -&gt; &#39;time:[4026531834]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 user -&gt; &#39;user:[4026531837]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 uts -&gt; &#39;uts:[4026533237]&#39; $ sudo lsns -p 415111 NS TYPE NPROCS PID USER COMMAND 4026531834 time 208 1 root /sbin/init 4026531837 user 208 1 root /sbin/init 4026533236 mnt 1 415111 root /bin/bash 4026533237 uts 1 415111 root /bin/bash 4026533238 ipc 1 415111 root /bin/bash 4026533239 pid 1 415111 root /bin/bash 4026533241 net 1 415111 root /bin/bash 4026533307 cgroup 1 415111 root /bin/bash The nsenter command expands to namespace enter. It accepts different options to only enter the specified namespace. Let&#8217;s enter the network namespace (-n) to check the IP address and route table. $ nsenter -t 415111 -n ip a s nsenter: cannot open /proc/415111/ns/net: Permission denied $ sudo nsenter -t 415111 -n ip a s 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 19: eth0@if20: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever Here, -t is the target process id, and -n refers to the network namespace. $ sudo nsenter -t 415111 -n ip route default via 172.17.0.1 dev eth0 172.17.0.0/16 dev eth0 proto kernel scope link src 172.17.0.2 Next, enter the process namespace to check the process details. $ sudo nsenter -t 415111 -p -r ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 04:00 pts/0 00:00:00 /bin/bash root 489 0 0 04:44 ? 00:00:00 ps -ef The -r option sets the root directory to the top-level directory within the namespace so that the commands run in the context of the namespace. $ sudo nsenter -t 415111 -p -r top Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu(s): 7.1 us, 14.3 sy, 0.0 ni, 78.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 3900.2 total, 129.4 free, 2200.9 used, 1569.8 buff/cache MiB Swap: 0.0 total, 0.0 free, 0.0 used. 1459.5 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4092 3492 2984 S 0.0 0.1 0:00.06 bash 490 root 20 0 6936 3192 2708 R 0.0 0.1 0:00.01 top The bash command, which executes during docker run, is the first process inside the namespace. Enter the UTC namespace to check the hostname. $ sudo nsenter -t 415111 -u hostname 5153317a7aa2 Modify the hostname within the namespace and verify the new name. $ sudo nsenter -t 415111 -u hostname foo.bar.buzz $ sudo nsenter -t 415111 -u hostname foo.bar.buzz Finally, enter all namespaces by using the -a option. $ sudo nsenter -t 415111 -a lsns NS TYPE NPROCS PID USER COMMAND 4026531834 time 2 1 root /bin/bash 4026531837 user 2 1 root /bin/bash 4026533236 mnt 2 1 root /bin/bash 4026533237 uts 2 1 root /bin/bash 4026533238 ipc 2 1 root /bin/bash 4026533239 pid 2 1 root /bin/bash 4026533241 net 2 1 root /bin/bash 4026533307 cgroup 2 1 root /bin/bash 1.6. free free is a popular command used by system administrators on Unix/Linux platforms. It&#8217;s a powerful tool that gives insight into the memory usage in human-readable format. The man page for this command states that free displays the total amount of free and used memory on the system, including physical and swap space, as well as the buffers and caches used by the kernel. The information is gathered by parsing /proc/meminfo. $ docker version -f &#39;{{.Server.Version}}&#39; 24.0.7 $ docker info | grep Cgroup Cgroup Driver: systemd Cgroup Version: 2 $ docker run --rm -it busybox free total used free shared buff/cache available Mem: 3993764 2410712 190544 2224 1392508 1344364 Swap: 0 0 0 $ docker run --rm -it --memory 100m busybox free total used free shared buff/cache available Mem: 3993764 2421988 178952 2180 1392824 1333032 Swap: 0 0 0 $ docker run --rm -it --memory 100m busybox sh -c &#39;echo $(($(cat /sys/fs/cgroup/memory.max) / 1024 / 1024))&#39; 100 $ docker version -f &#39;{{.Server.Version}}&#39; 24.0.7 $ docker info | grep Cgroup Cgroup Driver: cgroupfs Cgroup Version: 1 $ docker run --rm -d --memory 100m busybox sleep 10m 6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 $ lscgroup | grep 6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 pids:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 devices:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 cpu,cpuacct:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 memory:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 blkio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 cpuset:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 freezer:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 net_cls,net_prio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 hugetlb:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 perf_event:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 $ cgget -r memory.limit_in_bytes /docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 /docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540: memory.limit_in_bytes: 104857600 $ docker run --rm -it --memory 100m busybox cat /sys/fs/cgroup/memory/memory.limit_in_bytes 104857600 $ sudo crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 7fe801499210a 1ec24650902b1 2 hours ago Running kube-swagger-ui 0 bee47f1dec5bc $ lscgroup | grep 7fe801499210a cpu,cpuacct:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope net_cls,net_prio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope freezer:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope pids:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope blkio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope perf_event:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope devices:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope memory:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope cpuset:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope 2. What are Containers? Containers are lightweight packages of your application code together with dependencies such as specific versions of programming language runtimes and libraries required to run your software services. [8][9] Containers make it easy to share CPU, memory, storage, and network resources at the operating systems level and offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. 2.1. Containers vs. VMs You might already be familiar with VMs: a guest operating system such as Linux or Windows runs on top of a host operating system with access to the underlying hardware. Containers are often compared to virtual machines (VMs). Like virtual machines, containers allow you to package your application together with libraries and other dependencies, providing isolated environments for running your software services. Containers are much more lightweight than VMs Containers virtualize at the OS level while VMs virtualize at the hardware level Containers share the OS kernel and use a fraction of the memory VMs require 2.2. Open Container Initiative The Open Container Initiative (OCI) is an open governance structure for the express purpose of creating open industry standards around container formats and runtimes. Established in June 2015 by Docker and other leaders in the container industry, the OCI currently contains three specifications: the Runtime Specification (runtime-spec), the Image Specification (image-spec) and the Distribution Specification (distribution-spec). Docker is donating its container format and runtime, runC, to the OCI to serve as the cornerstone of this new effort. [10] runc runc is a CLI tool for spawning and running containers on Linux according to the OCI specification. containerd containerd is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. Containerd is built on top of the Open Container Initiative’s runC and specification. Containerd is a daemon providing a GRPC API to manage containers on the local system. Containerd leverages runC to provide advanced functionality like checkpoint and restore, seccomp, and user namespace support which will open the door for these features into Docker. [11] Containerd is designed to be embedded into a larger system, rather than being used directly by developers or end-users. Containerd was designed to be used by Docker and Kubernetes as well as any other container platform that wants to abstract away syscalls or OS specific functionality to run containers on linux, windows, solaris, or other OSes. [12] 2.3. What is Docker? The word &quot;Docker&quot; refers to several things, including an open source community project; tools from the open source project; Docker Inc., the company that primarily supports that project; and the tools that company formally supports. [13] With Docker, you can treat containers like extremely lightweight, modular virtual machines. [14] The Docker technology uses the Linux kernel and features of the kernel, like Cgroups and namespaces, to segregate processes so they can run independently. This independence is the intention of containers—the ability to run multiple processes and apps separately from one another to make better use of your infrastructure while retaining the security you would have with separate systems. 2.4. What is Kubernetes? Kubernetes (also known as k8s or “kube”) is an open source container orchestration platform that automates many of the manual processes involved in deploying, managing, and scaling containerized applications. [15] With Kubernetes you can: Orchestrate containers across multiple hosts. Make better use of hardware to maximize resources needed to run your enterprise apps. Control and automate application deployments and updates. Mount and add storage to run stateful apps. Scale containerized applications and their resources on the fly. Declaratively manage services, which guarantees the deployed applications are always running the way you intended them to run. Health-check and self-heal your apps with autoplacement, autorestart, autoreplication, and autoscaling. 2.4.1. Container Runtime Interface (CRI) At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. [16] CRI (Container Runtime Interface) consists of a specifications/requirements (to-be-added), protobuf API, and libraries for container runtimes to integrate with kubelet on a node. Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server. Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface). 2.4.2. What is dockershim Just as Kubernetes started out with built-in support for Docker Engine, it also included built-in support for various storage volume solutions, network solutions, and even cloud providers. But maintaining these things on an ongoing basis became too cumbersome, so the community decided to strip all third party solutions out of the core, creating the relevant interfaces, such as: [17] Container Runtime Interface (CRI) Container Network Interface (CNI) Container Storage Interface (CSI) The idea was that any vendor could create a product that automatically interfaces with Kubernetes, as long as it is compliant with these interfaces. That doesn’t mean that non-compliant components can’t be used with Kubernetes; Kubernetes can do anything with the right components. It just means that non-compliant components need a “shim”, which translates between the component and the relevant Kubernetes interface. For example, dockershim takes CRI commands and translates them into something Docker Engine understands, and vice versa. But with the drive to take third-party components like this out of the Kubernetes core, dockershim had to be removed. The Kubernetes project plans to deprecate Docker Engine support in the kubelet and support for dockershim will be removed in a future release announced as a part of the Kubernetes v1.20 release. https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/ https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/ https://kubernetes.io/blog/2020/12/02/dockershim-faq/ https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation https://www.docker.com/blog/what-developers-need-to-know-about-docker-docker-engine-and-kubernetes-v1-20/ https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/ 2.4.3. Debugging Kubernetes nodes with crictl crictl is a command-line interface for CRI-compatible container runtimes. You can use it to inspect and debug container runtimes and applications on a Kubernetes node. crictl and its source are hosted in the cri-tools repository. [18] crictl connects to unix:///var/run/dockershim.sock by default. For other runtimes, you can set the endpoint in multiple different ways: By setting flags --runtime-endpoint and --image-endpoint By setting environment variables CONTAINER_RUNTIME_ENDPOINT and IMAGE_SERVICE_ENDPOINT By setting the endpoint in the config file --config=/etc/crictl.yaml You can also specify timeout values when connecting to the server and enable or disable debugging, by specifying timeout or debug values in the configuration file or using the --timeout and --debug command-line flags. To view or edit the current configuration, view or edit the contents of /etc/crictl.yaml. runtime-endpoint: unix:///var/run/dockershim.sock image-endpoint: unix:///var/run/dockershim.sock timeout: 10 debug: true If you use crictl to create pod sandboxes or containers on a running Kubernetes cluster, the Kubelet will eventually delete them. crictl is not a general purpose workflow tool, but a tool that is useful for debugging. List pods $ sudo crictl pods POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) d6f1ec09d33d0 20 hours ago Ready kube-swagger-ui-69b565bcb9-kzv4d default 3 (default) 8b8633b6940b7 20 hours ago Ready metrics-server-559f9dc594-mmkch kube-system 3 (default) b9c3ee6965fb8 20 hours ago Ready overprovisioning-5767847cf9-47zh2 default 3 (default) ... $ sudo crictl pods --name echoserver-66dcc9bcc6-5hwd9 POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) $ sudo crictl pods --label app=echoserver POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) List images $ sudo crictl images IMAGE TAG IMAGE ID SIZE k8s.gcr.io/coredns/coredns v1.8.4 8d147537fb7d1 47.6MB k8s.gcr.io/echoserver 1.4 a90209bb39e3d 140MB k8s.gcr.io/etcd 3.5.0-0 0048118155842 295MB k8s.gcr.io/kube-apiserver v1.22.3 53224b502ea4d 128MB ... $ sudo crictl images k8s.gcr.io/kube-apiserver IMAGE TAG IMAGE ID SIZE k8s.gcr.io/kube-apiserver v1.16.10 d925057c2fa51 170MB k8s.gcr.io/kube-apiserver v1.18.3 7e28efa976bd1 173MB k8s.gcr.io/kube-apiserver v1.22.3 53224b502ea4d 128MB List containers $ sudo crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 36720d10e6ac8 8522d622299ca 3 hours ago Running kube-flannel 0 e33d2645a22ab a93b979cf6c5a 6120bd723dced 3 hours ago Running kube-proxy 0 affa1ca7f0a2b Execute a command in a running container $ sudo crictl exec -i -t a93b979cf6c5a ls bin dev home lib64 mnt proc run srv tmp var boot etc lib media opt root sbin sys usr Get a container&#8217;s logs $ sudo crictl logs --tail 10 36720d10e6ac8 I1124 03:53:12.446713 1 iptables.go:172] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully I1124 03:53:12.447346 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I1124 03:53:12.448650 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE --random-fully I1124 03:53:12.449900 1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.1.0/24 -j RETURN I1124 03:53:12.451037 1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully I1124 03:53:12.543281 1 iptables.go:148] Some iptables rules are missing; deleting and recreating rules I1124 03:53:12.543375 1 iptables.go:172] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT I1124 03:53:12.544132 1 iptables.go:172] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT I1124 03:53:12.544818 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT I1124 03:53:12.546086 1 iptables.go:160] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT Run a pod sandbox Using crictl to run a pod sandbox is useful for debugging container runtimes. On a running Kubernetes cluster, the sandbox will eventually be stopped and deleted by the Kubelet. Create a JSON file like the following: pod-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;nginx-sandbox&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;attempt&quot;: 1, &quot;uid&quot;: &quot;hdishd83djaidwnduwk28bcsb&quot; }, &quot;log_directory&quot;: &quot;/tmp&quot;, &quot;linux&quot;: { } } Use the crictl runp command to apply the JSON and run the sandbox. $ sudo crictl runp pod-config.json 7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada The ID of the sandbox is returned. Create a container Pull a busybox image $ sudo crictl pull busybox Image is up to date for busybox@sha256:e7157b6d7ebbe2cce5eaa8cfe8aa4fa82d173999b9f90a9ec42e57323546c353 Create configs for the pod and the container: pod-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;nginx-sandbox&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;attempt&quot;: 1, &quot;uid&quot;: &quot;hdishd83djaidwnduwk28bcsb&quot; }, &quot;log_directory&quot;: &quot;/tmp&quot;, &quot;linux&quot;: { } } container-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;busybox&quot; }, &quot;image&quot;:{ &quot;image&quot;: &quot;busybox&quot; }, &quot;command&quot;: [ &quot;top&quot; ], &quot;log_path&quot;:&quot;busybox.log&quot;, &quot;linux&quot;: { } } Create the container, passing the ID of the previously-created pod, the container config file, and the pod config file. The ID of the container is returned. $ sudo crictl create 7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada container-config.json pod-config.json 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 You should set the CLI argument container-config.json before pod-config.json. Start a container To start a container, pass its ID to crictl start: $ sudo crictl start 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 Create and start a container within one command $ sudo crictl run container-config.json pod-config.json 7dc686635ed282a71b4b46210fc061847ea19f001e10f5860c335aa4375c713e $ sudo crictl ps -a CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 7dc686635ed28 busybox 20 seconds ago Exited busybox 0 32623026e0ec8 References [1] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_features [2] https://en.wikipedia.org/wiki/Linux_namespaces [3] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index [4] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd [5] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/index [6] https://kubernetes.io/docs/concepts/architecture/cgroups/ [7] https://www.redhat.com/sysadmin/container-namespaces-nsenter [8] https://cloud.google.com/learn/what-are-containers [9] https://www.redhat.com/en/topics/containers/whats-a-linux-container [10] https://opencontainers.org/about/overview/ [11] https://www.docker.com/blog/containerd-daemon-to-control-runc/ [12] https://www.docker.com/blog/what-is-containerd-runtime/ [13] https://www.redhat.com/en/topics/containers/what-is-docker [14] https://www.docker.com/resources/what-container [15] https://www.redhat.com/en/topics/containers/what-is-kubernetes [16] https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/ [17] https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/ [18] https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/ [19] https://www.suse.com/c/demystifying-containers-part-i-kernel-space/" />
<link rel="canonical" href="https://blog.codefarm.me/2021/11/23/linux-cgroups-containers/" />
<meta property="og:url" content="https://blog.codefarm.me/2021/11/23/linux-cgroups-containers/" />
<meta property="og:site_name" content="CODE FARM" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-23T14:48:37+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Linux CGroups and Containers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-11-23T14:48:37+08:00","datePublished":"2021-11-23T14:48:37+08:00","description":"1. What are Control Groups 1.1. What are Resource Controllers 1.2. How Control Groups are Organized 1.3. Systemd 1.4. libcgroup 1.5. nsenter 1.6. free 2. What are Containers? 2.1. Containers vs. VMs 2.2. Open Container Initiative 2.3. What is Docker? 2.4. What is Kubernetes? References 1. What are Control Groups Linux Control Groups (cgroups) enable limits on the use of system hardware, ensuring that an individual process running inside a cgroup only utilizes as much as has been allowed in the cgroups configuration. [1] Control Groups restrict the volume of usage on a resource that has been enabled by a namespace. For example, the network namespace allows a process to access a particular network card, the cgroup ensures that the process does not exceed 50% usage of that card, ensuring bandwidth is available for other processes. Control Group Namespaces provide a virtualized view of individual cgroups through the /proc/self/ns/cgroup interface. Namespaces are a kernel feature that allow a virtual view of isolated system resources. By isolating a process from system resources, you can specify and control what a process is able to interact with. Namespaces are an essential part of Control Groups, and a fundamental aspect of containers in Linux. [2][19] Mount The mount namespace isolates file system mount points, enabling each process to have a distinct filesystem space within wich to operate. UTS UTS (UNIX Time-Sharing) namespaces allow a single system to appear to have different host and domain names to different processes. IPC System V IPC, POSIX message queues PID Process IDs Network Network namespaces virtualize the network stack. On creation, a network namespace contains only a loopback interface. Each network interface (physical or virtual) is present in exactly 1 namespace and can be moved between namespaces. Each namespace will have a private set of IP addresses, its own routing table, socket listing, connection tracking table, firewall, and other network-related resources. Destroying a network namespace destroys any virtual interfaces within it and moves any physical interfaces within it back to the initial network namespace. User User and group IDs Control Groups Isolates cgroups 1.1. What are Resource Controllers A resource controller, also called a cgroup subsystem, represents a single resource, such as CPU time or memory. The Linux kernel provides a range of resource controllers, that are mounted automatically by systemd. [3] Find the list of currently mounted resource controllers in /proc/cgroups. blkio — sets limits on input/output access to and from block devices; cpu — uses the CPU scheduler to provide cgroup tasks access to the CPU. It is mounted together with the cpuacct controller on the same mount; cpuacct — creates automatic reports on CPU resources used by tasks in a cgroup. It is mounted together with the cpu controller on the same mount; cpuset — assigns individual CPUs (on a multicore system) and memory nodes to tasks in a cgroup; devices — allows or denies access to devices for tasks in a cgroup; freezer — suspends or resumes tasks in a cgroup; memory — sets limits on memory use by tasks in a cgroup and generates automatic reports on memory resources used by those tasks; net_cls — tags network packets with a class identifier (classid) that allows the Linux traffic controller (the tc command) to identify packets originating from a particular cgroup task. A subsystem of net_cls, the net_filter (iptables) can also use this tag to perform actions on such packets. The net_filter tags network sockets with a firewall identifier (fwid) that allows the Linux firewall (the iptables command) to identify packets (skb&#8594;sk) originating from a particular cgroup task; perf_event — enables monitoring cgroups with the perf tool; hugetlb — allows to use virtual memory pages of large sizes and to enforce resource limits on these pages. 1.2. How Control Groups are Organized Cgroups are organized hierarchically, like processes, and child cgroups inherit some of the attributes of their parents. However, there are differences between the two models. [5] 1.2.1. The Linux Process Model All processes on a Linux system are child processes of a common parent: the init process, which is executed by the kernel at boot time and starts other processes (which may in turn start child processes of their own). Because all processes descend from a single parent, the Linux process model is a single hierarchy, or tree. Additionally, every Linux process except init inherits the environment (such as the PATH variable) and certain other attributes (such as open file descriptors) of its parent process. 1.2.2. The Cgroup Model Cgroups are similar to processes in that: they are hierarchical, and child cgroups inherit certain attributes from their parent cgroup. The fundamental difference is that many different hierarchies of cgroups can exist simultaneously on a system. If the Linux process model is a single tree of processes, then the cgroup model is one or more separate, unconnected trees of tasks (i.e. processes). Multiple separate hierarchies of cgroups are necessary because each hierarchy is attached to one or more subsystems. Remember that system processes are called tasks in cgroup terminology. Here are a few simple rules governing the relationships between subsystems, hierarchies of cgroups, and tasks, along with explanations of the consequences of those rules. Rule 1 A single hierarchy can have one or more subsystems attached to it. As a consequence, the cpu and memory subsystems (or any number of subsystems) can be attached to a single hierarchy, as long as each one is not attached to any other hierarchy which has any other subsystems attached to it already (see Rule 2). Rule 2 Any single subsystem (such as cpu) cannot be attached to more than one hierarchy if one of those hierarchies has a different subsystem attached to it already. As a consequence, the cpu subsystem can never be attached to two different hierarchies if one of those hierarchies already has the memory subsystem attached to it. However, a single subsystem can be attached to two hierarchies if both of those hierarchies have only that subsystem attached. Rule 3 Each time a new hierarchy is created on the systems, all tasks on the system are initially members of the default cgroup of that hierarchy, which is known as the root cgroup. For any single hierarchy you create, each task on the system can be a member of exactly one cgroup in that hierarchy. A single task may be in multiple cgroups, as long as each of those cgroups is in a different hierarchy. As soon as a task becomes a member of a second cgroup in the same hierarchy, it is removed from the first cgroup in that hierarchy. At no time is a task ever in two different cgroups in the same hierarchy. As a consequence, if the cpu and memory subsystems are attached to a hierarchy named cpu_mem_cg, and the net_cls subsystem is attached to a hierarchy named net, then a running httpd process could be a member of any one cgroup in cpu_mem_cg, and any one cgroup in net. The cgroup in cpu_mem_cg that the httpd process is a member of might restrict its CPU time to half of that allotted to other processes, and limit its memory usage to a maximum of 1024 MB. Additionally, the cgroup in net that the httpd process is a member of might limit its transmission rate to 30 MB/s (megabytes per second). When the first hierarchy is created, every task on the system is a member of at least one cgroup: the root cgroup. When using cgroups, therefore, every system task is always in at least one cgroup. Rule 4 Any process (task) on the system which forks itself creates a child task. A child task automatically inherits the cgroup membership of its parent but can be moved to different cgroups as needed. Once forked, the parent and child processes are completely independent. As a consequence, consider the httpd task that is a member of the cgroup named half_cpu_1gb_max in the cpu_and_mem hierarchy, and a member of the cgroup trans_rate_30 in the net hierarchy. When that httpd process forks itself, its child process automatically becomes a member of the half_cpu_1gb_max cgroup, and the trans_rate_30 cgroup. It inherits the exact same cgroups its parent task belongs to. From that point forward, the parent and child tasks are completely independent of each other: changing the cgroups that one task belongs to does not affect the other. Neither will changing cgroups of a parent task affect any of its grandchildren in any way. To summarize: any child task always initially inherits memberships to the exact same cgroups as their parent task, but those memberships can be changed or removed later. 1.3. Systemd Systemd is a system and service manager for Linux operating systems. It is designed to be backwards compatible with SysV init scripts, and provides a number of features such as parallel startup of system services at boot time, on-demand activation of daemons, or dependency-based service control logic. [4] Systemd introduces the concept of systemd units, represented by unit configuration files. Table 1. Available systemd Unit Types Unit Type File Extension Description Service unit .service A system service. Target unit .target A group of systemd units. Automount unit .automount A file system automount point. Device unit .device A device file recognized by the kernel. Mount unit .mount A file system mount point. Path unit .path A file or directory in a file system. Scope unit .scope An externally created process. Slice unit .slice A group of hierarchically organized units that manage system processes. Snapshot unit .snapshot A saved state of the systemd manager. Socket unit .socket An inter-process communication socket. Swap unit .swap A swap device or a swap file. Timer unit .timer A systemd timer. Table 2. Systemd Unit Files Locations Directory Description /usr/lib/systemd/system/ Systemd unit files distributed with installed RPM packages. /run/systemd/system/ Systemd unit files created at run time. This directory takes precedence over the directory with installed service unit files. /etc/systemd/system/ Systemd unit files created by systemctl enable as well as unit files added for extending a service. This directory takes precedence over the directory with runtime unit files. By default, systemd automatically creates a hierarchy of slice, scope and service units to provide a unified structure for the cgroup tree. Also, systemd automatically mounts hierarchies for important kernel resource controllers in the /sys/fs/cgroup/ directory. [3] All processes running on the system are child processes of the systemd init process. Systemd provides three unit types that are used for the purpose of resource control: Service — A process or a group of processes, which systemd started based on a unit configuration file. Services encapsulate the specified processes so that they can be started and stopped as one set. Scope — A group of externally created processes. Scopes encapsulate processes that are started and stopped by arbitrary processes through the fork() function and then registered by systemd at runtime. For instance, user sessions, containers, and virtual machines are treated as scopes. Slice — A group of hierarchically organized units. Slices do not contain processes, they organize a hierarchy in which scopes and services are placed. The actual processes are contained in scopes or in services. In this hierarchical tree, every name of a slice unit corresponds to the path to a location in the hierarchy. The dash (&#8220;-&#8221;) character acts as a separator of the path components. Services, scopes, and slices are created manually by the system administrator or dynamically by programs. By default, the operating system defines a number of built-in services that are necessary to run the system. Use the systemctl command to list system units and to view their status. Also, the systemd-cgls command is provided to view the hierarchy of control groups and systemd-cgtop to monitor their resource consumption in real time. Use the following command to list all active units on the system: systemctl list-units The list-units option is executed by default, which means that you will receive the same output when you omit this option. To list all unit files installed on your system and their status, type: systemctl list-unit-files To view a list of all slices used on the system, type: systemctl -t slice UNIT LOAD ACTIVE SUB DESCRIPTION -.slice loaded active active Root Slice system-getty.slice loaded active active system-getty.slice system-modprobe.slice loaded active active system-modprobe.slice system.slice loaded active active System Slice user-1000.slice loaded active active User Slice of UID 1000 user.slice loaded active active User and Session Slice LOAD = Reflects whether the unit definition was properly loaded. ACTIVE = The high-level unit activation state, i.e. generalization of SUB. SUB = The low-level unit activation state, values depend on unit type. 6 loaded units listed. Pass --all to see loaded but inactive units, too. To show all installed unit files use &#39;systemctl list-unit-files&#39;. To display detailed information about a service unit that corresponds to a system service, type: systemctl status ssh.service ● ssh.service - OpenBSD Secure Shell server Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-11-23 15:07:53 CST; 49min ago Docs: man:sshd(8) man:sshd_config(5) Process: 350 ExecStartPre=/usr/sbin/sshd -t (code=exited, status=0/SUCCESS) Main PID: 367 (sshd) Tasks: 1 (limit: 4641) Memory: 8.0M CPU: 265ms CGroup: /system.slice/ssh.service └─367 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups To display the whole cgroup hierarchy on your system, type: systemd-cgls When systemd-cgls is issued without parameters, it returns the entire cgroup hierarchy. To view the information that stored in dedicated process files, type as root: cat proc/PID/cgroup Where PID stands for the ID of the process you wish to examine. The systemd-cgls command provides a static snapshot of the cgroup hierarchy. To see a dynamic account of currently running cgroups ordered by their resource usage (CPU, Memory, and IO), use: systemd-cgtop 1.4. libcgroup The libcgroup package, which was the main tool for cgroup management in previous versions of Red Hat Enterprise Linux, is now deprecated. In order to use libcgroup tools, first ensure the cgroup-tools packages are installed on your system. sudo apt-get install cgroup-tools -y # sudo yum install libcgroup-tools -y The cgroup-tools with version 0.41-21.el7 does not work with Cgroup v2. $ sudo yum --showduplicates list libcgroup-tools libcgroup-tools.x86_64 0.41-21.el7 @base The cgroup version depends on the Linux distribution being used and the default cgroup version configured on the OS. To check which cgroup version your distribution uses, run the stat -fc %T /sys/fs/cgroup/ command on the node: [6] stat -fc %T /sys/fs/cgroup/ # For cgroup v2, the output is cgroup2fs. # For cgroup v1, the output is tmpfs. Finding a Process To find the cgroup to which a process belongs, run: ps -O cgroup $ ps -o pid,cgroup:60,command PID CGROUP COMMAND 5345 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev -bash 5441 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev ps -o pid,cgroup:60,command Or, if you know the PID for the process, run: cat /proc/PID/cgroup where PID stands for a ID of the inspected process. $ cat /proc/5345/cgroup 11:cpuset:/ 10:memory:/user.slice/user-1001.slice/session-21.scope 9:devices:/user.slice 8:perf_event:/ 7:blkio:/user.slice 6:rdma:/ 5:pids:/user.slice/user-1001.slice/session-21.scope 4:freezer:/ 3:net_cls,net_prio:/ 2:cpu,cpuacct:/user.slice 1:name=systemd:/user.slice/user-1001.slice/session-21.scope 0::/user.slice/user-1001.slice/session-21.scope Listing Controllers To find the controllers that are available in your kernel and information on how they are mounted together to hierarchies, execute: cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 4 68 1 cpu 3 277 1 cpuacct 3 277 1 memory 6 277 1 devices 8 277 1 freezer 11 68 1 net_cls 10 68 1 blkio 5 277 1 perf_event 2 68 1 hugetlb 7 68 1 pids 9 277 1 net_prio 10 68 1 Alternatively, to find the mount points of particular subsystems, execute the following command: $ lssubsys -m [controllers] Here controllers stands for a list of the subsystems seperated with space in which you are interested. $ lssubsys -m cpuset /sys/fs/cgroup/cpuset cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct memory /sys/fs/cgroup/memory devices /sys/fs/cgroup/devices freezer /sys/fs/cgroup/freezer net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio blkio /sys/fs/cgroup/blkio perf_event /sys/fs/cgroup/perf_event hugetlb /sys/fs/cgroup/hugetlb pids /sys/fs/cgroup/pids $ lssubsys -m cpu memory cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct memory /sys/fs/cgroup/memory Finding Hierarchies It is recommended that you mount hierarchies under the /sys/fs/cgroup/ directory. Assuming this is the case on your system, list or browse the contents of that directory to obtain a list of hierarchies. If the tree utility is installed on your system, run it to obtain an overview of all hierarchies and the cgroups within them: tree /sys/fs/cgroup $ tree -L 1 /sys/fs/cgroup/ /sys/fs/cgroup/ ├── blkio ├── cpu -&gt; cpu,cpuacct ├── cpuacct -&gt; cpu,cpuacct ├── cpu,cpuacct ├── cpuset ├── devices ├── freezer ├── hugetlb ├── memory ├── net_cls -&gt; net_cls,net_prio ├── net_cls,net_prio ├── net_prio -&gt; net_cls,net_prio ├── perf_event ├── pids └── systemd Finding Control Groups To list the cgroups on a system, execute as root: lscgroup To restrict the output to a specific hierarchy, specify a controller and a path in the format controller:path. For example: lscgroup cpuset:adminusers The above command lists only subgroups of the adminusers cgroup in the hierarchy to which the cpuset controller is attached. Displaying Parameters of Control Groups To display the parameters of specific cgroups, run: cgget -r parameter list_of_cgroups where parameter is a pseudofile that contains values for a controller, and list_of_cgroups is a list of cgroups separated with spaces. For example: cgget -r cpuset.cpus -r memory.limit_in_bytes group1 group2 displays the values of cpuset.cpus and memory.limit_in_bytes for cgroups group1 and group2. If you do not know the names of the parameters themselves, use a command like: cgget -g cpuset / 1.5. nsenter The kernel allocates and restricts the resources for individual processes running on the Linux operating system. The namespaces within the kernel partition these resources. Namespaces allocate the resources to a process such that the process only sees those specific resources, widely-used in container runtimes to provide a layer of isolation among containers that run on the same host. [7] Use Docker to create a container from the debian:bullseye image and install procps package inside the container, which provides top and ps commands. $ docker run --rm -it --name namespace-demo -it debian:bullseye /bin/bash root@5153317a7aa2:/# apt-get update &amp;&amp; apt-get install procps -y ... root@5153317a7aa2:/# ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 481 pts/0 00:00:00 ps In another terminal, use the docker inspect command to determine the process id associated with the new container. $ docker inspect namespace-demo -f &quot;{{.State.Pid}}&quot; 415111 The process id is 415111. Each process has a /proc/[pid]/ns/ subdirectory containing one entry for each namespace that supports being manipulated by setns. Use the ls or lsns command to list the namespaces associated with a given process. $ sudo ls -l /proc/415111/ns/ total 0 lrwxrwxrwx 1 root root 0 Nov 24 12:34 cgroup -&gt; &#39;cgroup:[4026533307]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 ipc -&gt; &#39;ipc:[4026533238]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 mnt -&gt; &#39;mnt:[4026533236]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 net -&gt; &#39;net:[4026533241]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid -&gt; &#39;pid:[4026533239]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid_for_children -&gt; &#39;pid:[4026533239]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 time -&gt; &#39;time:[4026531834]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 time_for_children -&gt; &#39;time:[4026531834]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 user -&gt; &#39;user:[4026531837]&#39; lrwxrwxrwx 1 root root 0 Nov 24 12:34 uts -&gt; &#39;uts:[4026533237]&#39; $ sudo lsns -p 415111 NS TYPE NPROCS PID USER COMMAND 4026531834 time 208 1 root /sbin/init 4026531837 user 208 1 root /sbin/init 4026533236 mnt 1 415111 root /bin/bash 4026533237 uts 1 415111 root /bin/bash 4026533238 ipc 1 415111 root /bin/bash 4026533239 pid 1 415111 root /bin/bash 4026533241 net 1 415111 root /bin/bash 4026533307 cgroup 1 415111 root /bin/bash The nsenter command expands to namespace enter. It accepts different options to only enter the specified namespace. Let&#8217;s enter the network namespace (-n) to check the IP address and route table. $ nsenter -t 415111 -n ip a s nsenter: cannot open /proc/415111/ns/net: Permission denied $ sudo nsenter -t 415111 -n ip a s 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 19: eth0@if20: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever Here, -t is the target process id, and -n refers to the network namespace. $ sudo nsenter -t 415111 -n ip route default via 172.17.0.1 dev eth0 172.17.0.0/16 dev eth0 proto kernel scope link src 172.17.0.2 Next, enter the process namespace to check the process details. $ sudo nsenter -t 415111 -p -r ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 04:00 pts/0 00:00:00 /bin/bash root 489 0 0 04:44 ? 00:00:00 ps -ef The -r option sets the root directory to the top-level directory within the namespace so that the commands run in the context of the namespace. $ sudo nsenter -t 415111 -p -r top Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie %Cpu(s): 7.1 us, 14.3 sy, 0.0 ni, 78.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 3900.2 total, 129.4 free, 2200.9 used, 1569.8 buff/cache MiB Swap: 0.0 total, 0.0 free, 0.0 used. 1459.5 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 4092 3492 2984 S 0.0 0.1 0:00.06 bash 490 root 20 0 6936 3192 2708 R 0.0 0.1 0:00.01 top The bash command, which executes during docker run, is the first process inside the namespace. Enter the UTC namespace to check the hostname. $ sudo nsenter -t 415111 -u hostname 5153317a7aa2 Modify the hostname within the namespace and verify the new name. $ sudo nsenter -t 415111 -u hostname foo.bar.buzz $ sudo nsenter -t 415111 -u hostname foo.bar.buzz Finally, enter all namespaces by using the -a option. $ sudo nsenter -t 415111 -a lsns NS TYPE NPROCS PID USER COMMAND 4026531834 time 2 1 root /bin/bash 4026531837 user 2 1 root /bin/bash 4026533236 mnt 2 1 root /bin/bash 4026533237 uts 2 1 root /bin/bash 4026533238 ipc 2 1 root /bin/bash 4026533239 pid 2 1 root /bin/bash 4026533241 net 2 1 root /bin/bash 4026533307 cgroup 2 1 root /bin/bash 1.6. free free is a popular command used by system administrators on Unix/Linux platforms. It&#8217;s a powerful tool that gives insight into the memory usage in human-readable format. The man page for this command states that free displays the total amount of free and used memory on the system, including physical and swap space, as well as the buffers and caches used by the kernel. The information is gathered by parsing /proc/meminfo. $ docker version -f &#39;{{.Server.Version}}&#39; 24.0.7 $ docker info | grep Cgroup Cgroup Driver: systemd Cgroup Version: 2 $ docker run --rm -it busybox free total used free shared buff/cache available Mem: 3993764 2410712 190544 2224 1392508 1344364 Swap: 0 0 0 $ docker run --rm -it --memory 100m busybox free total used free shared buff/cache available Mem: 3993764 2421988 178952 2180 1392824 1333032 Swap: 0 0 0 $ docker run --rm -it --memory 100m busybox sh -c &#39;echo $(($(cat /sys/fs/cgroup/memory.max) / 1024 / 1024))&#39; 100 $ docker version -f &#39;{{.Server.Version}}&#39; 24.0.7 $ docker info | grep Cgroup Cgroup Driver: cgroupfs Cgroup Version: 1 $ docker run --rm -d --memory 100m busybox sleep 10m 6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 $ lscgroup | grep 6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 pids:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 devices:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 cpu,cpuacct:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 memory:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 blkio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 cpuset:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 freezer:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 net_cls,net_prio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 hugetlb:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 perf_event:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 $ cgget -r memory.limit_in_bytes /docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540 /docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540: memory.limit_in_bytes: 104857600 $ docker run --rm -it --memory 100m busybox cat /sys/fs/cgroup/memory/memory.limit_in_bytes 104857600 $ sudo crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 7fe801499210a 1ec24650902b1 2 hours ago Running kube-swagger-ui 0 bee47f1dec5bc $ lscgroup | grep 7fe801499210a cpu,cpuacct:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope net_cls,net_prio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope freezer:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope pids:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope blkio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope perf_event:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope devices:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope memory:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope cpuset:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope 2. What are Containers? Containers are lightweight packages of your application code together with dependencies such as specific versions of programming language runtimes and libraries required to run your software services. [8][9] Containers make it easy to share CPU, memory, storage, and network resources at the operating systems level and offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. 2.1. Containers vs. VMs You might already be familiar with VMs: a guest operating system such as Linux or Windows runs on top of a host operating system with access to the underlying hardware. Containers are often compared to virtual machines (VMs). Like virtual machines, containers allow you to package your application together with libraries and other dependencies, providing isolated environments for running your software services. Containers are much more lightweight than VMs Containers virtualize at the OS level while VMs virtualize at the hardware level Containers share the OS kernel and use a fraction of the memory VMs require 2.2. Open Container Initiative The Open Container Initiative (OCI) is an open governance structure for the express purpose of creating open industry standards around container formats and runtimes. Established in June 2015 by Docker and other leaders in the container industry, the OCI currently contains three specifications: the Runtime Specification (runtime-spec), the Image Specification (image-spec) and the Distribution Specification (distribution-spec). Docker is donating its container format and runtime, runC, to the OCI to serve as the cornerstone of this new effort. [10] runc runc is a CLI tool for spawning and running containers on Linux according to the OCI specification. containerd containerd is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. Containerd is built on top of the Open Container Initiative’s runC and specification. Containerd is a daemon providing a GRPC API to manage containers on the local system. Containerd leverages runC to provide advanced functionality like checkpoint and restore, seccomp, and user namespace support which will open the door for these features into Docker. [11] Containerd is designed to be embedded into a larger system, rather than being used directly by developers or end-users. Containerd was designed to be used by Docker and Kubernetes as well as any other container platform that wants to abstract away syscalls or OS specific functionality to run containers on linux, windows, solaris, or other OSes. [12] 2.3. What is Docker? The word &quot;Docker&quot; refers to several things, including an open source community project; tools from the open source project; Docker Inc., the company that primarily supports that project; and the tools that company formally supports. [13] With Docker, you can treat containers like extremely lightweight, modular virtual machines. [14] The Docker technology uses the Linux kernel and features of the kernel, like Cgroups and namespaces, to segregate processes so they can run independently. This independence is the intention of containers—the ability to run multiple processes and apps separately from one another to make better use of your infrastructure while retaining the security you would have with separate systems. 2.4. What is Kubernetes? Kubernetes (also known as k8s or “kube”) is an open source container orchestration platform that automates many of the manual processes involved in deploying, managing, and scaling containerized applications. [15] With Kubernetes you can: Orchestrate containers across multiple hosts. Make better use of hardware to maximize resources needed to run your enterprise apps. Control and automate application deployments and updates. Mount and add storage to run stateful apps. Scale containerized applications and their resources on the fly. Declaratively manage services, which guarantees the deployed applications are always running the way you intended them to run. Health-check and self-heal your apps with autoplacement, autorestart, autoreplication, and autoscaling. 2.4.1. Container Runtime Interface (CRI) At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. [16] CRI (Container Runtime Interface) consists of a specifications/requirements (to-be-added), protobuf API, and libraries for container runtimes to integrate with kubelet on a node. Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server. Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface). 2.4.2. What is dockershim Just as Kubernetes started out with built-in support for Docker Engine, it also included built-in support for various storage volume solutions, network solutions, and even cloud providers. But maintaining these things on an ongoing basis became too cumbersome, so the community decided to strip all third party solutions out of the core, creating the relevant interfaces, such as: [17] Container Runtime Interface (CRI) Container Network Interface (CNI) Container Storage Interface (CSI) The idea was that any vendor could create a product that automatically interfaces with Kubernetes, as long as it is compliant with these interfaces. That doesn’t mean that non-compliant components can’t be used with Kubernetes; Kubernetes can do anything with the right components. It just means that non-compliant components need a “shim”, which translates between the component and the relevant Kubernetes interface. For example, dockershim takes CRI commands and translates them into something Docker Engine understands, and vice versa. But with the drive to take third-party components like this out of the Kubernetes core, dockershim had to be removed. The Kubernetes project plans to deprecate Docker Engine support in the kubelet and support for dockershim will be removed in a future release announced as a part of the Kubernetes v1.20 release. https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/ https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/ https://kubernetes.io/blog/2020/12/02/dockershim-faq/ https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation https://www.docker.com/blog/what-developers-need-to-know-about-docker-docker-engine-and-kubernetes-v1-20/ https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/ 2.4.3. Debugging Kubernetes nodes with crictl crictl is a command-line interface for CRI-compatible container runtimes. You can use it to inspect and debug container runtimes and applications on a Kubernetes node. crictl and its source are hosted in the cri-tools repository. [18] crictl connects to unix:///var/run/dockershim.sock by default. For other runtimes, you can set the endpoint in multiple different ways: By setting flags --runtime-endpoint and --image-endpoint By setting environment variables CONTAINER_RUNTIME_ENDPOINT and IMAGE_SERVICE_ENDPOINT By setting the endpoint in the config file --config=/etc/crictl.yaml You can also specify timeout values when connecting to the server and enable or disable debugging, by specifying timeout or debug values in the configuration file or using the --timeout and --debug command-line flags. To view or edit the current configuration, view or edit the contents of /etc/crictl.yaml. runtime-endpoint: unix:///var/run/dockershim.sock image-endpoint: unix:///var/run/dockershim.sock timeout: 10 debug: true If you use crictl to create pod sandboxes or containers on a running Kubernetes cluster, the Kubelet will eventually delete them. crictl is not a general purpose workflow tool, but a tool that is useful for debugging. List pods $ sudo crictl pods POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) d6f1ec09d33d0 20 hours ago Ready kube-swagger-ui-69b565bcb9-kzv4d default 3 (default) 8b8633b6940b7 20 hours ago Ready metrics-server-559f9dc594-mmkch kube-system 3 (default) b9c3ee6965fb8 20 hours ago Ready overprovisioning-5767847cf9-47zh2 default 3 (default) ... $ sudo crictl pods --name echoserver-66dcc9bcc6-5hwd9 POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) $ sudo crictl pods --label app=echoserver POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 1ebbed223d8d2 20 hours ago Ready echoserver-66dcc9bcc6-5hwd9 default 2 (default) List images $ sudo crictl images IMAGE TAG IMAGE ID SIZE k8s.gcr.io/coredns/coredns v1.8.4 8d147537fb7d1 47.6MB k8s.gcr.io/echoserver 1.4 a90209bb39e3d 140MB k8s.gcr.io/etcd 3.5.0-0 0048118155842 295MB k8s.gcr.io/kube-apiserver v1.22.3 53224b502ea4d 128MB ... $ sudo crictl images k8s.gcr.io/kube-apiserver IMAGE TAG IMAGE ID SIZE k8s.gcr.io/kube-apiserver v1.16.10 d925057c2fa51 170MB k8s.gcr.io/kube-apiserver v1.18.3 7e28efa976bd1 173MB k8s.gcr.io/kube-apiserver v1.22.3 53224b502ea4d 128MB List containers $ sudo crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 36720d10e6ac8 8522d622299ca 3 hours ago Running kube-flannel 0 e33d2645a22ab a93b979cf6c5a 6120bd723dced 3 hours ago Running kube-proxy 0 affa1ca7f0a2b Execute a command in a running container $ sudo crictl exec -i -t a93b979cf6c5a ls bin dev home lib64 mnt proc run srv tmp var boot etc lib media opt root sbin sys usr Get a container&#8217;s logs $ sudo crictl logs --tail 10 36720d10e6ac8 I1124 03:53:12.446713 1 iptables.go:172] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully I1124 03:53:12.447346 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I1124 03:53:12.448650 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE --random-fully I1124 03:53:12.449900 1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.1.0/24 -j RETURN I1124 03:53:12.451037 1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully I1124 03:53:12.543281 1 iptables.go:148] Some iptables rules are missing; deleting and recreating rules I1124 03:53:12.543375 1 iptables.go:172] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT I1124 03:53:12.544132 1 iptables.go:172] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT I1124 03:53:12.544818 1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT I1124 03:53:12.546086 1 iptables.go:160] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT Run a pod sandbox Using crictl to run a pod sandbox is useful for debugging container runtimes. On a running Kubernetes cluster, the sandbox will eventually be stopped and deleted by the Kubelet. Create a JSON file like the following: pod-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;nginx-sandbox&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;attempt&quot;: 1, &quot;uid&quot;: &quot;hdishd83djaidwnduwk28bcsb&quot; }, &quot;log_directory&quot;: &quot;/tmp&quot;, &quot;linux&quot;: { } } Use the crictl runp command to apply the JSON and run the sandbox. $ sudo crictl runp pod-config.json 7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada The ID of the sandbox is returned. Create a container Pull a busybox image $ sudo crictl pull busybox Image is up to date for busybox@sha256:e7157b6d7ebbe2cce5eaa8cfe8aa4fa82d173999b9f90a9ec42e57323546c353 Create configs for the pod and the container: pod-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;nginx-sandbox&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;attempt&quot;: 1, &quot;uid&quot;: &quot;hdishd83djaidwnduwk28bcsb&quot; }, &quot;log_directory&quot;: &quot;/tmp&quot;, &quot;linux&quot;: { } } container-config.json { &quot;metadata&quot;: { &quot;name&quot;: &quot;busybox&quot; }, &quot;image&quot;:{ &quot;image&quot;: &quot;busybox&quot; }, &quot;command&quot;: [ &quot;top&quot; ], &quot;log_path&quot;:&quot;busybox.log&quot;, &quot;linux&quot;: { } } Create the container, passing the ID of the previously-created pod, the container config file, and the pod config file. The ID of the container is returned. $ sudo crictl create 7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada container-config.json pod-config.json 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 You should set the CLI argument container-config.json before pod-config.json. Start a container To start a container, pass its ID to crictl start: $ sudo crictl start 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88 Create and start a container within one command $ sudo crictl run container-config.json pod-config.json 7dc686635ed282a71b4b46210fc061847ea19f001e10f5860c335aa4375c713e $ sudo crictl ps -a CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 7dc686635ed28 busybox 20 seconds ago Exited busybox 0 32623026e0ec8 References [1] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_features [2] https://en.wikipedia.org/wiki/Linux_namespaces [3] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index [4] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd [5] https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/index [6] https://kubernetes.io/docs/concepts/architecture/cgroups/ [7] https://www.redhat.com/sysadmin/container-namespaces-nsenter [8] https://cloud.google.com/learn/what-are-containers [9] https://www.redhat.com/en/topics/containers/whats-a-linux-container [10] https://opencontainers.org/about/overview/ [11] https://www.docker.com/blog/containerd-daemon-to-control-runc/ [12] https://www.docker.com/blog/what-is-containerd-runtime/ [13] https://www.redhat.com/en/topics/containers/what-is-docker [14] https://www.docker.com/resources/what-container [15] https://www.redhat.com/en/topics/containers/what-is-kubernetes [16] https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/ [17] https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/ [18] https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/ [19] https://www.suse.com/c/demystifying-containers-part-i-kernel-space/","headline":"Linux CGroups and Containers","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.codefarm.me/2021/11/23/linux-cgroups-containers/"},"url":"https://blog.codefarm.me/2021/11/23/linux-cgroups-containers/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="/assets/css/style.css"><!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SN88FJ18E5"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-SN88FJ18E5');
    </script></head>
  <body>
    <header class="c-header">
  <div class="o-container">
    <a class="c-header-title" href="/">CODE FARM</a>
    <button class="c-header-nav-toggle" id="nav-toggle" aria-label="Toggle navigation">
      <span class="c-header-nav-toggle-icon"></span>
    </button>
    <div class="c-header-nav-wrapper" id="nav-wrapper">
      <nav class="c-header-nav">
        <a href="/">Home</a>
        <a href="/categories/">Category</a>
        <a href="/tags/">Tag</a>
        <a href="/archives/">Archive</a>
        <a href="/about/">About</a>
        <a href="https://resume.github.io/?looogos" target="_blank">R&eacute;sum&eacute;</a>
      </nav>
    </div>
  </div>
  



<div class="o-container">
  <div class="c-banner">
    <img src="/assets/images/galaxy.svg" alt="Galaxy background" class="c-banner-bg">
    <div class="c-banner-quote">
      <p>"The Renaissance was a time when art, science, and philosophy flourished."</p>
      <cite>- Michelangelo</cite>
    </div>
  </div>
</div>
</header>

    <main class="o-container">
      <article class="c-post">
  <header class="c-post-header">
    <h1 class="c-post-title">Linux CGroups and Containers</h1><p class="c-post-meta">03 Feb 2024</p>
  </header>

  <div class="c-post-content">
    <div id="toc" class="toc">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#what-are-control-groups">1. What are Control Groups</a>
<ul class="sectlevel2">
<li><a href="#what-are-resource-controllers">1.1. What are Resource Controllers</a></li>
<li><a href="#how-control-groups-are-organized">1.2. How Control Groups are Organized</a></li>
<li><a href="#systemd">1.3. Systemd</a></li>
<li><a href="#libcgroup">1.4. libcgroup</a></li>
<li><a href="#nsenter">1.5. nsenter</a></li>
<li><a href="#free">1.6. free</a></li>
</ul>
</li>
<li><a href="#what-are-containers">2. What are Containers?</a>
<ul class="sectlevel2">
<li><a href="#containers-vs-vms">2.1. Containers vs. VMs</a></li>
<li><a href="#open-container-initiative">2.2. Open Container Initiative</a></li>
<li><a href="#what-is-docker">2.3. What is Docker?</a></li>
<li><a href="#what-is-kubernetes">2.4. What is Kubernetes?</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>
<div class="sect1">
<h2 id="what-are-control-groups">1. What are Control Groups</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Linux Control Groups (cgroups) enable limits on the use of system hardware, ensuring that an individual process running inside a <strong>cgroup</strong> only utilizes as much as has been allowed in the <strong>cgroups</strong> configuration. <a href="#rhel7-kernel_features">[1]</a></p>
</div>
<div class="paragraph">
<p>Control Groups restrict the volume of usage on a resource that has been enabled by a <strong>namespace</strong>. For example, the network namespace allows a process to access a particular network card, the cgroup ensures that the process does not exceed 50% usage of that card, ensuring bandwidth is available for other processes.</p>
</div>
<div class="paragraph">
<p>Control Group Namespaces provide a virtualized view of individual cgroups through the <code>/proc/self/ns/cgroup</code> interface.</p>
</div>
<div class="paragraph">
<p><strong>Namespace</strong>s are a kernel feature that allow a virtual view of isolated system resources. By isolating a process from system resources, you can specify and control what a process is able to interact with.</p>
</div>
<div class="paragraph">
<p>Namespaces are an essential part of Control Groups, and a fundamental aspect of <a href="https://en.wikipedia.org/wiki/OS-level_virtualization">containers</a> in Linux. <a href="#Linux_namespaces">[2]</a><a href="#suse-demystifying-containers-part-i-kernel-space">[19]</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Mount</strong></p>
<div class="paragraph">
<p>The mount namespace isolates file system mount points, enabling each process to have a distinct filesystem space within wich to operate.</p>
</div>
</li>
<li>
<p><strong>UTS</strong></p>
<div class="paragraph">
<p>UTS (UNIX Time-Sharing) namespaces allow a single system to appear to have different host and domain names to different processes.</p>
</div>
</li>
<li>
<p><strong>IPC</strong></p>
<div class="paragraph">
<p>System V IPC, POSIX message queues</p>
</div>
</li>
<li>
<p><strong>PID</strong></p>
<div class="paragraph">
<p>Process IDs</p>
</div>
</li>
<li>
<p><strong>Network</strong></p>
<div class="paragraph">
<p>Network namespaces virtualize the <a href="https://en.wikipedia.org/wiki/Network_stack">network stack</a>. On creation, a network namespace contains only a <a href="https://en.wikipedia.org/wiki/Localhost">loopback</a> interface.</p>
</div>
<div class="paragraph">
<p>Each network interface (physical or virtual) is present in exactly 1 namespace and can be moved between namespaces.</p>
</div>
<div class="paragraph">
<p>Each namespace will have a private set of <em>IP addresses</em>, its own <em>routing table</em>, <em>socket listing</em>, <em>connection tracking table</em>, <em>firewall</em>, and other network-related resources.</p>
</div>
<div class="paragraph">
<p>Destroying a network namespace destroys any virtual interfaces within it and moves any physical interfaces within it back to the initial network namespace.</p>
</div>
</li>
<li>
<p><strong>User</strong></p>
<div class="paragraph">
<p>User and group IDs</p>
</div>
</li>
<li>
<p><strong>Control Groups</strong></p>
<div class="paragraph">
<p>Isolates cgroups</p>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="what-are-resource-controllers">1.1. What are Resource Controllers</h3>
<div class="paragraph">
<p>A <strong>resource controller</strong>, also called a <strong>cgroup subsystem</strong>, represents a single resource, such as CPU time or memory. The Linux kernel provides a range of resource controllers, that are mounted automatically by <strong>systemd</strong>. <a href="#rhel7-resource_management_guide-index">[3]</a></p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Find the list of currently mounted resource controllers in <code>/proc/cgroups</code>.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>blkio</strong> — sets limits on input/output access to and from block devices;</p>
</li>
<li>
<p><strong>cpu</strong> — uses the CPU scheduler to provide cgroup tasks access to the CPU. It is mounted together with the <code>cpuacct</code> controller on the same mount;</p>
</li>
<li>
<p><strong>cpuacct</strong> — creates automatic reports on CPU resources used by tasks in a cgroup. It is mounted together with the <code>cpu</code> controller on the same mount;</p>
</li>
<li>
<p><strong>cpuset</strong> — assigns individual CPUs (on a multicore system) and memory nodes to tasks in a cgroup;</p>
</li>
<li>
<p><strong>devices</strong> — allows or denies access to devices for tasks in a cgroup;</p>
</li>
<li>
<p><strong>freezer</strong> — suspends or resumes tasks in a cgroup;</p>
</li>
<li>
<p><strong>memory</strong> — sets limits on memory use by tasks in a cgroup and generates automatic reports on memory resources used by those tasks;</p>
</li>
<li>
<p><strong>net_cls</strong> — tags network packets with a class identifier (<strong>classid</strong>) that allows the Linux traffic controller (the <code>tc</code> command) to identify packets originating from a particular cgroup task. A subsystem of <code>net_cls</code>, the <code>net_filter</code> (<code>iptables</code>) can also use this tag to perform actions on such packets. The <code>net_filter</code> tags network sockets with a firewall identifier (<strong>fwid</strong>) that allows the Linux firewall (the <code>iptables</code> command) to identify packets (skb&#8594;sk) originating from a particular cgroup task;</p>
</li>
<li>
<p><strong>perf_event</strong> — enables monitoring cgroups with the <strong>perf</strong> tool;</p>
</li>
<li>
<p><strong>hugetlb</strong> — allows to use virtual memory pages of large sizes and to enforce resource limits on these pages.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="how-control-groups-are-organized">1.2. How Control Groups are Organized</h3>
<div class="paragraph">
<p>Cgroups are organized hierarchically, like processes, and child cgroups inherit some of the attributes of their parents. However, there are differences between the two models. <a href="#rhel6-resource_management_guide-index">[5]</a></p>
</div>
<div class="sect3">
<h4 id="the-linux-process-model">1.2.1. The Linux Process Model</h4>
<div class="paragraph">
<p>All processes on a Linux system are child processes of a common parent: the <code>init</code> process, which is executed by the kernel at boot time and starts other processes (which may in turn start child processes of their own). Because all processes descend from a single parent, <em>the Linux process model is a single hierarchy, or tree</em>.</p>
</div>
<div class="paragraph">
<p>Additionally, every Linux process except init inherits the environment (such as the PATH variable) and certain other attributes (such as open file descriptors) of its parent process.</p>
</div>
</div>
<div class="sect3">
<h4 id="the-cgroup-model">1.2.2. The Cgroup Model</h4>
<div class="paragraph">
<p>Cgroups are similar to processes in that: they are hierarchical, and child cgroups inherit certain attributes from their parent cgroup.</p>
</div>
<div class="paragraph">
<p>The fundamental difference is that many different hierarchies of cgroups can exist simultaneously on a system. If the Linux process model is a single tree of processes, then <em>the cgroup model is one or more separate, unconnected trees of tasks (i.e. processes)</em>.</p>
</div>
<div class="paragraph">
<p>Multiple separate hierarchies of cgroups are necessary because each hierarchy is attached to one or more subsystems.</p>
</div>
<div class="paragraph">
<p><em>Remember that system <strong>processes</strong> are called <strong>tasks</strong> in <strong>cgroup</strong> terminology.</em></p>
</div>
<div class="paragraph">
<p>Here are a few simple rules governing the relationships between subsystems, hierarchies of cgroups, and tasks, along with explanations of the consequences of those rules.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Rule 1</strong></p>
<div class="paragraph">
<p>A single hierarchy can have one or more subsystems attached to it.</p>
</div>
<div class="paragraph">
<p><em>As a consequence, the <code>cpu</code> and <code>memory</code> subsystems (or any number of subsystems) can be attached to a single hierarchy, as long as each one is not attached to any other hierarchy which has any other subsystems attached to it already (see Rule 2).</em></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux-6-Resource_Management_Guide-en-US/images/fe94409bf79906ecb380e8fbd8063016/RMG-rule1.png" alt="RMG rule1" width="45%" height="45%">
</div>
</div>
</li>
<li>
<p><strong>Rule 2</strong></p>
<div class="paragraph">
<p>Any single subsystem (such as <code>cpu</code>) cannot be attached to more than one hierarchy if one of those hierarchies has a different subsystem attached to it already.</p>
</div>
<div class="paragraph">
<p><em>As a consequence, the <code>cpu</code> subsystem can never be attached to two different hierarchies if one of those hierarchies already has the <code>memory</code> subsystem attached to it. However, a single subsystem can be attached to two hierarchies if both of those hierarchies have only that subsystem attached.</em></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux-6-Resource_Management_Guide-en-US/images/c4b0445881422c88d957e352911bccd8/RMG-rule2.png" alt="RMG rule2" width="45%" height="45%">
</div>
</div>
</li>
<li>
<p><strong>Rule 3</strong></p>
<div class="paragraph">
<p>Each time a new hierarchy is created on the systems, all tasks on the system are initially members of the default cgroup of that hierarchy, which is known as the <strong>root cgroup</strong>. For any single hierarchy you create, each task on the system can be a member of exactly one cgroup in that hierarchy.</p>
</div>
<div class="paragraph">
<p>A single task may be in multiple cgroups, as long as each of those cgroups is in a different hierarchy.</p>
</div>
<div class="paragraph">
<p>As soon as a task becomes a member of a second cgroup in the same hierarchy, it is removed from the first cgroup in that hierarchy. At no time is a task ever in two different cgroups in the same hierarchy.</p>
</div>
<div class="paragraph">
<p><em>As a consequence, if the <code>cpu</code> and <code>memory</code> subsystems are attached to a hierarchy named <code>cpu_mem_cg</code>, and the <code>net_cls</code> subsystem is attached to a hierarchy named <code>net</code>, then a running <code>httpd</code> process could be a member of any one cgroup in <code>cpu_mem_cg</code>, and any one cgroup in <code>net</code>.</em></p>
</div>
<div class="paragraph">
<p>The cgroup in <code>cpu_mem_cg</code> that the <code>httpd</code> process is a member of might restrict its CPU time to half of that allotted to other processes, and limit its memory usage to a maximum of <code>1024</code> MB. Additionally, the cgroup in <code>net</code> that the <code>httpd</code> process is a member of might limit its transmission rate to <code>30</code> MB/s (megabytes per second).</p>
</div>
<div class="paragraph">
<p>When the first hierarchy is created, every task on the system is a member of at least one cgroup: the root cgroup. When using cgroups, therefore, every system task is always in at least one cgroup.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux-6-Resource_Management_Guide-en-US/images/fb48098033d1c4ccdb5a55516c9cb816/RMG-rule3.png" alt="RMG rule3" width="45%" height="45%">
</div>
</div>
</li>
<li>
<p><strong>Rule 4</strong></p>
<div class="paragraph">
<p>Any process (task) on the system which forks itself creates a child task. A child task automatically inherits the cgroup membership of its parent but can be moved to different cgroups as needed. Once forked, the parent and child processes are completely independent.</p>
</div>
<div class="paragraph">
<p><em>As a consequence, consider the <code>httpd</code> task that is a member of the cgroup named <code>half_cpu_1gb_max</code> in the <code>cpu_and_mem</code> hierarchy, and a member of the cgroup <code>trans_rate_30</code> in the <code>net</code> hierarchy. When that <code>httpd</code> process forks itself, its child process automatically becomes a member of the <code>half_cpu_1gb_max</code> cgroup, and the <code>trans_rate_30</code> cgroup. It inherits the exact same cgroups its parent task belongs to.</em></p>
</div>
<div class="paragraph">
<p><em>From that point forward, the parent and child tasks are completely independent of each other: changing the cgroups that one task belongs to does not affect the other. Neither will changing cgroups of a parent task affect any of its grandchildren in any way. To summarize: any child task always initially inherits memberships to the exact same cgroups as their parent task, but those memberships can be changed or removed later.</em></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux-6-Resource_Management_Guide-en-US/images/67e2c07808671294692acde9baf0b452/RMG-rule4.png" alt="RMG rule4" width="45%" height="45%">
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="systemd">1.3. Systemd</h3>
<div class="paragraph">
<p><strong>Systemd</strong> is a system and service manager for Linux operating systems. It is designed to be backwards compatible with SysV init scripts, and provides a number of features such as parallel startup of system services at boot time, on-demand activation of daemons, or dependency-based service control logic. <a href="#rhel7-systemd">[4]</a></p>
</div>
<div class="paragraph">
<p>Systemd introduces the concept of <em>systemd units</em>, represented by unit configuration files.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Available systemd Unit Types</caption>
<colgroup>
<col style="width: 18.1818%;">
<col style="width: 18.1818%;">
<col style="width: 63.6364%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Unit Type</th>
<th class="tableblock halign-left valign-top">File Extension</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Service unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.service</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A system service.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Target unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.target</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A group of systemd units.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Automount unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.automount</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A file system automount point.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Device unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.device</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A device file recognized by the kernel.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mount unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.mount</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A file system mount point.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Path unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.path</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A file or directory in a file system.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Scope unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.scope</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">An externally created process.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Slice unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.slice</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A group of hierarchically organized units that manage system processes.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Snapshot unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.snapshot</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A saved state of the systemd manager.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Socket unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.socket</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">An inter-process communication socket.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Swap unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.swap</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A swap device or a swap file.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Timer unit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>.timer</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A systemd timer.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Systemd Unit Files Locations</caption>
<colgroup>
<col style="width: 22.2222%;">
<col style="width: 77.7778%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Directory</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>/usr/lib/systemd/system/</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Systemd unit files distributed with installed RPM packages.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>/run/systemd/system/</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Systemd unit files created at run time. This directory takes precedence over the directory with installed service unit files.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>/etc/systemd/system/</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Systemd unit files created by systemctl enable as well as unit files added for extending a service. This directory takes precedence over the directory with runtime unit files.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>By default, systemd automatically creates a hierarchy of <code>slice</code>, <code>scope</code> and <code>service</code> units to provide a unified structure for the cgroup tree. Also, systemd automatically mounts hierarchies for important kernel resource controllers in the <code>/sys/fs/cgroup/</code> directory. <a href="#rhel7-resource_management_guide-index">[3]</a></p>
</div>
<div class="paragraph">
<p>All processes running on the system are child processes of the <em>systemd init process</em>. Systemd provides three unit types that are used for the purpose of resource control:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Service</strong> — A process or a group of processes, which systemd started based on a unit configuration file. Services encapsulate the specified processes so that they can be started and stopped as one set.</p>
</li>
<li>
<p><strong>Scope</strong> — A group of externally created processes. Scopes encapsulate processes that are started and stopped by arbitrary processes through the <code>fork()</code> function and then registered by systemd at runtime. For instance, user sessions, containers, and virtual machines are treated as scopes.</p>
</li>
<li>
<p><strong>Slice</strong> — A group of hierarchically organized units. Slices do not contain processes, they organize a hierarchy in which scopes and services are placed. The actual processes are contained in scopes or in services. In this hierarchical tree, every name of a slice unit corresponds to the path to a location in the hierarchy. The dash (&#8220;-&#8221;) character acts as a separator of the path components.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Services, scopes, and slices are created manually by the system administrator or dynamically by programs. By default, the operating system defines a number of built-in services that are necessary to run the system.</p>
</div>
<div class="paragraph">
<p>Use the <code>systemctl</code> command to list system units and to view their status. Also, the <code>systemd-cgls</code> command is provided to view the hierarchy of control groups and <code>systemd-cgtop</code> to monitor their resource consumption in real time.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use the following command to list all active units on the system:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">systemctl list-units</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>list-units</code> option is executed by default, which means that you will receive the same output when you omit this option.</p>
</div>
</li>
<li>
<p>To list all unit files installed on your system and their status, type:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">systemctl list-unit-files</code></pre>
</div>
</div>
</li>
<li>
<p>To view a list of all slices used on the system, type:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">systemctl <span class="nt">-t</span> slice</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">  UNIT                  LOAD   ACTIVE SUB    DESCRIPTION
  -.slice               loaded active active Root Slice
  system-getty.slice    loaded active active system-getty.slice
  system-modprobe.slice loaded active active system-modprobe.slice
  system.slice          loaded active active System Slice
  user-1000.slice       loaded active active User Slice of UID 1000
  user.slice            loaded active active User and Session Slice

LOAD   = Reflects whether the unit definition was properly loaded.
ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
SUB    = The low-level unit activation state, values depend on unit type.
6 loaded units listed. Pass --all to see loaded but inactive units, too.
To show all installed unit files use 'systemctl list-unit-files'.</span></code></pre>
</div>
</div>
</li>
<li>
<p>To display detailed information about a service unit that corresponds to a system service, type:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">systemctl status ssh.service</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">● ssh.service - OpenBSD Secure Shell server</span>
<span class="gp">     Loaded: loaded (/lib/systemd/system/ssh.service;</span><span class="w"> </span>enabled<span class="p">;</span> vendor preset: enabled<span class="o">)</span>
<span class="gp">     Active: active (running) since Tue 2021-11-23 15:07:53 CST;</span><span class="w"> </span>49min ago
<span class="go">       Docs: man:sshd(8)</span>
<span class="go">             man:sshd_config(5)</span>
<span class="go">    Process: 350 ExecStartPre=/usr/sbin/sshd -t (code=exited, status=0/SUCCESS)</span>
<span class="go">   Main PID: 367 (sshd)</span>
<span class="go">      Tasks: 1 (limit: 4641)</span>
<span class="go">     Memory: 8.0M</span>
<span class="go">        CPU: 265ms</span>
<span class="hll"><span class="go">     CGroup: /system.slice/ssh.service</span>
</span><span class="go">             └─367 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups</span></code></pre>
</div>
</div>
</li>
<li>
<p>To display the whole cgroup hierarchy on your system, type:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">systemd-cgls</code></pre>
</div>
</div>
<div class="paragraph">
<p>When <code>systemd-cgls</code> is issued without parameters, it returns the entire cgroup hierarchy.</p>
</div>
<div class="paragraph">
<p>To view the information that stored in dedicated process files, type as root:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="nb">cat </span>proc/PID/cgroup</code></pre>
</div>
</div>
<div class="paragraph">
<p>Where <code>PID</code> stands for the ID of the process you wish to examine.</p>
</div>
<div class="paragraph">
<p>The <strong>systemd-cgls</strong> command provides a static snapshot of the cgroup hierarchy. To see a dynamic account of currently running cgroups ordered by their resource usage (CPU, Memory, and IO), use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">systemd-cgtop</code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="libcgroup">1.4. libcgroup</h3>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
The libcgroup package, which was the main tool for cgroup management in previous versions of Red Hat Enterprise Linux, is now deprecated.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In order to use <a href="https://github.com/libcgroup/libcgroup">libcgroup</a> tools, first ensure the cgroup-tools packages are installed on your system.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="nb">sudo </span>apt-get <span class="nb">install </span>cgroup-tools <span class="nt">-y</span>
<span class="c"># sudo yum install libcgroup-tools -y</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>cgroup-tools</code> with version <code>0.41-21.el7</code> does not work with Cgroup v2.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>yum <span class="nt">--showduplicates</span> list libcgroup-tools
<span class="go">libcgroup-tools.x86_64                                             0.41-21.el7                                              @base</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The cgroup version depends on the Linux distribution being used and the default cgroup version configured on the OS. To check which cgroup version your distribution uses, run the <code>stat -fc %T /sys/fs/cgroup/</code> command on the node: <a href="#k8s-arch-cgroups">[6]</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="nb">stat</span> <span class="nt">-fc</span> %T /sys/fs/cgroup/
<span class="c"># For cgroup v2, the output is cgroup2fs.</span>
<span class="c"># For cgroup v1, the output is tmpfs.</span></code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Finding a Process</p>
<div class="paragraph">
<p>To find the cgroup to which a process belongs, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">ps <span class="nt">-O</span> cgroup</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>ps <span class="nt">-o</span> pid,cgroup:60,command
<span class="go">   PID CGROUP                                                       COMMAND
  5345 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev -bash
  5441 10:memory:/user.slice/user-1001.slice/session-21.scope,9:dev ps -o pid,cgroup:60,command</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Or, if you know the PID for the process, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="nb">cat</span> /proc/PID/cgroup</code></pre>
</div>
</div>
<div class="paragraph">
<p>where <code>PID</code> stands for a ID of the inspected process.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">cat</span> /proc/5345/cgroup
<span class="go">11:cpuset:/
10:memory:/user.slice/user-1001.slice/session-21.scope
9:devices:/user.slice
8:perf_event:/
7:blkio:/user.slice
6:rdma:/
5:pids:/user.slice/user-1001.slice/session-21.scope
4:freezer:/
3:net_cls,net_prio:/
2:cpu,cpuacct:/user.slice
1:name=systemd:/user.slice/user-1001.slice/session-21.scope
0::/user.slice/user-1001.slice/session-21.scope</span></code></pre>
</div>
</div>
</li>
<li>
<p>Listing Controllers</p>
<div class="paragraph">
<p>To find the controllers that are available in your kernel and information on how they are mounted together to hierarchies, execute:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="nb">cat</span> /proc/cgroups</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">#</span>subsys_name	hierarchy	num_cgroups	enabled
<span class="go">cpuset	4	68	1
cpu	3	277	1
cpuacct	3	277	1
memory	6	277	1
devices	8	277	1
freezer	11	68	1
net_cls	10	68	1
blkio	5	277	1
perf_event	2	68	1
hugetlb	7	68	1
pids	9	277	1
net_prio	10	68	1</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Alternatively, to find the mount points of particular subsystems, execute the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh"><span class="nv">$ </span>lssubsys <span class="nt">-m</span> <span class="o">[</span>controllers]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here <code>controllers</code> stands for a list of the subsystems seperated with space in which you are interested.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>lssubsys <span class="nt">-m</span>
<span class="go">cpuset /sys/fs/cgroup/cpuset
cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct
memory /sys/fs/cgroup/memory
devices /sys/fs/cgroup/devices
freezer /sys/fs/cgroup/freezer
net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio
blkio /sys/fs/cgroup/blkio
perf_event /sys/fs/cgroup/perf_event
hugetlb /sys/fs/cgroup/hugetlb
pids /sys/fs/cgroup/pids

</span><span class="gp">$</span><span class="w"> </span>lssubsys <span class="nt">-m</span> cpu memory
<span class="go">cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct
memory /sys/fs/cgroup/memory</span></code></pre>
</div>
</div>
</li>
<li>
<p>Finding Hierarchies</p>
<div class="paragraph">
<p>It is recommended that you mount hierarchies under the <code>/sys/fs/cgroup/</code> directory. Assuming this is the case on your system, list or browse the contents of that directory to obtain a list of hierarchies. If the tree utility is installed on your system, run it to obtain an overview of all hierarchies and the cgroups within them:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">tree /sys/fs/cgroup</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>tree <span class="nt">-L</span> 1 /sys/fs/cgroup/
<span class="go">/sys/fs/cgroup/
├── blkio
</span><span class="gp">├── cpu -&gt;</span><span class="w"> </span>cpu,cpuacct
<span class="gp">├── cpuacct -&gt;</span><span class="w"> </span>cpu,cpuacct
<span class="go">├── cpu,cpuacct
├── cpuset
├── devices
├── freezer
├── hugetlb
├── memory
</span><span class="gp">├── net_cls -&gt;</span><span class="w"> </span>net_cls,net_prio
<span class="go">├── net_cls,net_prio
</span><span class="gp">├── net_prio -&gt;</span><span class="w"> </span>net_cls,net_prio
<span class="go">├── perf_event
├── pids
└── systemd</span></code></pre>
</div>
</div>
</li>
<li>
<p>Finding Control Groups</p>
<div class="paragraph">
<p>To list the cgroups on a system, execute as root:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">lscgroup</code></pre>
</div>
</div>
<div class="paragraph">
<p>To restrict the output to a specific hierarchy, specify a controller and a path in the format <code>controller:path</code>. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">lscgroup cpuset:adminusers</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above command lists only subgroups of the <code>adminusers</code> cgroup in the hierarchy to which the <code>cpuset</code> controller is attached.</p>
</div>
</li>
<li>
<p>Displaying Parameters of Control Groups</p>
<div class="paragraph">
<p>To display the parameters of specific cgroups, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">cgget <span class="nt">-r</span> parameter list_of_cgroups</code></pre>
</div>
</div>
<div class="paragraph">
<p>where <code>parameter</code> is a pseudofile that contains values for a controller, and <code>list_of_cgroups</code> is a list of cgroups separated with spaces. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">cgget <span class="nt">-r</span> cpuset.cpus <span class="nt">-r</span> memory.limit_in_bytes group1 group2</code></pre>
</div>
</div>
<div class="paragraph">
<p>displays the values of <code>cpuset.cpus</code> and <code>memory.limit_in_bytes</code> for cgroups <code>group1</code> and <code>group2</code>.</p>
</div>
<div class="paragraph">
<p>If you do not know the names of the parameters themselves, use a command like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sh">cgget <span class="nt">-g</span> cpuset /</code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="nsenter">1.5. nsenter</h3>
<div class="paragraph">
<p>The kernel allocates and restricts the resources for individual processes running on the Linux operating system. The namespaces within the kernel partition these resources. Namespaces allocate the resources to a process such that the process only sees those specific resources, widely-used in container runtimes to provide a layer of isolation among containers that run on the same host. <a href="#rl-container-namespaces-nsenter">[7]</a></p>
</div>
<div class="paragraph">
<p>Use Docker to create a container from the <code>debian:bullseye</code> image and install <code>procps</code> package inside the container, which provides <code>top</code> and <code>ps</code> commands.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="nt">--name</span> namespace-demo <span class="nt">-it</span> debian:bullseye /bin/bash
<span class="gp">root@5153317a7aa2:/#</span><span class="w"> </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install </span>procps  <span class="nt">-y</span>
<span class="c">...
</span><span class="gp">root@5153317a7aa2:/#</span><span class="w"> </span>ps
<span class="go">    PID TTY          TIME CMD
      1 pts/0    00:00:00 bash
    481 pts/0    00:00:00 ps</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>In another terminal, use the <code>docker inspect</code> command to determine the process id associated with the new container.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>docker inspect namespace-demo <span class="nt">-f</span> <span class="s2">"{{.State.Pid}}"</span>
<span class="go">415111</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The process id is <strong>415111</strong>.</p>
</div>
<div class="paragraph">
<p>Each process has a <strong>/proc/[pid]/ns/</strong> subdirectory containing one entry for each namespace that supports being manipulated by <a href="https://man7.org/linux/man-pages/man2/setns.2.html">setns</a>.</p>
</div>
<div class="paragraph">
<p>Use the <code>ls</code> or <code>lsns</code> command to list the namespaces associated with a given process.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo ls</span> <span class="nt">-l</span> /proc/415111/ns/
<span class="go">total 0
</span><span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 cgroup -&gt;</span><span class="w"> </span><span class="s1">'cgroup:[4026533307]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 ipc -&gt;</span><span class="w"> </span><span class="s1">'ipc:[4026533238]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 mnt -&gt;</span><span class="w"> </span><span class="s1">'mnt:[4026533236]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 net -&gt;</span><span class="w"> </span><span class="s1">'net:[4026533241]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid -&gt;</span><span class="w"> </span><span class="s1">'pid:[4026533239]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 pid_for_children -&gt;</span><span class="w"> </span><span class="s1">'pid:[4026533239]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 time -&gt;</span><span class="w"> </span><span class="s1">'time:[4026531834]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 time_for_children -&gt;</span><span class="w"> </span><span class="s1">'time:[4026531834]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 user -&gt;</span><span class="w"> </span><span class="s1">'user:[4026531837]'</span>
<span class="gp">lrwxrwxrwx 1 root root 0 Nov 24 12:34 uts -&gt;</span><span class="w"> </span><span class="s1">'uts:[4026533237]'</span>
<span class="go">
</span><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>lsns <span class="nt">-p</span> 415111
<span class="go">        NS TYPE   NPROCS    PID USER COMMAND
4026531834 time      208      1 root /sbin/init
4026531837 user      208      1 root /sbin/init
4026533236 mnt         1 415111 root /bin/bash
4026533237 uts         1 415111 root /bin/bash
4026533238 ipc         1 415111 root /bin/bash
4026533239 pid         1 415111 root /bin/bash
4026533241 net         1 415111 root /bin/bash
4026533307 cgroup      1 415111 root /bin/bash</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>nsenter</code> command expands to <em>namespace enter</em>. It accepts different options to only enter the specified namespace.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s enter the network namespace (<code>-n</code>) to check the IP address and route table.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-n</span> ip a s
<span class="go">nsenter: cannot open /proc/415111/ns/net: Permission denied

</span><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-n</span> ip a s
<span class="gp">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt;</span><span class="w"> </span>mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
<span class="go">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
</span><span class="gp">19: eth0@if20: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt;</span><span class="w"> </span>mtu 1500 qdisc noqueue state UP group default
<span class="go">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Here, <code>-t</code> is the target process id, and <code>-n</code> refers to the network namespace.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-n</span> ip route
<span class="go">default via 172.17.0.1 dev eth0
172.17.0.0/16 dev eth0 proto kernel scope link src 172.17.0.2</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, enter the process namespace to check the process details.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-p</span> <span class="nt">-r</span> ps <span class="nt">-ef</span>
<span class="go">UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 04:00 pts/0    00:00:00 /bin/bash
root         489       0  0 04:44 ?        00:00:00 ps -ef</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>-r</code> option sets the root directory to the top-level directory within the namespace so that the commands run in the context of the namespace.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-p</span> <span class="nt">-r</span> top
<span class="go">Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s):  7.1 us, 14.3 sy,  0.0 ni, 78.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :   3900.2 total,    129.4 free,   2200.9 used,   1569.8 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1459.5 avail Mem

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
      1 root      20   0    4092   3492   2984 S   0.0   0.1   0:00.06 bash
    490 root      20   0    6936   3192   2708 R   0.0   0.1   0:00.01 top</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>bash</code> command, which executes during <code>docker run</code>, is the first process inside the namespace.</p>
</div>
<div class="paragraph">
<p>Enter the UTC namespace to check the hostname.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-u</span> <span class="nb">hostname</span>
<span class="go">5153317a7aa2</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Modify the hostname within the namespace and verify the new name.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-u</span> <span class="nb">hostname </span>foo.bar.buzz
<span class="go">
</span><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-u</span> <span class="nb">hostname</span>
<span class="go">foo.bar.buzz</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, enter all namespaces by using the <code>-a</code> option.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 415111 <span class="nt">-a</span> lsns
<span class="go">        NS TYPE   NPROCS PID USER COMMAND
4026531834 time        2   1 root /bin/bash
4026531837 user        2   1 root /bin/bash
4026533236 mnt         2   1 root /bin/bash
4026533237 uts         2   1 root /bin/bash
4026533238 ipc         2   1 root /bin/bash
4026533239 pid         2   1 root /bin/bash
4026533241 net         2   1 root /bin/bash
4026533307 cgroup      2   1 root /bin/bash</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="free">1.6. free</h3>
<div class="paragraph">
<p><code>free</code> is a popular command used by system administrators on Unix/Linux platforms. It&#8217;s a powerful tool that gives insight into the memory usage in human-readable format.</p>
</div>
<div class="paragraph">
<p>The <code>man</code> page for this command states that <code>free</code> displays the total amount of free and used memory on the system, including physical and swap space, as well as the buffers and caches used by the kernel. The information is gathered by parsing <strong><code>/proc/meminfo</code></strong>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>docker version <span class="nt">-f</span> <span class="s1">'{{.Server.Version}}'</span>
<span class="go">24.0.7</span>

<span class="gp">$</span><span class="w"> </span>docker info | <span class="nb">grep </span>Cgroup
<span class="go"> Cgroup Driver: systemd</span>
<span class="hll"><span class="go"> Cgroup Version: 2</span>
</span>
<span class="gp">$</span><span class="w"> </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> busybox free
<span class="go">              total        used        free      shared  buff/cache   available</span>
<span class="go">Mem:        3993764     2410712      190544        2224     1392508     1344364</span>
<span class="go">Swap:             0           0           0</span>

<span class="gp">$</span><span class="w"> </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="nt">--memory</span> 100m busybox free
<span class="go">              total        used        free      shared  buff/cache   available</span>
<span class="go">Mem:        3993764     2421988      178952        2180     1392824     1333032</span>
<span class="go">Swap:             0           0           0</span>

<span class="gp">$</span><span class="w"> </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="nt">--memory</span> 100m busybox sh <span class="nt">-c</span> <span class="s1">'echo $(($(cat /sys/fs/cgroup/memory.max) / 1024 / 1024))'</span>
<span class="go">100</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span>docker version <span class="nt">-f</span> <span class="s1">'{{.Server.Version}}'</span>
<span class="go">24.0.7</span>

<span class="gp">$</span><span class="w"> </span>docker info | <span class="nb">grep </span>Cgroup
<span class="hll"><span class="go"> Cgroup Driver: cgroupfs</span>
</span><span class="go"> Cgroup Version: 1</span>

<span class="gp">$</span><span class="w"> </span>docker run <span class="nt">--rm</span> <span class="nt">-d</span> <span class="nt">--memory</span> 100m busybox <span class="nb">sleep </span>10m
<span class="go">6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>

<span class="gp">$</span><span class="w"> </span>lscgroup | <span class="nb">grep </span>6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540
<span class="go">pids:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">devices:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">cpu,cpuacct:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">memory:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">blkio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">cpuset:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">freezer:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">net_cls,net_prio:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">hugetlb:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>
<span class="go">perf_event:/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540</span>

<span class="gp">$</span><span class="w"> </span>cgget <span class="nt">-r</span> memory.limit_in_bytes /docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540
<span class="go">/docker/6e5f69c591d46a3701e383ce3469eb3b63f3dfb2608012177fe646b8469a3540:</span>
<span class="go">memory.limit_in_bytes: 104857600</span>

<span class="gp">$</span><span class="w"> </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="nt">--memory</span> 100m busybox <span class="nb">cat</span> /sys/fs/cgroup/memory/memory.limit_in_bytes
<span class="go">104857600</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl ps
<span class="go">CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID
7fe801499210a       1ec24650902b1       2 hours ago         Running             kube-swagger-ui     0                   bee47f1dec5bc

</span><span class="gp">$</span><span class="w"> </span>lscgroup | <span class="nb">grep </span>7fe801499210a
<span class="go">cpu,cpuacct:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
net_cls,net_prio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
freezer:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
pids:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
blkio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
perf_event:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
devices:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
memory:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope
cpuset:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod26d20484_ab3f_4f87_92a4_7842aab7170f.slice/docker-7fe801499210afdc6490ee78bff40f9017a843814f4867b39ba6976c626a80a6.scope</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="what-are-containers">2. What are Containers?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Containers are lightweight packages of your application code together with dependencies such as specific versions of programming language runtimes and libraries required to run your software services. <a href="#gcp-what-are-containers">[8]</a><a href="#rl-whats-a-linux-container">[9]</a></p>
</div>
<div class="paragraph">
<p>Containers make it easy to share CPU, memory, storage, and network resources at the operating systems level and offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run.</p>
</div>
<div class="sect2">
<h3 id="containers-vs-vms">2.1. Containers vs. VMs</h3>
<div class="paragraph">
<p>You might already be familiar with VMs: a guest operating system such as Linux or Windows runs on top of a host operating system with access to the underlying hardware. Containers are often compared to virtual machines (VMs). Like virtual machines, containers allow you to package your application together with libraries and other dependencies, providing isolated environments for running your software services.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Containers are much more lightweight than VMs</p>
</li>
<li>
<p>Containers virtualize at the OS level while VMs virtualize at the hardware level</p>
</li>
<li>
<p>Containers share the OS kernel and use a fraction of the memory VMs require</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="https://www.redhat.com/cms/managed-files/virtualization-vs-containers.png" alt="virtualization vs containers" width="55%" height="55%">
</div>
</div>
</div>
<div class="sect2">
<h3 id="open-container-initiative">2.2. Open Container Initiative</h3>
<div class="paragraph">
<p>The <a href="https://opencontainers.org/">Open Container Initiative</a> (OCI) is an open governance structure for the express purpose of creating open industry standards around container formats and runtimes.</p>
</div>
<div class="paragraph">
<p>Established in June 2015 by Docker and other leaders in the container industry, the OCI currently contains three specifications: the <em>Runtime Specification</em> (runtime-spec), the <em>Image Specification</em> (image-spec) and the <em>Distribution Specification</em> (distribution-spec).</p>
</div>
<div class="paragraph">
<p>Docker is donating its container format and runtime, runC, to the OCI to serve as the cornerstone of this new effort. <a href="#oci-about">[10]</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p>runc</p>
<div class="paragraph">
<p><a href="https://github.com/opencontainers/runc"><code>runc</code></a> is a CLI tool for spawning and running containers on Linux according to the OCI specification.</p>
</div>
</li>
<li>
<p>containerd</p>
<div class="paragraph">
<p><a href="https://github.com/containerd/containerd">containerd</a> is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.</p>
</div>
<div class="paragraph">
<p>Containerd is built on top of the Open Container Initiative’s runC and specification. Containerd is a daemon providing a GRPC API to manage containers on the local system. Containerd leverages runC to provide advanced functionality like checkpoint and restore, seccomp, and user namespace support which will open the door for these features into Docker. <a href="#containerd-daemon-to-control-runc">[11]</a></p>
</div>
<div class="paragraph">
<p>Containerd is designed to be embedded into a larger system, rather than being used directly by developers or end-users.</p>
</div>
<div class="paragraph">
<p>Containerd was designed to be used by Docker and Kubernetes as well as any other container platform that wants to abstract away syscalls or OS specific functionality to run containers on linux, windows, solaris, or other OSes. <a href="#docker-what-is-containerd-runtime">[12]</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://containerd.io/img/architecture.png" alt="containerd architecture diagram" width="55%" height="55%">
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="what-is-docker">2.3. What is Docker?</h3>
<div class="paragraph">
<p>The word "Docker" refers to several things, including an open source community project; tools from the open source project; Docker Inc., the company that primarily supports that project; and the tools that company formally supports. <a href="#rl-what-is-docker">[13]</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://upload.wikimedia.org/wikipedia/commons/0/0a/Docker-containerized-and-vm-transparent-bg.png" alt="Docker containerized and vm transparent bg" width="55%" height="55%">
</div>
</div>
<div class="paragraph">
<p>With Docker, you can treat containers like extremely lightweight, modular virtual machines. <a href="#docker-what-container">[14]</a></p>
</div>
<div class="paragraph">
<p>The Docker technology uses the <a href="https://www.redhat.com/en/topics/linux/what-is-the-linux-kernel">Linux kernel</a> and features of the kernel, like <a href="https://www.redhat.com/en/blog/world-domination-cgroups-rhel-8-welcome-cgroups-v2">Cgroups</a> and <a href="https://lwn.net/Articles/528078/">namespaces</a>, to segregate processes so they can run independently. This independence is the intention of containers—the ability to run multiple processes and apps separately from one another to make better use of your infrastructure while retaining the security you would have with separate systems.</p>
</div>
</div>
<div class="sect2">
<h3 id="what-is-kubernetes">2.4. What is Kubernetes?</h3>
<div class="paragraph">
<p>Kubernetes (also known as k8s or “kube”) is an open source container orchestration platform that automates many of the manual processes involved in deploying, managing, and scaling containerized applications. <a href="#rl-what-is-kubernetes">[15]</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://www.redhat.com/cms/managed-files/kubernetes_diagram-v3-770x717_0_0_v2.svg?" alt="kubernetes diagram v3 770x717 0 0 v2" width="45%" height="45%">
</div>
</div>
<div class="paragraph">
<p>With Kubernetes you can:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Orchestrate containers across multiple hosts.</p>
</li>
<li>
<p>Make better use of hardware to maximize resources needed to run your enterprise apps.</p>
</li>
<li>
<p>Control and automate application deployments and updates.</p>
</li>
<li>
<p>Mount and add storage to run stateful apps.</p>
</li>
<li>
<p>Scale containerized applications and their resources on the fly.</p>
</li>
<li>
<p>Declaratively manage services, which guarantees the deployed applications are always running the way you intended them to run.</p>
</li>
<li>
<p>Health-check and self-heal your apps with autoplacement, autorestart, autoreplication, and autoscaling.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="container-runtime-interface-cri">2.4.1. Container Runtime Interface (CRI)</h4>
<div class="paragraph">
<p>At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. <a href="#k8s-cri-in-kubernetes">[16]</a></p>
</div>
<div class="paragraph">
<p>CRI (<em>Container Runtime Interface</em>) consists of a specifications/requirements (to-be-added), protobuf API, and libraries for container runtimes to integrate with <em>kubelet</em> on a node.</p>
</div>
<div class="paragraph">
<p>Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cl.ly/3I2p0D1V0T26/Image%202016-12-19%20at%2017.13.16.png" alt="Image%202016 12 19%20at%2017.13.16" width="55%" height="55%">
</div>
</div>
<div class="paragraph">
<p>Kubernetes supports several container runtimes: <a href="https://docs.docker.com/engine/">Docker</a>, <a href="https://containerd.io/docs/">containerd</a>, <a href="https://cri-o.io/#what-is-cri-o">CRI-O</a>, and any implementation of the Kubernetes CRI (Container Runtime Interface).</p>
</div>
</div>
<div class="sect3">
<h4 id="what-is-dockershim">2.4.2. What is dockershim</h4>
<div class="paragraph">
<p>Just as Kubernetes started out with built-in support for Docker Engine, it also included built-in support for various storage volume solutions, network solutions, and even cloud providers. But maintaining these things on an ongoing basis became too cumbersome, so the community decided to strip all third party solutions out of the core, creating the relevant interfaces, such as: <a href="#mirantis-to-take-over-support-of-kubernetes-dockershim-2">[17]</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Container Runtime Interface (CRI)</p>
</li>
<li>
<p>Container Network Interface (CNI)</p>
</li>
<li>
<p>Container Storage Interface (CSI)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The idea was that any vendor could create a product that automatically interfaces with Kubernetes, as long as it is compliant with these interfaces.</p>
</div>
<div class="paragraph">
<p>That doesn’t mean that non-compliant components can’t be used with Kubernetes; Kubernetes can do anything with the right components. It just means that non-compliant components need a “shim”, which translates between the component and the relevant Kubernetes interface. For example, <em>dockershim</em> takes CRI commands and translates them into something Docker Engine understands, and vice versa. But with the drive to take third-party components like this out of the Kubernetes core, dockershim had to be removed.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://d33wubrfki0l68.cloudfront.net/6b4290afef76cad8a084292cd1b5e468e31c9bb3/c26ce/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png" alt="cri containerd" width="55%" height="55%">
</div>
</div>
<div class="paragraph">
<p>The Kubernetes project plans to deprecate <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">Docker Engine support</a> in the kubelet and support for dockershim will be removed in a future release announced as a part of the Kubernetes v1.20 release.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/" class="bare">https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/" class="bare">https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/" class="bare">https://kubernetes.io/blog/2020/12/02/dockershim-faq/</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/" class="bare">https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</a></p>
</li>
<li>
<p><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation" class="bare">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation</a></p>
</li>
<li>
<p><a href="https://www.docker.com/blog/what-developers-need-to-know-about-docker-docker-engine-and-kubernetes-v1-20/" class="bare">https://www.docker.com/blog/what-developers-need-to-know-about-docker-docker-engine-and-kubernetes-v1-20/</a></p>
</li>
<li>
<p><a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/" class="bare">https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/</a></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="debugging-kubernetes-nodes-with-crictl">2.4.3. Debugging Kubernetes nodes with crictl</h4>
<div class="paragraph">
<p><code>crictl</code> is a command-line interface for CRI-compatible container runtimes. You can use it to inspect and debug container runtimes and applications on a Kubernetes node. crictl and its source are hosted in the <a href="https://github.com/kubernetes-sigs/cri-tools">cri-tools</a> repository. <a href="#k8s-debug-cluster-crictl">[18]</a></p>
</div>
<div class="paragraph">
<p><code>crictl</code> connects to <code>unix:///var/run/dockershim.sock</code> by default. For other runtimes, you can set the endpoint in multiple different ways:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>By setting flags <code>--runtime-endpoint</code> and <code>--image-endpoint</code></p>
</li>
<li>
<p>By setting environment variables <code>CONTAINER_RUNTIME_ENDPOINT</code> and <code>IMAGE_SERVICE_ENDPOINT</code></p>
</li>
<li>
<p>By setting the endpoint in the config file <code>--config=/etc/crictl.yaml</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can also specify timeout values when connecting to the server and enable or disable debugging, by specifying <code>timeout</code> or <code>debug</code> values in the configuration file or using the <code>--timeout</code> and <code>--debug</code> command-line flags.</p>
</div>
<div class="paragraph">
<p>To view or edit the current configuration, view or edit the contents of <code>/etc/crictl.yaml</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
timeout: 10
debug: true</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you use <code>crictl</code> to create pod sandboxes or containers on a running Kubernetes cluster, the Kubelet will eventually delete them. <code>crictl</code> is not a general purpose workflow tool, but a tool that is useful for debugging.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>List pods</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl pods
<span class="go">POD ID              CREATED             STATE               NAME                                        NAMESPACE           ATTEMPT             RUNTIME
1ebbed223d8d2       20 hours ago        Ready               echoserver-66dcc9bcc6-5hwd9                 default             2                   (default)
d6f1ec09d33d0       20 hours ago        Ready               kube-swagger-ui-69b565bcb9-kzv4d            default             3                   (default)
8b8633b6940b7       20 hours ago        Ready               metrics-server-559f9dc594-mmkch             kube-system         3                   (default)
b9c3ee6965fb8       20 hours ago        Ready               overprovisioning-5767847cf9-47zh2           default             3                   (default)
</span><span class="c">...
</span><span class="go">
</span><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl pods <span class="nt">--name</span> echoserver-66dcc9bcc6-5hwd9
<span class="go">POD ID              CREATED             STATE               NAME                          NAMESPACE           ATTEMPT             RUNTIME
1ebbed223d8d2       20 hours ago        Ready               echoserver-66dcc9bcc6-5hwd9   default             2                   (default)

</span><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl pods <span class="nt">--label</span> <span class="nv">app</span><span class="o">=</span>echoserver
<span class="go">POD ID              CREATED             STATE               NAME                          NAMESPACE           ATTEMPT             RUNTIME
1ebbed223d8d2       20 hours ago        Ready               echoserver-66dcc9bcc6-5hwd9   default             2                   (default)</span></code></pre>
</div>
</div>
</li>
<li>
<p>List images</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl images
<span class="go">IMAGE                                                            TAG                 IMAGE ID            SIZE
k8s.gcr.io/coredns/coredns                                       v1.8.4              8d147537fb7d1       47.6MB
k8s.gcr.io/echoserver                                            1.4                 a90209bb39e3d       140MB
k8s.gcr.io/etcd                                                  3.5.0-0             0048118155842       295MB
k8s.gcr.io/kube-apiserver                                        v1.22.3             53224b502ea4d       128MB
</span><span class="c">...
</span><span class="go">
</span><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl images k8s.gcr.io/kube-apiserver
<span class="go">IMAGE                       TAG                 IMAGE ID            SIZE
k8s.gcr.io/kube-apiserver   v1.16.10            d925057c2fa51       170MB
k8s.gcr.io/kube-apiserver   v1.18.3             7e28efa976bd1       173MB
k8s.gcr.io/kube-apiserver   v1.22.3             53224b502ea4d       128MB</span></code></pre>
</div>
</div>
</li>
<li>
<p>List containers</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl ps
<span class="go">CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID
36720d10e6ac8       8522d622299ca       3 hours ago         Running             kube-flannel        0                   e33d2645a22ab
a93b979cf6c5a       6120bd723dced       3 hours ago         Running             kube-proxy          0                   affa1ca7f0a2b</span></code></pre>
</div>
</div>
</li>
<li>
<p>Execute a command in a running container</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> a93b979cf6c5a <span class="nb">ls</span>
<span class="go">bin   dev  home  lib64	mnt  proc  run	 srv  tmp  var
boot  etc  lib	 media	opt  root  sbin  sys  usr</span></code></pre>
</div>
</div>
</li>
<li>
<p>Get a container&#8217;s logs</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl logs <span class="nt">--tail</span> 10 36720d10e6ac8
<span class="go">I1124 03:53:12.446713       1 iptables.go:172] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully
I1124 03:53:12.447346       1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN
I1124 03:53:12.448650       1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE --random-fully
I1124 03:53:12.449900       1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.1.0/24 -j RETURN
I1124 03:53:12.451037       1 iptables.go:160] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE --random-fully
</span><span class="gp">I1124 03:53:12.543281       1 iptables.go:148] Some iptables rules are missing;</span><span class="w"> </span>deleting and recreating rules
<span class="go">I1124 03:53:12.543375       1 iptables.go:172] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT
I1124 03:53:12.544132       1 iptables.go:172] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT
I1124 03:53:12.544818       1 iptables.go:160] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT
I1124 03:53:12.546086       1 iptables.go:160] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT</span></code></pre>
</div>
</div>
</li>
<li>
<p>Run a pod sandbox</p>
<div class="paragraph">
<p>Using <code>crictl</code> to run a pod sandbox is useful for debugging container runtimes. On a running Kubernetes cluster, the sandbox will eventually be stopped and deleted by the Kubelet.</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>Create a JSON file like the following:</p>
<div class="listingblock">
<div class="title">pod-config.json</div>
<div class="content">
<pre class="rouge highlight"><code data-lang="json"><span class="p">{</span><span class="w">
  </span><span class="nl">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"nginx-sandbox"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"namespace"</span><span class="p">:</span><span class="w"> </span><span class="s2">"default"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"attempt"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"uid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"hdishd83djaidwnduwk28bcsb"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"log_directory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/tmp"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"linux"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre>
</div>
</div>
</li>
<li>
<p>Use the <code>crictl runp</code> command to apply the JSON and run the sandbox.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl runp pod-config.json
<span class="go">7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The ID of the sandbox is returned.</p>
</div>
</li>
</ul>
</div>
</div>
</div>
</li>
<li>
<p>Create a container</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>Pull a busybox image</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl pull busybox
<span class="go">Image is up to date for busybox@sha256:e7157b6d7ebbe2cce5eaa8cfe8aa4fa82d173999b9f90a9ec42e57323546c353</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create configs for the pod and the container:</p>
<div class="listingblock">
<div class="title">pod-config.json</div>
<div class="content">
<pre class="rouge highlight"><code data-lang="json"><span class="p">{</span><span class="w">
  </span><span class="nl">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"nginx-sandbox"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"namespace"</span><span class="p">:</span><span class="w"> </span><span class="s2">"default"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"attempt"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"uid"</span><span class="p">:</span><span class="w"> </span><span class="s2">"hdishd83djaidwnduwk28bcsb"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"log_directory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/tmp"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"linux"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">container-config.json</div>
<div class="content">
<pre class="rouge highlight"><code data-lang="json"><span class="p">{</span><span class="w">
  </span><span class="nl">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"busybox"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"image"</span><span class="p">:{</span><span class="w">
    </span><span class="nl">"image"</span><span class="p">:</span><span class="w"> </span><span class="s2">"busybox"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"command"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"top"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nl">"log_path"</span><span class="p">:</span><span class="s2">"busybox.log"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"linux"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create the container, passing the ID of the previously-created pod, the container config file, and the pod config file. The ID of the container is returned.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl create 7a42c484476cd008df5730df0cbbd679c72ac57b7d16b82d40917ed5ffe20ada container-config.json pod-config.json
<span class="go">1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You should set the CLI argument <code>container-config.json</code> before <code>pod-config.json</code>.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</div>
</div>
</li>
<li>
<p>Start a container</p>
<div class="paragraph">
<p>To start a container, pass its ID to <code>crictl start</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl start 1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88
<span class="go">1c0bbf7dce7207af100541032c273f0426b1a0d7f44fb00c263b185ee388dc88</span></code></pre>
</div>
</div>
</li>
<li>
<p>Create and start a container within one command</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl run container-config.json pod-config.json
<span class="go">7dc686635ed282a71b4b46210fc061847ea19f001e10f5860c335aa4375c713e

</span><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>crictl ps <span class="nt">-a</span>
<span class="go">CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
7dc686635ed28       busybox             20 seconds ago      Exited              busybox                   0                   32623026e0ec8</span></code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="references">References</h2>
<div class="sectionbody">
<div class="ulist bibliography">
<ul class="bibliography">
<li>
<p><a id="rhel7-kernel_features"></a>[1] <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_features" class="bare">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_features</a></p>
</li>
<li>
<p><a id="Linux_namespaces"></a>[2] <a href="https://en.wikipedia.org/wiki/Linux_namespaces" class="bare">https://en.wikipedia.org/wiki/Linux_namespaces</a></p>
</li>
<li>
<p><a id="rhel7-resource_management_guide-index"></a>[3] <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index" class="bare">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index</a></p>
</li>
<li>
<p><a id="rhel7-systemd"></a>[4] <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd" class="bare">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd</a></p>
</li>
<li>
<p><a id="rhel6-resource_management_guide-index"></a>[5] <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/index" class="bare">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/index</a></p>
</li>
<li>
<p><a id="k8s-arch-cgroups"></a>[6] <a href="https://kubernetes.io/docs/concepts/architecture/cgroups/" class="bare">https://kubernetes.io/docs/concepts/architecture/cgroups/</a></p>
</li>
<li>
<p><a id="rl-container-namespaces-nsenter"></a>[7] <a href="https://www.redhat.com/sysadmin/container-namespaces-nsenter" class="bare">https://www.redhat.com/sysadmin/container-namespaces-nsenter</a></p>
</li>
<li>
<p><a id="gcp-what-are-containers"></a>[8] <a href="https://cloud.google.com/learn/what-are-containers" class="bare">https://cloud.google.com/learn/what-are-containers</a></p>
</li>
<li>
<p><a id="rl-whats-a-linux-container"></a>[9] <a href="https://www.redhat.com/en/topics/containers/whats-a-linux-container" class="bare">https://www.redhat.com/en/topics/containers/whats-a-linux-container</a></p>
</li>
<li>
<p><a id="oci-about"></a>[10] <a href="https://opencontainers.org/about/overview/" class="bare">https://opencontainers.org/about/overview/</a></p>
</li>
<li>
<p><a id="containerd-daemon-to-control-runc"></a>[11] <a href="https://www.docker.com/blog/containerd-daemon-to-control-runc/" class="bare">https://www.docker.com/blog/containerd-daemon-to-control-runc/</a></p>
</li>
<li>
<p><a id="docker-what-is-containerd-runtime"></a>[12] <a href="https://www.docker.com/blog/what-is-containerd-runtime/" class="bare">https://www.docker.com/blog/what-is-containerd-runtime/</a></p>
</li>
<li>
<p><a id="rl-what-is-docker"></a>[13] <a href="https://www.redhat.com/en/topics/containers/what-is-docker" class="bare">https://www.redhat.com/en/topics/containers/what-is-docker</a></p>
</li>
<li>
<p><a id="docker-what-container"></a>[14] <a href="https://www.docker.com/resources/what-container" class="bare">https://www.docker.com/resources/what-container</a></p>
</li>
<li>
<p><a id="rl-what-is-kubernetes"></a>[15] <a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes" class="bare">https://www.redhat.com/en/topics/containers/what-is-kubernetes</a></p>
</li>
<li>
<p><a id="k8s-cri-in-kubernetes"></a>[16] <a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/" class="bare">https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/</a></p>
</li>
<li>
<p><a id="mirantis-to-take-over-support-of-kubernetes-dockershim-2"></a>[17] <a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/" class="bare">https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/</a></p>
</li>
<li>
<p><a id="k8s-debug-cluster-crictl"></a>[18] <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/" class="bare">https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/</a></p>
</li>
<li>
<p><a id="suse-demystifying-containers-part-i-kernel-space"></a>[19] <a href="https://www.suse.com/c/demystifying-containers-part-i-kernel-space/" class="bare">https://www.suse.com/c/demystifying-containers-part-i-kernel-space/</a></p>
</li>
</ul>
</div>
</div>
</div>
<style>
  .utterances {
      max-width: 100%;
  }
</style>
<script src="https://utteranc.es/client.js"
        repo="looogos/utterances"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

</div>
</article>
    </main>
    <footer class="c-footer">
  <div class="c-footer-license">
    <span>Article licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span>
  </div>
  
  <details class="c-footer-extralinks" open>
    <summary class="c-footer-extralinks-summary">Extral Links</summary>
    <div class="c-footer-extralinks-content">
      
      <a href="https://jekyllrb.com/">Jekyll</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://shopify.github.io/liquid/">Liquid</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://docs.asciidoctor.org/">Asciidoctor</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://github.com/qqbuby/">GitHub</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="/feed.xml">RSS</a>
      
      
    </div>
  </details>
  
</footer>

    <script src="/assets/js/nav.js" defer></script>
    <script src="/assets/js/heading-anchors.js" defer></script>
    <!-- https://cdn.jsdelivr.net/gh/lurongkai/anti-baidu/js/anti-baidu-latest.min.js -->    
    <script type="text/javascript" src="/js/anti-baidu.min.js" charset="UTF-8"></script>
  </body>
</html>
